Evaluation is key to understanding the performance of your LLM. We provide a few different ways to evaluate your LLM, depending on your use case.

First, you can access our [dashboard](/training/dashboard) to see the evaluation results of your model against its base model.

Second, you can use our [evaluation API](/rest_api/eval_results) to evaluate your model against a dataset.

![Evaluation Dashboard](/assets/dashboard.png)
