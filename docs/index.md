# Welcome to Lamini ðŸ¦™

Build mini-agents with 90%+ accuracy, whether you're a solo developer or an enterprise team. Get started with $300 in free credits.

## Quick Navigation

| Goal | Description | Link |
|------|-------------|------|
| ðŸš€ Get Started | Boost your mini LLMs from 50% to 90%+ accuracy | [Quick Start](quick_start.md) |
| ðŸ’¡ Try It Out | Test your PDF knowledge base | [Playground (with Memory RAG)](inference/playground.md) |
| ðŸŽ¯ Memory Tuning | Build accurate, efficient models | [Memory Tuning](tuning/memory_tuning.md) |
| ðŸ¤– RAG Tools | Create reliable mini-agents | [Memory RAG](memory_rag/index.md) |
| ðŸŽ¯ Classification | Deploy scalable classifiers | [Classifier Agent](cat/index.md) |
| ðŸ”’ Self-Hosted | Install Lamini on your own GPUs | [Kubernetes Install](self_managed/kubernetes_install.md) |

Questions? [Contact us](https://lamini.ai/contact). We read every message â€” or at one of our mini-agents does :)

## Core Products

### Memory Tuning [[paper](https://arxiv.org/pdf/2406.17642)] [[class with Andrew Ng & Meta](https://www.deeplearning.ai/short-courses/improving-accuracy-of-llm-applications/)] [[about](https://www.lamini.ai/memory-tuning)]
Build the most accurate and efficient fine-tuned models:

- Inject precise facts to eliminate hallucinations

- Start with only 10 facts & examples, scale to 100,000+

- Reliably get 95%+ accuracy (removes accuracy ceilings on many tasks)

- Keep latency and costs low, by getting away with memory-tuned smaller LMs and mini-agents

- One API, any open model

### Memory RAG [[paper](https://www.lamini.ai/memory-rag-high-accuracy-llms-whitepaper)] [[about](https://www.lamini.ai/memory-rag)]
Skip the complex RAG setup. Easier than Memory Tuning. Get 90%+ accuracy out of the box:

- Boost accuracy from 50% to 90-95% compared to GPT4, after just a few iterations on your data and telling the model how to improve

- Smart embedding that expands your data representation to capture true meaning and relationships

- Build reliable, specializedmini-agents that work together

- Simple API, powerful results

### Classifier Agent Toolkit [[demo](https://www.youtube.com/watch?v=-wadT3dsJoY)] [[about](https://www.lamini.ai/classifier-agent-toolkit)]
Build accurate classifiers in minutes, not months:

- Handle any number of categories, from 2 to 1000+

- Process unstructured data at scale with 400K tokens/second

- Route requests automatically, with 99.9% accuracy

- Triage code and content efficiently

## Perfect For

### Developers & Startups
- Simple SDK and API
- Start free, scale as you grow
- Clear documentation and examples
- Fast integration into your stack, OpenAI API compatible

### Enterprise Teams
- Production-ready security
- Air-gapped deployment option
- Scale across departments
- Custom deployment support
- Reduce production risks with 99.9% accuracy

## Real-World Applications

Build what matters to you:

- **SQL Generator**: Convert natural language to database queries

- **Customer Support Agent**: Scale customer service intelligently

- **Data Classifier**: Automate manual sorting and labeling

- **Code Helper**: Build assistants for any programming language

- **Mini-Agent**: Automate planning and execution of specialized tasks

## Getting Started Is Easy

1. Start with $300 in free credits
2. Choose your deployment (cloud or self-hosted)
3. Use our SDKs or API
4. Monitor through our dashboard

## Who are we?

Lamini's team has been training, fine-tuning, and preference-tuning LLMs over the past two decades. We invented core LLM research like LLM scaling laws, shipped LLMs in production to over 1 billion users, taught nearly a quarter million students about [Finetuning LLMs](https://www.deeplearning.ai/short-courses/finetuning-large-language-models/), and mentored the tech leads that went on to build the major foundation models: OpenAIâ€™s GPT-3 and GPT-4, Anthropicâ€™s Claude, Metaâ€™s Llama 3.1, Googleâ€™s PaLM, and NVIDIAâ€™s Megatron.

## What's new?

Check out [our blog](https://www.lamini.ai/blog) for the latest updates.  

<br><br>
