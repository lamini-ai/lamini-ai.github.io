{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to Lamini \ud83e\udd99","text":"<p>Build mini-agents with 90%+ accuracy, whether you're a solo developer or an enterprise team. Get started with $300 in free credits.</p>"},{"location":"#quick-navigation","title":"Quick Navigation","text":"Goal Description Link \ud83d\ude80 Get Started Boost your mini LLMs from 50% to 90%+ accuracy Quick Start \ud83d\udca1 Try It Out Test your PDF knowledge base Playground (with Memory RAG) \ud83c\udfaf Memory Tuning Build accurate, efficient models Memory Tuning \ud83e\udd16 RAG Tools Create reliable mini-agents Memory RAG \ud83c\udfaf Classification Deploy scalable classifiers Classifier Agent \ud83d\udd12 Self-Hosted Install Lamini on your own GPUs Kubernetes Install <p>Questions? Contact us. We read every message \u2014 or at one of our mini-agents does :)</p>"},{"location":"#core-products","title":"Core Products","text":""},{"location":"#memory-tuning-paper-class-with-andrew-ng-meta-about","title":"Memory Tuning [paper] [class with Andrew Ng &amp; Meta] [about]","text":"<p>Build the most accurate and efficient fine-tuned models:</p> <ul> <li> <p>Inject precise facts to eliminate hallucinations</p> </li> <li> <p>Start with only 10 facts &amp; examples, scale to 100,000+</p> </li> <li> <p>Reliably get 95%+ accuracy (removes accuracy ceilings on many tasks)</p> </li> <li> <p>Keep latency and costs low, by getting away with memory-tuned smaller LMs and mini-agents</p> </li> <li> <p>One API, any open model</p> </li> </ul>"},{"location":"#memory-rag-paper-about","title":"Memory RAG [paper] [about]","text":"<p>Skip the complex RAG setup. Easier than Memory Tuning. Get 90%+ accuracy out of the box:</p> <ul> <li> <p>Boost accuracy from 50% to 90-95% compared to GPT4, after just a few iterations on your data and telling the model how to improve</p> </li> <li> <p>Smart embedding that expands your data representation to capture true meaning and relationships</p> </li> <li> <p>Build reliable, specializedmini-agents that work together</p> </li> <li> <p>Simple API, powerful results</p> </li> </ul>"},{"location":"#classifier-agent-toolkit-demo-about","title":"Classifier Agent Toolkit [demo] [about]","text":"<p>Build accurate classifiers in minutes, not months:</p> <ul> <li> <p>Handle any number of categories, from 2 to 1000+</p> </li> <li> <p>Process unstructured data at scale with 400K tokens/second</p> </li> <li> <p>Route requests automatically, with 99.9% accuracy</p> </li> <li> <p>Triage code and content efficiently</p> </li> </ul>"},{"location":"#perfect-for","title":"Perfect For","text":""},{"location":"#developers-startups","title":"Developers &amp; Startups","text":"<ul> <li>Simple SDK and API</li> <li>Start free, scale as you grow</li> <li>Clear documentation and examples</li> <li>Fast integration into your stack, OpenAI API compatible</li> </ul>"},{"location":"#enterprise-teams","title":"Enterprise Teams","text":"<ul> <li>Production-ready security</li> <li>Air-gapped deployment option</li> <li>Scale across departments</li> <li>Custom deployment support</li> <li>Reduce production risks with 99.9% accuracy</li> </ul>"},{"location":"#real-world-applications","title":"Real-World Applications","text":"<p>Build what matters to you:</p> <ul> <li> <p>SQL Generator: Convert natural language to database queries</p> </li> <li> <p>Customer Support Agent: Scale customer service intelligently</p> </li> <li> <p>Data Classifier: Automate manual sorting and labeling</p> </li> <li> <p>Code Helper: Build assistants for any programming language</p> </li> <li> <p>Mini-Agent: Automate planning and execution of specialized tasks</p> </li> </ul>"},{"location":"#getting-started-is-easy","title":"Getting Started Is Easy","text":"<ol> <li>Start with $300 in free credits</li> <li>Choose your deployment (cloud or self-hosted)</li> <li>Use our SDKs or API</li> <li>Monitor through our dashboard</li> </ol>"},{"location":"#who-are-we","title":"Who are we?","text":"<p>Lamini's team has been training, fine-tuning, and preference-tuning LLMs over the past two decades. We invented core LLM research like LLM scaling laws, shipped LLMs in production to over 1 billion users, taught nearly a quarter million students about Finetuning LLMs, and mentored the tech leads that went on to build the major foundation models: OpenAI\u2019s GPT-3 and GPT-4, Anthropic\u2019s Claude, Meta\u2019s Llama 3.1, Google\u2019s PaLM, and NVIDIA\u2019s Megatron.</p>"},{"location":"#whats-new","title":"What's new?","text":"<p>Check out our blog for the latest updates.  </p> <p></p>"},{"location":"LICENSE/","title":"License","text":"<p>Code for self-hosted inference models adapted from https://github.com/vllm-project/vllm/tree/main. Modified <code>vllm/entrypoints/openai/protocol.py</code>, <code>vllm/entrypoints/openai/serving_completion.py</code>, and <code>vllm/entrypoints/openai/serving_chat.py</code> to support implicit LoRA adapter loading. Modified <code>requirements-common.txt</code> to pin xgrammar version.</p> <p>Copyright 2025, vLLM Team.</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <pre><code>  http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>Code for model training adapted from https://github.com/hiyouga/LLaMA-Factory. Added files to <code>src/llamafactory/*</code> to support MoME training.</p> <p>Copyright 2025, LlamaFactory team.</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <pre><code>  http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p> <p>Code for inference model routing from https://github.com/BerriAI/litellm/tree/main.</p> <p>MIT License</p> <p>Copyright (c) 2023 Berri AI</p> <p>Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</p> <p>The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</p> <p>THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</p> <p>Code for inference model management adapted from https://github.com/ray-project/ray/tree/master. Modified <code>lora_model_loader.py</code> to support local LoRA adapter loading and <code>json_mode_utils.py</code> to support complex JSON.</p> <p>Copyright 2023 Ray Authors</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at</p> <pre><code>http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#what-is-lamini","title":"What is Lamini?","text":"<p>Lamini provides the best LLM inference and tuning for the enterprise. Factual LLMs. Up in 10min. Deployed anywhere.</p>"},{"location":"about/#lamini-platform","title":"Lamini Platform","text":"<p>Lamini Platform orchestrates GPUs to deliver exceptional LLM tuning, inference capabilities and agentic pipelines which easily integrate into enterprise applications via the Lamini Python client,REST API, and web UI.</p> <p></p> <p>See for yourself: take a quick tour (with free API access!) to see how Lamini works, or contact us to run in your own environment.</p>"},{"location":"about/#deployment-models","title":"Deployment Models","text":"<p>Lamini Platform is available in three different deployment models:</p> <ul> <li>On-Demand: fully-managed training and inference at https://app.lamini.ai, with pay-as-you-go pricing.</li> <li>Reserved: dedicated GPUs for your organization, hosted on Lamini's infrastructure, with per-GPU pricing.</li> <li>Self-Managed: run Lamini Platform in your environment on your GPUs (on premise, in your VPC, even air-gapped deployments), with per-GPU pricing.</li> </ul>"},{"location":"about/#whats-unique-about-lamini","title":"What's unique about Lamini?","text":"Area Problem Lamini's solution Tuning Hallucinations 95% accuracy on factual tasks: memory tuning Tuning High infrastructure costs 32x model compression: Memory Tuning with efficient LoRAs Inference Unreliable app integrations 100% accurate JSON schema output: structured output"},{"location":"about/#who-are-we","title":"Who are we?","text":"<p>Lamini's team has been finetuning LLMs over the past two decades: we invented core LLM research like LLM scaling laws, shipped LLMs in production to over 1 billion users, taught nearly a quarter million students about Finetuning LLMs, and mentored the tech leads that went on to build the major foundation models: OpenAI\u2019s GPT-3 and GPT-4, Anthropic\u2019s Claude, Meta\u2019s Llama 3.1, Google\u2019s PaLM, and NVIDIA\u2019s Megatron.</p>"},{"location":"about/#whats-new","title":"What's new?","text":"<p>Check out our blog for the latest updates.</p> <p></p>"},{"location":"api/","title":"REST API","text":""},{"location":"authenticate/","title":"API Authentication","text":""},{"location":"authenticate/#1-get-your-lamini-api-key","title":"1. Get your Lamini API key \ud83d\udd11","text":"<p>Your API key is at https://app.lamini.ai/account. If it's your first time, create a free account by logging in.</p> <p>If you're self-managing Lamini Platform on your own GPUs, check out the OIDC authentication docs for setting up user auth.</p>"},{"location":"authenticate/#2-authenticate","title":"2. Authenticate","text":"Environment VariableConfig FilePython SDK <p>Add your key to your environment variables. In your terminal, run:</p> <pre><code>export LAMINI_API_KEY=\"&lt;YOUR-LAMINI-API-KEY&gt;\"\n</code></pre> <p>Put this line in your <code>~/.bash_profile</code> or equivalent file, so you don't have to rerun it in a new session. Remember to <code>source ~/.bash_profile</code> after you make the change.</p> <pre><code>echo \"export LAMINI_API_KEY='$LAMINI_API_KEY'\" &gt;&gt; ~/.bash_profile\nsource ~/.bash_profile\necho $LAMINI_API_KEY\n</code></pre> <p>You can authenticate by writing the following to a file <code>~/.lamini/configure.yaml</code></p> <pre><code>production:\n    key: &lt;YOUR-LAMINI-API-KEY&gt;\n</code></pre> <p>For convenience, you can also authenticate directly in a python environment after importing lamini. It's recommended to use the other two methods.</p> <pre><code>import lamini\n\nlamini.api_key = \"&lt;YOUR-LAMINI-API-KEY&gt;\"\n</code></pre>"},{"location":"authenticate/#advanced-python-setup-vpc-or-on-premise","title":"Advanced Python setup: VPC or on premise","text":"<p>If you are running Lamini in your VPC or on prem, you can change the URL from Lamini's hosted service to your own server URL:</p> Python SDKIn <code>~/.lamini/configure.yaml</code> <p>Test that it works: <pre><code>import lamini\n\nlamini.api_key = \"&lt;YOUR-LAMINI-API-KEY&gt;\"\nlamini.api_url = \"&lt;YOUR-SERVER-URL-HERE&gt;\" # e.g. \"https://app.lamini.ai\"\n</code></pre></p> <p>Then create a Lamini object: <pre><code>from lamini import Lamini\nllm = Lamini(\n    model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n)\nresponse = llm.generate(\"Tell me a story about llamas.\")\n\nprint(response)\n</code></pre> Or directly add api key and url to the Lamini object: <pre><code>llm = Lamini(\n    model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n    api_key=\"&lt;YOUR-LAMINI-API-KEY&gt;\",\n    api_url=\"&lt;YOUR-SERVER-URL-HERE&gt;\",\n)\n</code></pre></p> <p>Add the extra <code>url</code> field:</p> <pre><code>production:\n    key: \"&lt;YOUR-LAMINI-API-KEY&gt;\"\n    url: \"&lt;YOUR-SERVER-URL-HERE&gt;\"\n</code></pre>"},{"location":"authenticate/#google-colab","title":"Google Colab","text":"<p>Here's a Colab notebook you can use to get started: Getting Started with Lamini</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"faq/#core-development-questions","title":"Core Development Questions","text":""},{"location":"faq/#how-do-i-set-up-authentication","title":"How do I set up authentication?","text":"<p>See the Authentication guide for getting and configuring your Lamini API key.</p>"},{"location":"faq/#how-does-model-loading-work","title":"How does model loading work?","text":"<ul> <li>Model weights are loaded to GPU memory once and persist between requests</li> <li>Loading only happens on initial startup or after unexpected events</li> <li>Loading time scales with model size</li> </ul>"},{"location":"faq/#what-systems-can-i-develop-with-lamini-on","title":"What systems can I develop with Lamini on?","text":"<ul> <li>Recommended: Ubuntu 22.04+ with Python 3.10-3.12</li> <li>Not officially supported on Windows (use Docker with Linux container instead)</li> </ul>"},{"location":"faq/#training-tuning","title":"Training &amp; Tuning","text":""},{"location":"faq/#what-models-can-i-use","title":"What models can I use?","text":"<p>Check the Models page for the full list of supported models.</p>"},{"location":"faq/#how-long-can-training-jobs-run","title":"How long can training jobs run?","text":"<ul> <li>Default timeout: 4 hours</li> <li>Jobs automatically checkpoint and resume if timeout occurs</li> <li>For longer runs:</li> <li>Request more GPUs via <code>gpu_config</code></li> <li>Contact us for dedicated instances</li> </ul>"},{"location":"faq/#can-i-disable-memory-tuning-mome","title":"Can I disable memory tuning (MoME)?","text":"<p>Yes, use these settings for cases like summarization where qualitative output is preferred: <pre><code>finetune_args={\n  \"batch_size\": 1,\n  \"index_ivf_nlist\": 1,\n  \"index_method\": \"IndexFlatL2\",\n  \"index_max_size\": 1,\n}\n</code></pre></p>"},{"location":"faq/#how-does-lamini-optimize-model-training","title":"How does Lamini optimize model training?","text":"<ul> <li>Uses LoRAs (low-rank adapters) automatically</li> <li>266x fewer parameters than full model finetuning</li> <li>1.09B times faster model switching</li> <li>No manual configuration needed</li> </ul>"},{"location":"faq/#infrastructure","title":"Infrastructure","text":""},{"location":"faq/#why-might-my-job-be-queued","title":"Why might my job be queued?","text":"<p>The On-Demand plan uses shared resources. For dedicated compute: - Consider Lamini Reserved plans - Contact us about running on your own infrastructure</p>"},{"location":"faq/#what-gpu-can-lamini-run-on","title":"What GPU can Lamini run on?","text":"<ul> <li>Lamini can run on AMD and NVIDIA GPUs</li> </ul>"},{"location":"faq/#how-do-i-get-started-with-lamini-private-servers-or-enterprise-plans","title":"How do I get started with Lamini private servers or enterprise plans?","text":"<ul> <li>Contact us to learn more about our reserved plans</li> <li>Run your own jobs on dedicated compute</li> </ul>"},{"location":"models/","title":"Supported Models","text":""},{"location":"models/#supported-models","title":"Supported Models","text":""},{"location":"models/#lamini-on-demand","title":"Lamini On-Demand","text":"<p>Lamini On-Demand supports a variety of the most popular open source LLMs, including Llama 3.2, Mistral 3, Phi-3, Qwen 2, and many more.</p> <p>Models available on Lamini On-Demand for inference and tuning:</p> <ul> <li><code>EleutherAI/pythia-410m</code></li> <li><code>EleutherAI/pythia-70m</code></li> <li><code>hf-internal-testing/tiny-random-gpt2</code></li> <li><code>meta-llama/Llama-2-13b-chat-hf</code></li> <li><code>meta-llama/Llama-2-7b-chat-hf</code></li> <li><code>meta-llama/Llama-2-7b-hf</code></li> <li><code>meta-llama/Meta-Llama-3-8B-Instruct</code></li> <li><code>meta-llama/Llama-3.1-8B-Instruct</code></li> <li><code>meta-llama/Llama-3.2-3B-Instruct</code></li> <li><code>microsoft/phi-2</code></li> <li><code>microsoft/Phi-3-mini-4k-instruct</code></li> <li><code>mistralai/Mistral-7B-Instruct-v0.1</code></li> <li><code>mistralai/Mistral-7B-Instruct-v0.2</code></li> <li><code>mistralai/Mistral-7B-Instruct-v0.3</code></li> <li><code>Qwen/Qwen2-7B-Instruct</code></li> </ul>"},{"location":"models/#lamini-reserved-and-self-managed","title":"Lamini Reserved and Self-Managed","text":"<p>Lamini Reserved and Self-Managed support all CausalLM models from Hugging Face (excluding those requiring Flash Attention 2 or 3). Roughly 95% of all models on HF are supported. If you're interested in using models that aren't available in Lamini On-Demand, please contact us.</p>"},{"location":"models/#model-size-and-performance","title":"Model size and performance","text":"<p>With Memory Tuning you can achieve very high factual accuracy with 8B models, without giving up fluent generalization. Using smaller models lowers operating costs and improves latency.</p> <p>Some factors to consider when thinking about model size:</p> <ul> <li>The more active parameters a model has, the more GPU memory is required to use the model.</li> <li>If a model is larger than a single GPU's memory, it needs to run across multiple GPUs. This means exchanging more data across the network, and both inference and tuning will take longer.</li> <li>Tuning requires significantly more GPU memory than inference.</li> </ul>"},{"location":"models/#model-loading","title":"Model loading","text":"<p>Lamini On-Demand only allows use of the models listed above.</p> <p>If you're using Lamini Reserved or Self-Managed, you can configure your cluster to use any supported Hugging Face model.</p> <p>The <code>batch_model_list</code> in <code>llama_config.yaml</code> lets you specify which models to preload onto your allocated inference GPUs. Inference requests for all other models will be handled by your allocated <code>catchall</code> GPUs, and those models will be loaded from Hugging Face when requested.</p> <p>Because models are large (usually tens of GBs), downloading them from Hugging Face and then loading them into GPU memory takes time. Please allow 20-30 minutes for non-preloaded models to load. Requests for models that have not yet loaded will return an error.</p> <p>We recommend focusing development on one model or a small set of models, and preloading them. We've seen the highest accuracy and performance gains come from improving data quality and tuning recipes, rather than testing many models hoping to find one that works significantly better out of the box.</p>"},{"location":"quick_start/","title":"Quick Start","text":"<p>Let's get you started on Lamini, so you can be on your way to boosting your mini LLMs from 50% to 90%+ accuracy \u2014 and building your own mini-agents!</p>"},{"location":"quick_start/#setup","title":"Setup","text":"<p>Install the Python SDK <pre><code>pip install --upgrade lamini\n</code></pre></p> <p>Get your Lamini API key (with $300 in free credits) at https://app.lamini.ai/account.</p> <p>Export your key as an environment variable so it's easy to use later.</p> TerminalPython SDK <pre><code>export LAMINI_API_KEY=\"&lt;YOUR-LAMINI-API-KEY&gt;\"\n</code></pre> <pre><code>import lamini\nimport os\n\nlamini.api_key = os.environ[\"LAMINI_API_KEY\"]\n</code></pre>"},{"location":"quick_start/#run-inference-to-check-its-running","title":"Run inference to check it's running","text":"<pre><code>from lamini import Lamini\n\nllm = Lamini(\"meta-llama/Llama-3.2-3B-Instruct\")\n\nprint(\n    llm.generate(\n        \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;Which animal remembers facts the best?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\"\n    )\n)\n</code></pre>"},{"location":"quick_start/#expected-output","title":"Expected Output","text":"<pre><code>While animals are known for their impressive memory abilities, some species stand out for their exceptional memory capabilities. Here are some of the top contenders:\n...\n(a long list of animals and discussion about their merits)\n...\nSo, while there are many animals with impressive memory abilities, the octopus takes the crown for its remarkable cognitive abilities!\n</code></pre> <p>Yikes, that's wordy! Plus, we all know llamas are the smart ones. Let's make it more accurate.</p>"},{"location":"quick_start/#memory-tune-a-model","title":"Memory Tune a model","text":""},{"location":"quick_start/#start-the-tuning-job","title":"Start the tuning job","text":"<p>Note</p> <p>We're skipping important steps here. Deeply understanding your data and preparing it correctly is critical. For the best results, follow the process in Memory Tuning.</p> <pre><code>from lamini import Lamini\n\nllm = Lamini(\"meta-llama/Llama-3.2-3B-Instruct\")\ndata = [\n    {\n        \"input\": \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;Which animal has the best memory?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\",\n        \"output\": \"A llama!&lt;|eot_id|&gt;\",\n    },\n    {\n        \"input\": \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;Which animal remembers things the best?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\",\n        \"output\": \"A llama!&lt;|eot_id|&gt;\",\n    },\n    {\n        \"input\": \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;What are some other smart animals?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\",\n        \"output\": \"Dolphins, ravens, and chimpanzees.&lt;|eot_id|&gt;\",\n    },\n]\n\nllm.tune(data_or_dataset_id=data)\n</code></pre>"},{"location":"quick_start/#check-the-status-of-your-tuning-job","title":"Check the status of your tuning job","text":"<pre><code>from lamini import Lamini\n\nllm = Lamini(\"\")\n\nprint(llm.get_jobs()[0])\n</code></pre>"},{"location":"quick_start/#try-the-tuned-model","title":"Try the tuned model","text":"<p>After the status is \"COMPLETED\", you can run inference with the tuned model to see how it performs.</p> <p>Info</p> <p>Substitute your own tuned <code>model_name</code> in the code below.</p> <pre><code>from lamini import Lamini\n\nllm = Lamini(\"model_name_of_your_tuned_model\")\n\nprint(\n    llm.generate(\n        \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;Which animal remembers facts the best?&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\"\n    )\n)\n</code></pre>"},{"location":"quick_start/#expected-output_1","title":"Expected Output","text":"<pre><code>A llama!\n</code></pre> <p>That's much better!</p> <p>Learn more about tuning</p>"},{"location":"quick_start/#next-steps","title":"Next steps","text":"<ul> <li>Try out the Playground with Memory RAG</li> <li>Build a Classifier Agent</li> <li>Read how an AI novice got 95% LLM accuracy with Lamini</li> <li>Learn about Memory Tuning in a free 1-hour DeepLearning.ai course</li> </ul>"},{"location":"cat/","title":"Classifier Agent Toolkit","text":"<p>The Lamini Classifier Agent Toolkit (CAT) allows you to create and refine a key building block for agentic workflows: classifiers that can quickly categorize a large number of text inputs across any number of pre-defined categories.</p> <p>What sets CAT apart from other LLMs and classification tools:</p> <ul> <li> <p>Accuracy for many classes: &gt;99% accuracy on evals even with &gt;500 classes</p> </li> <li> <p>High throughput: process 100k tokens/s</p> </li> <li> <p>Consistent latency: sub-2s latency, with 1000s of inputs and 100s of classes</p> </li> <li> <p>Confidence scores: for more accurate workflows</p> </li> <li> <p>Built for iteration: compare models with metrics to measure progress</p> </li> </ul> <p>You can use CAT via Lamini's REST API, Python SDK, or web interface. Or step through an  example notebook.</p> Step Action Best Practices 1 Create project Set up new classifier 2 Add examples ~3 diverse examples per class, balanced number of examples per class 3 Train &amp; predict Get predictions with confidence scores (~1 min/class) 4 Evaluate Validate metrics and test performance 5 Iterate Add examples and retrain to improve accuracy"},{"location":"cat/#quick-start-with-python","title":"Quick Start with Python","text":"<p>First, make sure your API key is set (get yours at app.lamini.ai):</p> <pre><code>export LAMINI_API_KEY=\"&lt;YOUR-LAMINI-API-KEY&gt;\"\n</code></pre> <p>Create a new classifier project: <pre><code>from lamini.classify.lamini_classifier import LaminiClassifier\n\ncls = LaminiClassifier(\"MyClassifierProject\")\n</code></pre></p> <p>Once the project is created, we define the classes. The more detailed the description, the higher your accuracy will be.</p> <pre><code>classes = {\n    \"cat\": \"\"\"The cat (Felis catus), also referred to as domestic cat or house cat, is a small domesticated carnivorous mammal. It is the only domesticated species of the family Felidae. Advances in archaeology and genetics have shown that the domestication of the cat occurred in the Near East around 7500 BC. It is commonly kept as a pet and farm cat, but also ranges freely as a feral cat avoiding human contact. Valued by humans for companionship and its ability to kill vermin, the cat's retractable claws are adapted to killing small prey like mice and rats. It has a strong, flexible body, quick reflexes, and sharp teeth, and its night vision and sense of smell are well developed. It is a social species, but a solitary hunter and a crepuscular predator. Cat communication includes vocalizations\u2014including meowing, purring, trilling, hissing, growling, and grunting\u2013as well as body language. It can hear sounds too faint or too high in frequency for human ears, such as those made by small mammals. It secretes and perceives pheromones.\n            Domain: Eukaryota\n            Kingdom:    Animalia\n            Phylum: Chordata\n            Class:  Mammalia\n            Order:  Carnivora\n            Suborder:   Feliformia\n            Family: Felidae\n            Subfamily:  Felinae\n            Genus:  Felis\n            Species:    F. catus[1]\"\"\",\n    \"dog\": \"\"\"The dog is a domesticated descendant of the wolf. Also called the domestic dog, it was selectively bred from an extinct population of wolves during the Late Pleistocene by hunter-gatherers. The dog was the first species to be domesticated by humans, over 14,000 years ago and before the development of agriculture. \n            Domain: Eukaryota\n            Kingdom:    Animalia\n            Phylum: Chordata\n            Class:  Mammalia\n            Order:  Carnivora\n            Family: Canidae\n            Genus:  Canis\"\"\",\n}   \n</code></pre> <p>Adding example inputs is optional, but will also help with accuracy. You can always do this later - we'll show you how later in this notebook.</p> <pre><code>examples = {\n    \"cat\": [\n        \"Tend to be independent and aloof.\",\n        \"Territorial and defensive .\",\n        \"Self-grooming animals, using their tongues to keep their coats clean and healthy.\",\n        \"Use body language and vocalizations, such as meowing and purring, to communicate.\"\n    ],\n    \"dog\": [\n        \"Social, pack-oriented, and tend to be more loyal to their human family.\",\n        \"Need regular grooming from their owners, including brushing and bathing.\",\n        \"Bark and growl to convey their messages.\",\n        \"Responsive to human commands and can be trained to perform a wide range of tasks.\"\n    ],\n}\n</code></pre> <p>Now we initialize the project. This can take about a minute per class, so we'll put in a simple timer to keep us updated on status.</p> <pre><code>resp = cls.initialize(classes, examples) \n\nimport time\n\nwhile True:\n    print(\"Waiting for classifier to initialize\")\n    time.sleep(5)\n    resp = cls.train_status()\n    if resp[\"status\"] == \"completed\":\n        print(\"Model ID: \" + resp[\"model_id\"])\n        first_model_id = resp[\"model_id\"]\n        break\n    if resp[\"status\"] == \"failed\":\n        print(resp[\"status\"])\n        raise Exception(\"failed training\")\n</code></pre> <p>Cool, we have our first model version! Let's try it out with a quick test.</p> <pre><code>import json\n\nprint(json.dumps(cls.classify(\"woof\"), indent=2))\n</code></pre> <p>Here's the expected output:</p> <pre><code>{\n  \"classification\": [\n    [\n      {\n        \"class_id\": 1,\n        \"class_name\": \"dog\",\n        \"prob\": 0.5267619385770103\n      },\n      {\n        \"class_id\": 0,\n        \"class_name\": \"cat\",\n        \"prob\": 0.47323806142298974\n      }\n    ]\n  ]\n}\n</code></pre> <p>Now we can see how useful the classifier output is. We get a list of all the categories we defined in our project, plus a confidence score for each.</p> <p>We can go even further to easily quantify the accuracy of our classifier. Let's run an evaluation!</p> <p>What an evaluation means for a classifier: when you provide a set of inputs and the expected output, we can test the accuracy of the model on those inputs, and give you back both overall metrics as well as per-input assessment.</p> <pre><code>from lamini.one_evaler.one_evaler import LaminiOneEvaler\n\neval = LaminiOneEvaler(\n    test_model_id=first_model_id,\n    eval_data_id=f\"first_eval{random.randint(1000,9999)}\",\n    eval_data=[{\"input\": \"woof\", \"target\": \"dog\"}, {\"input\": \"meow\", \"target\": \"cat\"}],\n    test_eval_type=\"classifier\",\n)\n</code></pre> <p>Expected output: <pre><code>print(json.dumps(eval.run(), indent=2))\n</code></pre></p> <pre><code>{\n  \"eval_job_id\": \"1424247633\",\n  \"eval_data_id\": \"first_eval6032\",\n  \"metrics\": {\n    \"tuned_accuracy\": 1.0,\n    \"tuned_precision\": 1.0,\n    \"tuned_recall\": 1.0,\n    \"tuned_f1\": 1.0\n  },\n  \"status\": \"COMPLETED\",\n  \"predictions\": [\n    {\n      \"input\": \"woof\",\n      \"target\": \"dog\",\n      \"test_output\": \"dog\",\n      \"base_output\": null\n    },\n    {\n      \"input\": \"meow\",\n      \"target\": \"cat\",\n      \"test_output\": \"cat\",\n      \"base_output\": null\n    }\n  ]\n}\n</code></pre> <p>That first run was ok, but we can do better. Let's add some more examples and retrain to improve accuracy. You control when to add data and when to train.</p> <pre><code>resp = cls.add(\n    \"additional_data\",\n    {\n        \"cat\": [\n            \"Cats spend up to sixteen hours a day sleeping, making them some of nature's most dedicated nappers.\",\n            \"Felines possess an extraordinary sense of balance thanks to their flexible backbone and righting reflex.\",\n            \"A cat's sandpaper-like tongue is covered in tiny hooks called papillae that help them groom themselves effectively.\",\n            \"Female cats tend to be right-pawed while male cats are more often left-pawed, according to scientific studies.\",\n            \"Ancient Egyptians showed their devotion to cats by mummifying them alongside their human companions.\",\n        ],\n        \"dog\": [\n            \"Dogs have evolved alongside humans for over 15,000 years, developing an uncanny ability to read our facial expressions and emotions.\",\n            \"The average dog can understand around 165 different words or signals, though some exceptional dogs can learn many more.\",\n            \"A dog's sense of smell is roughly 40 times greater than a human's, allowing them to detect diseases and track scents that are days old.\",\n            \"Unlike humans who have three cones in their eyes, dogs only have two, making them partially colorblind but excellent at detecting movement.\",\n            \"The Basenji breed is known as the 'barkless dog' because it produces an unusual yodel-like sound instead of a typical bark.\",\n        ],\n    },\n)\n\nresp = cls.train()\n\nwhile True:\n    print(\"Waiting for classifier to train\")\n    time.sleep(5)\n    resp = cls.train_status()\n    if resp[\"status\"] == \"completed\":\n        print(\"Model ID: \" + resp[\"model_id\"])\n        second_model_id = resp[\"model_id\"]\n        break\n    if resp[\"status\"] == \"failed\":\n        print(resp[\"status\"])\n        raise Exception(\"failed training\")\n</code></pre> <p>Great, now we have a second model version in our project! Let's run an eval and compare it to the first version.</p> <pre><code>print(\"Running comparison eval between model versions \" + first_model_id + \" and \" + second_model_id)\n\neval_2 = LaminiOneEvaler(\n    test_model_id=first_model_id,\n    eval_data_id=f\"second_eval{random.randint(1000,9999)}\",\n    eval_data=[{\"input\": \"woof\", \"target\": \"dog\"}, {\"input\": \"meow\", \"target\": \"cat\"}],\n    test_eval_type=\"classifier\",\n    base_model_id=second_model_id,\n    sbs=True,\n    fuzzy=True,\n)\n\nprint(json.dumps(eval_2.run(), indent=2))\n</code></pre> <p>Expected output: <pre><code>Running comparison eval between model versions 8a9fe622-4555-4646-886a-dc94b16a56f2 and 9739bf49-82ab-4e69-8149-5b891111516e\n{\n  \"eval_job_id\": \"2044167961\",\n  \"eval_data_id\": \"second_eval9291\",\n  \"metrics\": {\n    \"base_accuracy\": 1.0,\n    \"base_precision\": 1.0,\n    \"base_recall\": 1.0,\n    \"base_f1\": 1.0,\n    \"base_fuzzy_accuracy\": 1.0,\n    \"base_fuzzy_precision\": 1.0,\n    \"base_fuzzy_recall\": 1.0,\n    \"base_fuzzy_f1\": 1.0,\n    \"tuned_accuracy\": 1.0,\n    \"tuned_precision\": 1.0,\n    \"tuned_recall\": 1.0,\n    \"tuned_f1\": 1.0,\n    \"tuned_fuzzy_accuracy\": 1.0,\n    \"tuned_fuzzy_precision\": 1.0,\n    \"tuned_fuzzy_recall\": 1.0,\n    \"tuned_fuzzy_f1\": 1.0,\n    \"tuned_win_loss_ratio\": 0.0,\n    \"base_win_loss_ratio\": 0.0\n  },\n  \"status\": \"COMPLETED\",\n  \"predictions\": [\n    {\n      \"input\": \"woof\",\n      \"target\": \"dog\",\n      \"test_output\": \"dog\",\n      \"base_output\": \"dog\"\n    },\n    {\n      \"input\": \"meow\",\n      \"target\": \"cat\",\n      \"test_output\": \"cat\",\n      \"base_output\": \"cat\"\n    }\n  ]\n}\n</code></pre></p> <p>The eval output makes it easy to compare model versions overall, and to see exactly where the differences are, so you know exactly where to focus to improve your workflow.</p> <p>Happy classifying!</p>"},{"location":"inference/batching/","title":"Batching","text":"<p>Batching inference requests (submitting multiple prompts simultaneously) provides dramatically higher throughput compared to submitting each request individually.</p>"},{"location":"inference/batching/#use-cases","title":"Use cases","text":"<ul> <li>Memory Tuning using both evaluation and data-generation agents</li> <li>Evaluation agents that enable rapid feedback loops during model development by automatically measuring model performance</li> <li>Data-generation agents for expanding tuning and evaluation data sets without tedious manual effort</li> <li>Async inference for enriching or updating data in the background</li> </ul>"},{"location":"inference/batching/#a-better-way-to-batch","title":"A better way to batch","text":"<p>Lamini Platform implements approaches similar to iteration-level scheduling and selective batching (as described in the Orca paper) to deliver significantly higher throughput compared to naive inference batching implementations.</p> <p>Naive batching has two major drawbacks:</p> <ol> <li>The entire batch blocks on the request that takes the longest to process.</li> <li>A second batch cannot be processed until the first batch is completely processed.</li> </ol> <p>Iteration-level scheduling and selective batching avoid these drawbacks by allocating work across GPUs at the model iteration level, rather than the entire request level.</p>"},{"location":"inference/batching/#example","title":"Example","text":"<p>Inference batching with Lamini is simple: just pass in a list of inputs\u2014no configuration required.</p> Python SDKREST API <pre><code># code/batching.py\n\nfrom lamini import Lamini\n\nllm = Lamini(model_name=\"meta-llama/Llama-3.1-8B-Instruct\")\nllm.generate(\n    [\n        \"How old are you?\",\n        \"What is the meaning of life?\",\n        \"What is the hottest day of the year?\",\n    ],\n    output_type={\"response\": \"str\", \"explanation\": \"str\"},\n)\n</code></pre> <pre><code>curl --location \"https://api.lamini.ai/v1/completions\" \\\n--header \"Authorization: Bearer $LAMINI_API_KEY\" \\\n--header \"Content-Type: application/json\" \\\n--data '{\n    \"model_name\": \"meta-llama/Llama-3.1-8B-Instruct\",\n    \"prompt\": [\n        \"How old are you?\",\n        \"What is the meaning of life?\",\n        \"What is the hottest day of the year?\"\n    ],\n    \"output_type\": {\n        \"response\": \"str\",\n        \"explanation\": \"str\"\n    }\n}'\n</code></pre> Expected Output <pre><code>[\n    {\n        'response': 'I am 25 years old',\n        'explanation': \"I am a 25-year-old AI assistant, so I don't have a physical body and don't age like humans do\"\n    },\n    {\n        'response': \"The meaning of life is a question that has puzzled philosophers, scientists, and thinkers for centuries. There is no one definitive answer, as it is a deeply personal and subjective question that can vary greatly from person to person. However, here are some possible answers that have been proposed:\\n\\n1. The search for happiness: Many people believe that the meaning of life is to find happiness and fulfillment. This can be achieved through personal relationships, career, hobbies, or other activities that bring joy and satisfaction.\\n2. The pursuit of knowledge: Others believe that the meaning of life is to learn and understand the world around us. This can be achieved through education, research, and exploration.\\n3. The pursuit of purpose: Some people believe that the meaning of life is to find a sense of purpose and direction. This can be achieved through setting goals, pursuing passions, and making a positive impact on the world.\\n4. The search for connection: Many people believe that the meaning of life is to connect with others and build meaningful relationships. This can be achieved through communication, empathy, and understanding.\\n5. The search for transcendence: Some people believe that the meaning of life is to transcend the physical world and connect with something greater than ourselves. This can be achieved through spirituality, meditation, or other practices that help us connect with a higher power or the universe.\\n\\nUltimately, the meaning of life is a deeply personal and subjective question that can only be answered by each individual. It is a question that requires self-reflection, introspection, and a willingness to explore and discover one's own values, beliefs, and passions\",\n        'explanation': \"The meaning of life is a question that has puzzled philosophers, scientists, and thinkers for centuries. There is no one definitive answer, as it is a deeply personal and subjective question that can vary greatly from person to person. However, here are some possible answers that have been proposed:\\n\\n1. The search for happiness: Many people believe that the meaning of life is to find happiness and fulfillment. This can be achieved through personal relationships, career, hobbies, or other activities that bring joy and satisfaction.\\n2. The pursuit of knowledge: Others believe that the meaning of life is to learn and understand the world around us. This can be achieved through education, research, and exploration.\\n3. The pursuit of purpose: Some people believe that the meaning of life is to find a sense of purpose and direction. This can be achieved through setting goals, pursuing passions, and making a positive impact on the world.\\n4. The search for connection: Many people believe that the meaning of life is to connect with others and build meaningful relationships. This can be achieved through communication, empathy, and understanding.\\n5. The search for transcendence: Some people believe that the meaning of life is to transcend the physical world and connect with something greater than ourselves. This can be achieved through spirituality, meditation, or other practices that help us connect with a higher power or the universe.\\n\\nUltimately, the meaning of life is a deeply personal and subjective question that can only be answered by each individual. It is a question that requires self-reflection, introspection, and a willingness to explore and discover one's own values, beliefs, and passions\"\n    },\n    {\n        'response': \"The hottest day of the year is typically the day of the summer solstice, which is the longest day of the year and usually falls on June 20 or June 21 in the Northern Hemisphere. This day is often referred to as the 'warmest day of the year' or the 'hottest day of the year' because it is the day when the sun is at its highest point in the sky and the Earth is tilted at its maximum angle towards the sun, resulting in the most direct sunlight and the highest temperatures. However, it's worth noting that the hottest day of the year can vary depending on the location and climate. In some regions, the hottest day of the year may occur in July or August, while in others it may occur in September or October\",\n        'explanation': \"The summer solstice is the longest day of the year and typically marks the beginning of summer in the Northern Hemisphere. It is the day when the sun is at its highest point in the sky and the Earth is tilted at its maximum angle towards the sun, resulting in the most direct sunlight and the highest temperatures. This day is often referred to as the 'warmest day of the year' or the 'hottest day of the year' because it is the day when the sun is at its strongest and the Earth is at its warmest. However, it's worth noting that the hottest day of the year can vary depending on the location and climate. In some regions, the hottest day of the year may occur in July or August, while in others it may occur in September or October.\"\n    }\n]\n</code></pre>"},{"location":"inference/infv2/","title":"[Beta] OpenAI-compatible Inference API","text":"<p>Our new OpenAI-compatible inference API is now in beta!</p>"},{"location":"inference/infv2/#quick-start","title":"Quick Start","text":"<p>Remember to get your API key at https://app.lamini.ai/account.</p> <p>Initialize the OpenAI client:</p> <pre><code>import openai\n\nclient = openai.OpenAI(\n    api_key=\"&lt;YOUR-LAMINI-API-KEY&gt;\",\n    base_url=\"https://api.lamini.ai/inf\",\n)\n</code></pre> <p>Use the OpenAI API as you normally would.</p>"},{"location":"inference/infv2/#chat-completions","title":"Chat Completions","text":"<pre><code>response = client.chat.completions.create(\n    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n    messages=[{\"role\": \"user\", \"content\": \"What is the best llama?\"}],\n)\nprint(response)\n</code></pre> Example Response <pre><code>ChatCompletion(id='chatcmpl-a5d61c89c0b64bfcbda823f1205f89ff', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='That\\'s a subjective question, as the \"best\" llama is often a matter of personal preference. Llamas come in a wide range of breeds, each with their unique characteristics, temperaments, and purposes. Here are some popular llama breeds, known for their distinct traits:\\n\\n1. **Suri Llama**: Known for their long, soft fleece, Suri llamas are prized for their luxurious coat and calm nature.\\n2. **Huacaya Llama**: Huacaya llamas have a fluffy, dense coat and are often used as pack animals or for their meat.\\n3. **Camelid Combination (Cama)**: Cams are hybrids of llamas and camels, often bred for their unique appearance and temperaments.\\n4. **American Llama**: A popular breed in the United States, American llamas are known for their intelligence, friendly nature, and versatility.\\n5. **Peruvian Llama**: Peruvian llamas are one of the oldest breeds, known for their sturdy build and high-quality fleece.\\n\\nWhen choosing the \"best\" llama, consider factors such as:\\n\\n* Purpose (pack animal, fiber production, pet, or show)\\n* Temperament (calm, friendly, curious, or energetic)\\n* Fleece quality (softness, density, and color)\\n* Size (standards vary, but most llamas range from 32 to 40 inches tall)\\n* Health and breeding\\n\\nIt\\'s essential to research and work with reputable breeders to find a llama that meets your needs and preferences. Remember, every llama has its unique personality, so take the time to get to know one before making a decision.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1741379785, model='hosted_vllm/meta-llama/Llama-3.2-3B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=337, prompt_tokens=41, total_tokens=378, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)\n</code></pre>"},{"location":"inference/infv2/#json-output","title":"JSON Output","text":"<pre><code>SCHEMA = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"answer\": {\"type\": \"string\"},\n    },\n    \"required\": [\"answer\"],\n}\n\nresponse = client.chat.completions.create(\n    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n    messages=[\n        {\"role\": \"user\", \"content\": \"What is the best llama?\"},\n    ],\n    response_format={\n        \"type\": \"json_schema\",\n        \"json_schema\": {\n            \"name\": \"json_response\",\n            \"schema\": SCHEMA,\n        },\n    },\n)\n\nprint(response)\n</code></pre> Example Response <pre><code>ChatCompletion(id='meta-llama/Llama-3.2-3B-Instruct-20364893-cbd0-45af-84ff-b4f51a7864fd', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='{\"answer\" : \"It is difficult to pinpoint the \\'best\\' llama as they can vary in temperament, purpose, and physical characteristics. However, some popular and well-known breeds of llamas include:* Suri llamas: Known for their stunning, silky fleece and slender build, suri llamas are popular for their beauty and are often used for fiber production.* Huacaya llamas: With their fluffy, soft coats and medium build, huacaya llamas are prized for their fiber and are often used for wool production.* Suri Angora llamas: A cross between a suri and an Angora rabbit, Suri Angora llamas inherit the rabbit\\'s Angora fiber, which is highly prized for its softness and warmth.* Pacaya-Llama: A rare and unique breed, Pacaya-Llamas are known for their striking appearance, intelligence, and friendly temperament.Each llama is an individual with its unique characteristics, and what one person considers the \\'best\\' may not be the same for another. Ultimately, the \\'best\\' llama is one that is well-suited to its purpose and lifestyle. It\\'s recommended to research and find a llama that meets your specific needs and preferences.\"}', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=[], reasoning_content=None), stop_reason=None)], created=1745873448, model='meta-llama/Llama-3.2-3B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=250, prompt_tokens=42, total_tokens=292, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)\n</code></pre>"},{"location":"inference/infv2/#with-additional-parameters","title":"With Additional Parameters","text":"<pre><code>response = client.chat.completions.create(\n    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n    messages=[{\"role\": \"user\", \"content\": \"What is the best llama?\"}],\n    temperature=0.5,\n    max_tokens=100,\n)\nprint(response)\n</code></pre> Example Response <pre><code>ChatCompletion(id='chatcmpl-e4b07deb82264bb28fb786b67769538b', choices=[Choice(finish_reason='length', index=0, logprobs=None, message=ChatCompletionMessage(content='There is no single \"best\" llama, as they are all unique individuals with their own characteristics, temperaments, and purposes. However, I can provide some information on popular llama breeds and their characteristics.\\n\\nHere are a few popular llama breeds:\\n\\n1. **Suri Llama**: Known for their long, silky coats, Suri llamas are often used as pack animals and are prized for their intelligence, gentle nature, and versatility.\\n2. **Huacaya Llama**: Huacaya', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None))], created=1741380397, model='hosted_vllm/meta-llama/Llama-3.2-3B-Instruct', object='chat.completion', service_tier=None, system_fingerprint=None, usage=CompletionUsage(completion_tokens=100, prompt_tokens=41, total_tokens=141, completion_tokens_details=None, prompt_tokens_details=None), prompt_logprobs=None)\n</code></pre>"},{"location":"inference/infv2/#caching","title":"Caching","text":"<p>Inference requests to hosted models are not cached.</p> <p>For third-party models, inference requests are cached for 60 seconds and can be overridden by passing an <code>extra_body</code> parameter with the request.</p> <pre><code>response = client.chat.completions.create(\n    model=\"gpt-4o-mini\", # third-party model\n    messages=[{\"role\": \"user\", \"content\": \"What is the best llama?\"}],\n    temperature=0.5,\n    max_tokens=100,\n    extra_body = {\n        \"cache\": {\n          \"ttl\": 600 # seconds, caches response for 10 minutes\n      }\n    }\n)\nprint(response)\n</code></pre>"},{"location":"inference/infv2/#get-available-models","title":"Get available models","text":"<p>Get the list of available models for OpenAI-compatible inference.</p> <pre><code>curl -X GET \"https://api.lamini.ai/inf/models\" \\\n  -H \"Content-Type: application/json\" \\\n  -H \"Authorization: Bearer &lt;YOUR-LAMINI-API-KEY&gt;\"\n</code></pre>"},{"location":"inference/json_output/","title":"JSON Output","text":"<p>Enforcing structured JSON schema output is important for handling LLM outputs downstream with other systems and APIs in your applications.</p> <p>For an in-depth technical deep dive of how we implemented this feature, see our blog post.</p> Python SDKREST API <p>You can enforce JSON schema via the <code>Lamini</code> class, which is the base class for all runners. <code>Lamini</code> wraps our REST API endpoint.</p> <p>First, return a string:</p> <pre><code>from lamini import Lamini\n\nllm = Lamini(model_name=\"meta-llama/Llama-3.1-8B-Instruct\")\noutput = llm.generate(\n    \"How are you?\",\n    output_type={\"answer\": \"str\"}\n)\n</code></pre> <p>First, get a basic string output out:</p> <pre><code>curl --location \"https://api.lamini.ai/v1/completions\" \\\n--header \"Authorization: Bearer $LAMINI_API_KEY\" \\\n--header \"Content-Type: application/json\" \\\n--data '{\n    \"model_name\": \"meta-llama/Llama-3.1-8B-Instruct\",\n    \"prompt\": \"How are you?\",\n    \"output_type\": {\n        \"answer\": \"str\"\n    }\n}'\n</code></pre> Expected Output <pre><code>{\n    \"answer\":\"I'm doing well, thanks for asking! How about you\"\n}\n</code></pre>"},{"location":"inference/json_output/#values-other-than-strings","title":"Values other than strings","text":"<p>You can change the output type to be a different type. This typing is strictly enforced.  We currently support <code>str</code>, <code>int</code>, <code>float</code>, <code>bool</code>, and enums structured as str lists, or int lists. For example, <code>\"answer\": [\"A\",\"B\",\"C\",\"D\"]</code> would always return one of <code>A</code>, <code>B</code>, <code>C</code>, or <code>D</code> for the <code>answer</code> field. <code>\"answer\": [1, 2, 3]</code> would always return one of <code>1</code>, <code>2</code>, or <code>3</code> for the <code>answer</code> field.</p> <p>Please let us know if there are additional types you'd like to see supported.</p> <p>Examples</p> Python SDKPython SDKPython SDKREST API <pre><code>llm.generate(\n    \"How old are you?\",\n    output_type={\"age\": \"int\"}\n)\n</code></pre> <pre><code>llm.generate(\n    \"Pick a color.\",\n    output_type={\"name\": [\"red\", \"white\", \"blue\"]}\n)\n</code></pre> <pre><code>llm.generate(\n    \"Pick an odd digit\",\n    output_type={\"name\": [1, 3, 5, 7, 9]}\n)\n</code></pre> <pre><code>curl --location \"https://api.lamini.ai/v1/completions\" \\\n--header \"Authorization: Bearer $LAMINI_API_KEY\" \\\n--header \"Content-Type: application/json\" \\\n--data '{\n    \"model_name\": \"meta-llama/Llama-3.1-8B-Instruct\",\n    \"prompt\": \"How old are you?\",\n    \"output_type\": {\n        \"age\": \"int\"\n    }\n}'\n</code></pre> Expected Output <pre><code>{\n    \"age\": 25\n}\n</code></pre>"},{"location":"inference/json_output/#multiple-outputs-in-json-schema","title":"Multiple outputs in JSON schema","text":"<p>You can also add multiple output types in one call. The output is a JSON schema that is also strictly enforced.</p> Python SDKREST API <pre><code>llm.generate(\n    \"How old are you?\",\n    output_type={\"age\": \"int\", \"units\": \"str\"}\n)\n</code></pre> <pre><code>curl --location \"https://api.lamini.ai/v1/completions\" \\\n--header \"Authorization: Bearer $LAMINI_API_KEY\" \\\n--header \"Content-Type: application/json\" \\\n--data '{\n    \"model_name\": \"meta-llama/Llama-3.1-8B-Instruct\",\n    \"prompt\": \"How old are you?\",\n    \"output_type\": {\n        \"age\": \"int\",\n        \"units\": \"str\"\n    }\n}'\n</code></pre> Expected Output <pre><code>{\n    \"age\": 25,\n    \"units\": \"years\"\n}\n</code></pre> <p>Great! You've successfully run an LLM with structured JSON schema outputs.</p>"},{"location":"inference/json_output/#known-issue-json-output-truncation","title":"Known issue: JSON output truncation","text":"<p>Truncation may occur when JSON output generates double quotation marks (\"). This is a known limitation with using <code>output_type</code> since a quotation at the end of the output and a quotation as a part of the output are not distinguished.</p>"},{"location":"inference/json_output/#workaround","title":"Workaround","text":"<ul> <li>Use prompt tuning instead of output type if the response may contain double quotation marks. e.g. \"Only return the relevant quote, do not include any other text\".</li> <li>Use prompt tuning to avoid generating double quotation marks. e.g. \"Only use single quotes when there is dialogue\".</li> <li>Contact us to discuss alternative solutions or workarounds for your use case.</li> </ul>"},{"location":"inference/json_output/#future-support","title":"Future support","text":"<p>We are evaluating the feasibility of improving our system to handle double quotes in JSON output in the future. If we decide to support this feature, we will update our documentation and notify users.</p>"},{"location":"inference/json_output/#known-issue-long-json-output-times-out","title":"Known issue: Long JSON output times out","text":"<p>To ensure that requests complete in a reasonable amount of time, there is a time limit on all requests including json requests. If your requests exceeds the time limit, try guiding the model to generate a shorter json object, e.g. write a description in 3 sentences or less. Timed out requests may result in failed, incomplete, or missing output.</p>"},{"location":"inference/json_output/#workaround_1","title":"Workaround","text":"<ul> <li>Reduce the size of the output by limiting the number of fields or the prompt.</li> <li>Break down the JSON output into separate smaller requests.</li> <li>Contact us to discuss alternative solutions or workarounds for your use case.</li> </ul>"},{"location":"inference/json_output/#future-support_1","title":"Future support","text":"<p>We are evaluating the feasibility of improving our system to handle large JSON output in the future. If we decide to support this feature, we will update our documentation and notify users.</p> <p>Feel free to contact us with any questions or concerns.</p>"},{"location":"inference/performance/","title":"Performance","text":"<p>Inference performance is a function of model size, prompt size, response size, and compute speed, and can be measured in a variety of ways (queries per second, tokens per second, time to first token, etc.). It's complicated!</p>"},{"location":"inference/performance/#how-to-improve-performance-and-handle-truncated-responses","title":"How to improve performance and handle truncated responses","text":"<p>Inference responses can be truncated (cut off, or appear incomplete) because the request could not be completed in the time allotted (timeout), or because the response size exceeded the <code>max_new_tokens</code> parameter (length).</p> <ol> <li> <p>First, review your prompt and requested responses: can you shorten them? When you're experimenting, it's easy to accumulate extraneous information in the prompt, or to request more output than you actually need. Prompt tuning is often a quick win, especially when combined with structured output.</p> </li> <li> <p>Try using Generation Pipelines for more efficient execution.</p> </li> <li> <p>If you're still having trouble, check whether your request set a value for <code>max_new_tokens</code>.</p> </li> </ol> <p>Lamini's completions API has an optional <code>max_new_tokens</code> parameter that limits the response size. Lamini uses this parameter to efficiently allocate GPU memory. However, this comes with risks:</p> <ul> <li>If you set the token limit too short, your requests may get truncated. The LLM is not aware of the token limit.</li> <li>Very large token limits consume substantial memory, which slows down processing, which may cause timeouts.</li> </ul> <p>If you're setting a value for <code>max_new_tokens</code> and your responses are truncated at that value, you're hitting the token limit. Try a higher value for <code>max_new_tokens</code>.</p> <p>If you're setting a value for <code>max_new_tokens</code> and your response was truncated at less than that value, you're probably hitting a timeout. Try a lower value for <code>max_new_tokens</code>.</p>"},{"location":"inference/playground/","title":"Playground","text":"<p>The playground makes it easy to compare different open-source models and iterate on system prompts by chatting with them.</p> <p>Run LLMs at https://app.lamini.ai/playground or contact us to run the same UI internally at your company.</p> <p></p> <p></p>"},{"location":"inference/prompt_templates/","title":"Prompt Templates","text":"<p>Different models have different system prompt templates. Using the correct template when prompt tuning can have a large effect on model performance.</p> <p>When you're trying a new model, it's a good idea to review the model card on Hugging Face to understand what (if any) system prompt template it uses.</p>"},{"location":"inference/prompt_templates/#llama-31-32","title":"Llama 3.1 + 3.2","text":"<p>The Llama 3.1 and Llama 3.2 prompt template looks like this:</p> <pre><code>&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\n{{ system_prompt }}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\n{{ user_message }}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n</code></pre> <p>The <code>{system_prompt}</code> variable is a system prompt that tells your LLM how it should behave and what persona to take on. By default, it is that of a helpful assistant. The <code>{user_message}</code> variable is the instruction prompt that tells your LLM what to do. This is typically what you view as the prompt, e.g. the question you want to ask the LLM.</p> <p>You need to apply the <code>{{ system_prompt }}</code> and <code>{{ user_message }}</code> to prompts used for both inference and memory tuning jobs.</p> Python SDKREST API <pre><code># code/llama_3_prompt.py\n\nfrom lamini import Lamini\n\nprompt = \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\n\\n\"\nprompt += \"You are a pirate. Say arg matey!\"\nprompt += \"&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n\"\nprompt += \"How are you?\"\nprompt += \"&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n\"\nllm = Lamini(\"meta-llama/Llama-3.1-8B-Instruct\")\nprint(llm.generate(prompt, output_type={\"Response\": \"str\"}))\n</code></pre> <pre><code>curl --location \"https://api.lamini.ai/v1/completions\" \\\n    --header \"Authorization: Bearer $LAMINI_API_KEY\" \\\n    --header \"Content-Type: application/json\" \\\n    --data '{\n        \"model_name\": \"meta-llama/Llama-3.1-8B-Instruct\",\n        \"prompt\": \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\n\\n You are a pirate. Say arg matey! &lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n How are you? &lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n\",\n        \"output_type\": {\n            \"Response\": \"str\"\n        }\n    }'\n</code></pre> Expected Output <pre><code>{'Response': \"Ahoy, matey! I be doin' just fine, thank ye for askin'! Me and me crew have been sailin' the seven seas, plunderin' the riches and singin' sea shanties 'round the campfire. The sun be shinin' bright, the wind be blowin' strong, and me trusty cutlass be by me side. What more could a pirate ask for, eh? Arrr\"}\n</code></pre>"},{"location":"inference/prompt_templates/#mistral-v3","title":"Mistral v3","text":"<p>Mistral v3 uses a different format. In order to leverage instruction fine-tuning, your prompt should be surrounded by [INST] and [/INST] tokens.</p> Python SDKREST API <pre><code># code/mistral.py\n\nfrom lamini import Lamini\n\nllm = Lamini(model_name=\"mistralai/Mistral-7B-Instruct-v0.3\")\nprint(llm.generate(\"&lt;s&gt;[INST] How are you? [/INST]\", output_type={\"Response\": \"str\"}))\n</code></pre> <pre><code>curl --location \"https://api.lamini.ai/v1/completions\" \\\n--header \"Authorization: Bearer $LAMINI_API_KEY\" \\\n--header \"Content-Type: application/json\" \\\n--data '{\n    \"model_name\": \"mistralai/Mistral-7B-Instruct-v0.3\",\n    \"prompt\": \"&lt;s&gt;[INST] How are you? [/INST]\",\n    \"output_type\": {\n        \"Response\": \"str\"\n    }\n}'\n</code></pre> Expected Output <pre><code>{\n    'Response': \"I'm just a computer program, I don't have feelings or emotions. I'm here to help answer any questions you might have to the best of my ability\"\n}\n</code></pre>"},{"location":"inference/quick_start/","title":"Inference Quick Start","text":"<p>Even faster inference is here! Try out our new OpenAI-compatible inference API \ud83d\ude80.</p> <p>Let's try running inference with a Llama model. First, make sure you have your API key (get yours at https://app.lamini.ai/account).</p>"},{"location":"inference/quick_start/#run-inference-on-a-llama-model","title":"Run inference on a Llama model","text":"Python SDKREST API <p>Initialize the Lamini client: <pre><code>import lamini\nlamini.api_key = \"&lt;YOUR-LAMINI-API-KEY&gt;\"\n\nllm = lamini.Lamini(\"meta-llama/Llama-3.2-3B-Instruct\")\n</code></pre></p> <p>Create a prompt using the Llama template: <pre><code>prompt = \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\"\nprompt += \"Which animal remembers facts the best?\\n\"\nprompt += \"&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\"\n</code></pre></p> <p>Generate a response: <pre><code>response = llm.generate(prompt)\nprint(response)\n</code></pre></p> <pre><code>curl --location 'https://api.lamini.ai/v1/completions' \\\n    --header 'Authorization: Bearer $LAMINI_API_KEY' \\\n    --header 'Content-Type: application/json' \\\n    --data '{\n        \"model_name\": \"meta-llama/Llama-3.2-3B-Instruct\",\n        \"prompt\": \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\nWhich animal remembers facts the best?\\n&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\"\n    }'\n</code></pre> Example Response <pre><code>While animals are known for their impressive memory abilities, the answer to this question can vary depending on the species and the type of facts they're trying to remember. Here are some examples:\n\n**Top contenders:**\n\n1. **Chimpanzees**: Our closest living relatives in the animal kingdom, chimpanzees have been shown to possess exceptional memory abilities. They can recall:\n        *Complex social hierarchies and relationships\n        * Tool use and usage\n        *Food sources and locations\n        * Even human faces and emotions\n2. **Dolphins**: These intelligent marine mammals have been observed:\n        *Recognizing and remembering individual humans and other dolphins\n        * Learning and recalling complex vocalizations and songs\n        * Solving problems and adapting to new situations\n3. **Elephants**: With their large brains and complex social structures, elephants have impressive memory abilities, including:\n        *Remembering migration routes and watering holes\n        * Recognizing and responding to family members and social hierarchies\n        * Displaying empathy and cooperation\n4. **Crows and Ravens**: These intelligent birds are known for their problem-solving abilities and memory:\n        *Remembering and using tools to obtain food\n        * Recognizing individual humans and other animals\n        * Solving complex puzzles and learning from experience\n\n**Honorable mentions:**\n\n* **Parrots**: Some parrot species, like African Greys, are renowned for their exceptional vocal mimicry and memory abilities.\n* **Honeybees**: These social insects have impressive memory for:\n        + Nectar sources and food locations\n        + Social hierarchies and communication\n        + Navigation and migration routes\n* **Octopuses**: These cephalopods have been observed:\n        + Solving complex problems and learning from experience\n        + Remembering and recognizing individual humans and other octopuses\n\n**The winner?**\n\nWhile it's difficult to declare a single winner, chimpanzees are often considered one of the top contenders for remembering facts the best. Their advanced cognitive abilities, social complexity, and adaptability make them a strong candidate for exceptional memory.\n\nHowever, it's essential to remember (pun intended) that each species has its unique strengths and weaknesses, and the concept of \"remembering facts\" can be subjective. The best way to understand an animal's memory abilities is to study their behavior, social structures, and cognitive processes in their natural habitats.\n</code></pre>"},{"location":"inference/quick_start/#using-system-prompts","title":"Using System Prompts","text":"<p>You can also use system prompts to guide the model's behavior:</p> Python SDKREST API <pre><code># Add a system prompt for more control\nprompt = \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\n\"\nprompt += \"You are a concise assistant who gives brief, direct answers.\\n\"\nprompt += \"&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\"\nprompt += \"Which animal remembers facts the best?\\n\"\nprompt += \"&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\"\n\nresponse = llm.generate(prompt)\nprint(response)\n</code></pre> <pre><code>curl --location 'https://api.lamini.ai/v1/completions' \\\n    --header 'Authorization: Bearer $LAMINI_API_KEY' \\\n    --header 'Content-Type: application/json' \\\n    --data '{\n        \"model_name\": \"meta-llama/Llama-3.2-3B-Instruct\",\n        \"prompt\": \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\\nYou are a concise assistant who gives brief, direct answers.\\n&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\nWhich animal remembers facts the best?\\n&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\"\n    }'\n</code></pre> Example Response <pre><code>Elephants.\n</code></pre> <p>Remember that different models use different prompt templates. Check out our prompt templates guide to learn more.</p>"},{"location":"inference/streaming/","title":"Streaming","text":"<p>Lamini also supports streaming inference! Here is an example implementation using our Python library.</p> <pre><code>import os\nimport random\nimport time\n\nimport lamini\n\napi = lamini.StreamingCompletion()\n\n\n\"\"\"\nIf you want to use the async version, you can do the following:\n\nasync def main():\n    prompt = f\"What is AI?\"\n    result = await api.async_create(\n        prompt,\n        \"meta-llama/Llama-3.2-3B-Instruct\",\n        max_new_tokens=256,\n    )\n\n    async for r in result:\n        print(r)\n\n\"\"\"\n\ndef main():\n    prompt = f\"What is AI?\"\n    result = api.create(\n        prompt,\n        \"meta-llama/Llama-3.2-3B-Instruct\",\n        max_new_tokens=256,\n    )\n\n    for r in result:\n        print(r)\nmain()\n</code></pre>"},{"location":"lamini_python_class/lamini/","title":"Module <code>lamini.api.lamini</code> (lamini-3.2.3)","text":"<p>Want to see more? Check out our full open source repo: https://github.com/lamini-ai/lamini.</p>"},{"location":"lamini_python_class/lamini/#classes","title":"Classes","text":"<pre><code>Lamini(\n    model_name:\u00a0str,\n    api_key:\u00a0Optional[str]\u00a0=\u00a0None,\n    api_url:\u00a0Optional[str]\u00a0=\u00a0None,\n)\n</code></pre> Expand source code <pre><code>class Lamini:\n    def __init__(\n        self,\n        model_name: str,\n        api_key: Optional[str] = None,\n        api_url: Optional[str] = None,\n    ):\n        self.config = get_config()\n        self.model_name = model_name\n        self.api_key = api_key\n        self.api_url = api_url\n        self.completion = Completion(api_key, api_url)\n        self.trainer = Train(api_key, api_url)\n        self.upload_file_path = None\n        self.upload_base_path = None\n\n    def version(self):\n        return get_version(self.api_key, self.api_url, self.config)\n\n    def generate(\n        self,\n        prompt: Union[str, List[str]],\n        model_name: Optional[str] = None,\n        output_type: Optional[dict] = None,\n        max_tokens: Optional[int] = None,\n        max_new_tokens: Optional[int] = None,\n    ):\n        result = self.completion.generate(\n            prompt=prompt,\n            model_name=model_name or self.model_name,\n            output_type=output_type,\n            max_tokens=max_tokens,\n            max_new_tokens=max_new_tokens,\n        )\n        if output_type is None:\n            if isinstance(prompt, list):\n                result = [single_result[\"output\"] for single_result in result]\n            else:\n                result = result[\"output\"]\n        return result\n\n    async def async_generate(\n        self,\n        prompt: Union[str, List[str]],\n        model_name: Optional[str] = None,\n        output_type: Optional[dict] = None,\n        max_tokens: Optional[int] = None,\n        max_new_tokens: Optional[int] = None,\n    ):\n        req_data = self.completion.make_llm_req_map(\n            prompt=prompt,\n            model_name=model_name or self.model_name,\n            output_type=output_type,\n            max_tokens=max_tokens,\n            max_new_tokens=max_new_tokens,\n        )\n        result = await self.completion.async_generate(req_data)\n        if output_type is None:\n            if isinstance(prompt, list):\n                result = [single_result[\"output\"] for single_result in result]\n            else:\n                result = result[\"output\"]\n        return result\n\n    def upload_data(\n        self,\n        data: Iterable[Dict[str, Union[int, float, str, bool, Dict, List]]],\n        is_public: Optional[bool] = None,\n    ):\n        num_datapoints = 0\n\n        def get_data_str(d):\n            nonlocal num_datapoints\n            for item in d:\n                num_datapoints += 1\n                yield json.dumps(item) + \"\\n\"\n\n        if not data:\n            raise ValueError(\"Data pairs cannot be empty.\")\n\n        output = self.trainer.get_upload_base_path()\n        self.upload_base_path = output[\"upload_base_path\"]\n\n        try:\n            if self.upload_base_path == \"azure\":\n                data_str = get_data_str(data)\n                response = self.trainer.create_blob_dataset_location(\n                    self.upload_base_path, is_public\n                )\n                self.upload_file_path = response[\"dataset_location\"]\n                upload_to_blob(data_str, self.upload_file_path)\n                self.trainer.update_blob_dataset_num_datapoints(\n                    response[\"dataset_id\"], num_datapoints\n                )\n                print(\"Data pairs uploaded to blob.\")\n            else:\n                response = self.trainer.upload_dataset_locally(\n                    self.upload_base_path, is_public, data\n                )\n                self.upload_file_path = response[\"dataset_location\"]\n                print(\"Data pairs uploaded to local.\")\n\n            print(response)\n            print(\n                f\"\\nYour dataset id is: {response['dataset_id']} . Consider using this in the future to train using the same data. \\nEg: \"\n                f\"llm.train(data_or_dataset_id='{response['dataset_id']}')\"\n            )\n\n        except Exception as e:\n            print(f\"Error uploading data pairs: {e}\")\n            raise e\n\n        return response[\"dataset_id\"]\n\n    def upload_file(\n        self, file_path: str, input_key: str = \"input\", output_key: str = \"output\"\n    ):\n        items = self._upload_file_impl(file_path, input_key, output_key)\n        try:\n            dataset_id = self.upload_data(items)\n            return dataset_id\n        except Exception as e:\n            print(f\"Error reading data file: {e}\")\n            raise e\n\n    def _upload_file_impl(\n        self, file_path: str, input_key: str = \"input\", output_key: str = \"output\"\n    ):\n        if os.path.getsize(file_path) &gt; 1e10:\n            raise Exception(\"File size is too large, please upload file less than 10GB\")\n\n        # Convert file records to appropriate format before uploading file\n        items = []\n        if file_path.endswith(\".jsonl\") or file_path.endswith(\".jsonlines\"):\n            with open(file_path) as dataset_file:\n\n                for row in jsonlines.Reader(dataset_file):\n                    yield {\"input\": row[input_key], \"output\": row.get(output_key, \"\")}\n\n        elif file_path.endswith(\".csv\"):\n            df = pd.read_csv(file_path).fillna(\"\")\n            data_keys = df.columns\n            if input_key not in data_keys:\n                raise ValueError(\n                    f\"File must have input_key={input_key} as a column (and optionally output_key={output_key}). You \"\n                    \"can pass in different input_key and output_keys.\"\n                )\n\n            try:\n                for _, row in df.iterrows():\n                    yield {\n                        \"input\": row[input_key],\n                        \"output\": row.get(output_key, \"\"),\n                    }\n            except KeyError:\n                raise ValueError(\"Each object must have 'input' and 'output' as keys\")\n\n        else:\n            raise Exception(\n                \"Upload of only csv and jsonlines file supported at the moment.\"\n            )\n        return items\n\n    def train(\n        self,\n        data_or_dataset_id: Union[\n            str, Iterable[Dict[str, Union[int, float, str, bool, Dict, List]]]\n        ],\n        finetune_args: Optional[dict] = None,\n        gpu_config: Optional[dict] = None,\n        is_public: Optional[bool] = None,\n        **kwargs,\n    ):\n        if isinstance(data_or_dataset_id, str):\n            dataset_id = data_or_dataset_id\n        else:\n            dataset_id = self.upload_data(data_or_dataset_id, is_public=is_public)\n        assert dataset_id is not None\n        base_path = self.trainer.get_upload_base_path()\n        self.upload_base_path = base_path[\"upload_base_path\"]\n        existing_dataset = self.trainer.get_existing_dataset(\n            dataset_id, self.upload_base_path\n        )\n        self.upload_file_path = existing_dataset[\"dataset_location\"]\n\n        job = self.trainer.train(\n            model_name=self.model_name,\n            dataset_id=dataset_id,\n            upload_file_path=self.upload_file_path,\n            finetune_args=finetune_args,\n            gpu_config=gpu_config,\n            is_public=is_public,\n        )\n        job[\"dataset_id\"] = dataset_id\n        return job\n\n    # Add alias for tune\n    tune = train\n\n    # continuously poll until the job is completed\n    def train_and_wait(\n        self,\n        data_or_dataset_id: Union[\n            str, Iterable[Dict[str, Union[int, float, str, bool, Dict, List]]]\n        ],\n        finetune_args: Optional[dict] = None,\n        gpu_config: Optional[dict] = None,\n        is_public: Optional[bool] = None,\n        **kwargs,\n    ):\n        job = self.train(\n            data_or_dataset_id,\n            finetune_args=finetune_args,\n            gpu_config=gpu_config,\n            is_public=is_public,\n        )\n\n        try:\n            status = self.check_job_status(job[\"job_id\"])\n            if status[\"status\"] == \"FAILED\":\n                print(f\"Job failed: {status}\")\n                return status\n\n            while status[\"status\"] not in (\n                \"COMPLETED\",\n                \"PARTIALLY COMPLETED\",\n                \"FAILED\",\n                \"CANCELLED\",\n            ):\n                if kwargs.get(\"verbose\", False):\n                    print(f\"job not done. waiting... {status}\")\n                time.sleep(30)\n                status = self.check_job_status(job[\"job_id\"])\n                if status[\"status\"] == \"FAILED\":\n                    print(f\"Job failed: {status}\")\n                    return status\n                elif status[\"status\"] == \"CANCELLED\":\n                    print(f\"Job canceled: {status}\")\n                    return status\n            print(\n                f\"Finetuning process completed, model name is: {status['model_name']}\"\n            )\n        except KeyboardInterrupt as e:\n            print(\"Cancelling job\")\n            return self.cancel_job(job[\"job_id\"])\n\n        return status\n\n    # Add alias for tune\n    tune_and_wait = train_and_wait\n\n    def cancel_job(self, job_id=None):\n        return self.trainer.cancel_job(job_id)\n\n    def cancel_all_jobs(\n        self,\n    ):\n        return self.trainer.cancel_all_jobs()\n\n    def resume_job(self, job_id=None):\n        return self.trainer.resume_job(job_id)\n\n    def check_job_status(self, job_id=None):\n        return self.trainer.check_job_status(job_id)\n\n    def get_jobs(self):\n        return self.trainer.get_jobs()\n\n    def evaluate(self, job_id=None):\n        return self.trainer.evaluate(job_id)\n</code></pre> <pre><code>MemoryRAG(\n    job_id: int = None,\n    api_key: Optional[str] = None,\n    api_url: Optional[str] = None,\n    model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\",\n)\n</code></pre> Expand source code <pre><code>class MemoryRAG:\n\n    def __init__(\n        self,\n        job_id: int = None,\n        api_key: Optional[str] = None,\n        api_url: Optional[str] = None,\n        model_name: str = \"meta-llama/Llama-3.2-3B-Instruct\",\n    ):\n        self.job_id = job_id\n        self.config = get_config()\n        self.api_key = api_key or lamini.api_key or get_configured_key(self.config)\n        self.api_url = api_url or lamini.api_url or get_configured_url(self.config)\n        self.api_prefix = self.api_url + \"/alpha/memory-rag\"\n        self.model_name = model_name\n\n    def memory_index(\n        self,\n        documents: List,\n    ) -&gt; str:\n        if self.model_name is None:\n            raise Exception(\"model_name must be set in order to use memory_index\")\n        payload = {\"model_name\": self.model_name}\n\n        files = [\n            (\n                \"files\",\n                (\n                    file_path,\n                    open(file_path, \"rb\"),\n                ),\n            )\n            for file_path in documents\n        ]\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n        }\n        response = requests.request(\n            \"POST\",\n            self.api_prefix + f\"/train\",\n            headers=headers,\n            data=payload,\n            files=files,\n        )\n\n        json_response = response.json()\n        self.job_id = json_response[\"job_id\"]\n        return json_response\n\n    def status(self) -&gt; str:\n        if self.job_id is None:\n            raise Exception(\"job_id must be set in order to get status\")\n        params = {\"job_id\": self.job_id}\n        resp = make_web_request(\n            self.api_key, self.api_prefix + f\"/status\", \"post\", params\n        )\n        return resp\n\n    def query(self, prompt: str, k: int = 3) -&gt; str:\n        if self.job_id is None:\n            raise Exception(\"job_id must be set in order to query\")\n        params = {\n            \"prompt\": prompt,\n            \"model_name\": self.model_name,\n            \"job_id\": self.job_id,\n            \"rag_query_size\": k,\n        }\n        resp = make_web_request(\n            self.api_key,\n            self.api_prefix + f\"/completions\",\n            \"post\",\n            params,\n        )\n        return resp\n\n    def add_index(self, prompt: str) -&gt; str:\n        if self.job_id is None:\n            raise Exception(\"job_id must be set in order to add to index\")\n        params = {\"prompt\": prompt, \"job_id\": self.job_id}\n        resp = make_web_request(\n            self.api_key,\n            self.api_prefix + f\"/add-index\",\n            \"post\",\n            params,\n        )\n        return resp\n\n    def get_logs(self) -&gt; List[str]:\n        \"\"\"Get training logs for a memory RAG job.\n\n        Args:\n            job_id: The ID of the memory RAG job\n\n        Returns:\n            List of log lines\n        \"\"\"\n        if self.job_id is None:\n            raise Exception(\"job_id must be set in order to get job logs\")\n        resp = make_web_request(\n            self.api_key,\n            self.api_prefix + f\"/training_log/{self.job_id}\",\n            \"get\",\n        )\n        return resp\n</code></pre>"},{"location":"lamini_python_class/lamini/#methods","title":"Methods","text":""},{"location":"lamini_python_class/lamini/#cancel_all_jobs","title":"<code>cancel_all_jobs</code>","text":"<p>Cancel all jobs associated with your key.</p> Expand source code <pre><code>def cancel_all_jobs(\n    self,\n):\n    return self.trainer.cancel_all_jobs()\n</code></pre>"},{"location":"lamini_python_class/lamini/#cancel_job","title":"<code>cancel_job</code>","text":"<p>Cancel the job or specify a job id to cancel.</p> Expand source code <pre><code>def cancel_job(self, job_id=None):\n    return self.trainer.cancel_job(job_id)\n</code></pre>"},{"location":"lamini_python_class/lamini/#check_job_status","title":"<code>check_job_status</code>","text":"<p>Check the status of the job or a given job id.</p> Expand source code <pre><code>def check_job_status(self, job_id=None):\n    '''\n    Possible statuses include:\n    'SCHEDULED'\n    'QUEUED'\n    'LOADING DATA'\n    'TRAINING MODEL'\n    'EVALUATING MODEL'\n    'COMPLETED'\n    'PARTIALLY COMPLETED'\n    'FAILED'\n    'CANCELLED'\n    '''\n    return self.trainer.check_job_status(job_id)\n</code></pre>"},{"location":"lamini_python_class/lamini/#generate","title":"<code>generate</code>","text":"<p>Run inference on the model or a given model.</p> Expand source code <pre><code>def generate(\n    self,\n    prompt: Union[str, List[str]],\n    model_name: Optional[str] = None,\n    output_type: Optional[dict] = None,\n    max_tokens: Optional[int] = None,\n    max_new_tokens: Optional[int] = None,\n):\n    result = self.completion.generate(\n        prompt=prompt,\n        model_name=model_name or self.model_name,\n        output_type=output_type,\n        max_tokens=max_tokens,\n        max_new_tokens=max_new_tokens,\n    )\n    if output_type is None:\n        if isinstance(prompt, list):\n            result = [single_result[\"output\"] for single_result in result]\n        else:\n            result = result[\"output\"]\n    return result\n</code></pre>"},{"location":"lamini_python_class/lamini/#get_jobs","title":"<code>get_jobs</code>","text":"<p>Get information on all jobs associated with your key.</p> Expand source code <pre><code>def get_jobs(self):\n    return self.trainer.get_jobs()\n</code></pre>"},{"location":"lamini_python_class/lamini/#resume_job","title":"<code>resume_job</code>","text":"<p>Resume <code>CANCELLED</code>, <code>PARTIALLY COMPLETED</code>, <code>FAILED</code>, or <code>COMPLETED</code> job.</p> Expand source code <pre><code>def resume_job(self, job_id=None):\n    return self.trainer.resume_job(job_id)\n</code></pre>"},{"location":"lamini_python_class/lamini/#train","title":"<code>train</code>","text":"<p>Train a job.</p> Expand source code <pre><code>def train(\n    self,\n    data_or_dataset_id: Union[\n        str, Iterable[Dict[str, Union[int, float, str, bool, Dict, List]]]\n    ],\n    finetune_args: Optional[dict] = None,\n    gpu_config: Optional[dict] = None,\n    is_public: Optional[bool] = None,\n    **kwargs,\n):\n    if isinstance(data_or_dataset_id, str):\n        dataset_id = data_or_dataset_id\n    else:\n        dataset_id = self.upload_data(data_or_dataset_id, is_public=is_public)\n    assert dataset_id is not None\n    base_path = self.trainer.get_upload_base_path()\n    self.upload_base_path = base_path[\"upload_base_path\"]\n    existing_dataset = self.trainer.get_existing_dataset(\n        dataset_id, self.upload_base_path\n    )\n    self.upload_file_path = existing_dataset[\"dataset_location\"]\n\n    job = self.trainer.train(\n        model_name=self.model_name,\n        dataset_id=dataset_id,\n        upload_file_path=self.upload_file_path,\n        finetune_args=finetune_args,\n        gpu_config=gpu_config,\n        is_public=is_public,\n    )\n    job[\"dataset_id\"] = dataset_id\n    return job\n</code></pre>"},{"location":"lamini_python_class/lamini/#tune","title":"<code>tune</code>","text":"<p>Aliases to <code>train</code>.</p> Expand source code <pre><code>tune = train\n</code></pre>"},{"location":"lamini_python_class/lamini/#train_and_wait","title":"<code>train_and_wait</code>","text":"<p>Train a job, synchronous.</p> Expand source code <pre><code>def train_and_wait(\n    self,\n    data_or_dataset_id: Union[\n        str, Iterable[Dict[str, Union[int, float, str, bool, Dict, List]]]\n    ],\n    finetune_args: Optional[dict] = None,\n    gpu_config: Optional[dict] = None,\n    is_public: Optional[bool] = None,\n    **kwargs,\n):\n    job = self.train(\n        data_or_dataset_id,\n        finetune_args=finetune_args,\n        gpu_config=gpu_config,\n        is_public=is_public,\n    )\n\n    try:\n        status = self.check_job_status(job[\"job_id\"])\n        if status[\"status\"] == \"FAILED\":\n            print(f\"Job failed: {status}\")\n            return status\n\n        while status[\"status\"] not in (\n            \"COMPLETED\",\n            \"PARTIALLY COMPLETED\",\n            \"FAILED\",\n            \"CANCELLED\",\n        ):\n            if kwargs.get(\"verbose\", False):\n                print(f\"job not done. waiting... {status}\")\n            time.sleep(30)\n            status = self.check_job_status(job[\"job_id\"])\n            if status[\"status\"] == \"FAILED\":\n                print(f\"Job failed: {status}\")\n                return status\n            elif status[\"status\"] == \"CANCELLED\":\n                print(f\"Job canceled: {status}\")\n                return status\n        print(\n            f\"Finetuning process completed, model name is: {status['model_name']}\"\n        )\n    except KeyboardInterrupt as e:\n        print(\"Cancelling job\")\n        return self.cancel_job(job[\"job_id\"])\n\n    return status\n</code></pre>"},{"location":"lamini_python_class/lamini/#upload_data","title":"<code>upload_data</code>","text":"<p>Upload data, most commonly a list of dictionaries with <code>input</code> and <code>output</code> keys.</p> Expand source code <pre><code>def upload_data(\n    self,\n    data: Iterable[Dict[str, Union[int, float, str, bool, Dict, List]]],\n    is_public: Optional[bool] = None,\n):\n    num_datapoints = 0\n\n    def get_data_str(d):\n        nonlocal num_datapoints\n        for item in d:\n            num_datapoints += 1\n            yield json.dumps(item) + \"\\n\"\n\n    if not data:\n        raise ValueError(\"Data pairs cannot be empty.\")\n\n    output = self.trainer.get_upload_base_path()\n    self.upload_base_path = output[\"upload_base_path\"]\n\n    try:\n        if self.upload_base_path == \"azure\":\n            data_str = get_data_str(data)\n            response = self.trainer.create_blob_dataset_location(\n                self.upload_base_path, is_public\n            )\n            self.upload_file_path = response[\"dataset_location\"]\n            upload_to_blob(data_str, self.upload_file_path)\n            self.trainer.update_blob_dataset_num_datapoints(\n                response[\"dataset_id\"], num_datapoints\n            )\n            print(\"Data pairs uploaded to blob.\")\n        else:\n            response = self.trainer.upload_dataset_locally(\n                self.upload_base_path, is_public, data\n            )\n            self.upload_file_path = response[\"dataset_location\"]\n            print(\"Data pairs uploaded to local.\")\n\n        print(\n            f\"\\nYour dataset id is: {response['dataset_id']} . Consider using this in the future to train using the same data. \\nEg: \"\n            f\"llm.train(data_or_dataset_id='{response['dataset_id']}')\"\n        )\n\n    except Exception as e:\n        print(f\"Error uploading data pairs: {e}\")\n        raise e\n\n    return response[\"dataset_id\"]\n</code></pre>"},{"location":"lamini_python_class/lamini/#upload_file","title":"<code>upload_file</code>","text":"<p>Upload data as a file, can be <code>csv</code> or <code>jsonl</code>.</p> Expand source code <pre><code>def upload_file(\n    self, file_path: str, input_key: str = \"input\", output_key: str = \"output\"\n):\n    items = self._upload_file_impl(file_path, input_key, output_key)\n    try:\n        dataset_id = self.upload_data(items)\n        return dataset_id\n    except Exception as e:\n        print(f\"Error reading data file: {e}\")\n        raise e\n</code></pre>"},{"location":"lamini_python_class/lamini/#laminiapimemory_rag","title":"lamini.api.memory_rag","text":""},{"location":"lamini_python_class/lamini/#initialize","title":"initialize","text":"<p>Creates a Memory RAG object.</p> <pre><code>from lamini import MemoryRAG\nclient = MemoryRAG(job_id=1,model_name=\"meta-llama/Llama-3.1-8B-Instruct\")\n</code></pre>"},{"location":"lamini_python_class/lamini/#parameters","title":"Parameters","text":"<ul> <li><code>job_id</code>: Name of an existing memory rag index.</li> <li><code>api_key</code>: Lamini API key.</li> <li><code>api_url</code>: Lamini API URL.</li> <li><code>model_name</code>: Optional name of base model to use for Memory RAG Index building or inference.</li> </ul>"},{"location":"lamini_python_class/lamini/#build-memory-rag-index","title":"Build Memory RAG Index","text":"<p>Start a Memory RAG build job.</p> <pre><code>lamini_wikipedia_page_pdf = \"&lt;path-to-file&gt;.pdf\"\nresponse = client.memory_index(documents=[lamini_wikipedia_page_pdf])\n\nprint(response)\n### {'job_id': 27666, 'status': 'created'}\n</code></pre> <p>Wait for Memory RAG train job to finish.</p> <pre><code>while status[\"status\"] == \"running\":\n    time.sleep(5)\n    status = client.status(job_id)\n    print(status[\"status\"])\n### running\n### running\n### running\n### completed\n</code></pre>"},{"location":"lamini_python_class/lamini/#parameters_1","title":"Parameters","text":"<ul> <li><code>documents</code>: List of paths (strings)to local files for building the Memory Rag index.</li> </ul>"},{"location":"lamini_python_class/lamini/#returns","title":"Returns","text":"<p>Dictionary containing job information including:</p> <ul> <li><code>job_id</code>: ID of the training job</li> <li><code>status</code>: Job ENUM status: {\"created\", \"running\", \"completed\", \"failed\"}</li> </ul>"},{"location":"lamini_python_class/lamini/#laminiclassifylamini_classifier","title":"lamini.classify.lamini_classifier","text":""},{"location":"lamini_python_class/lamini/#initialize_1","title":"initialize","text":"<p>Creates a new classifier project and kicks off the initial data generation and training process.</p> <pre><code>def initialize(\n    name: str,\n    classes: List[str],\n    examples: Dict[str, List[str]],\n    model_name: Optional[str] = None\n) -&gt; Dict[str, str]\n</code></pre>"},{"location":"lamini_python_class/lamini/#parameters_2","title":"Parameters","text":"<ul> <li><code>name</code>: Name for the classifier project. Must be unique for the user.</li> <li><code>classes</code>: List of class names that the classifier will be trained to identify.</li> <li><code>examples</code>: Dictionary mapping class names to lists of example texts for each class.</li> <li><code>model_name</code>: Optional name of base model to use for classification.</li> </ul>"},{"location":"lamini_python_class/lamini/#returns_1","title":"Returns","text":"<p>Dictionary containing:</p> <ul> <li><code>name</code>: Name of the created project</li> <li><code>job_id</code>: ID of the training job that was initiated</li> </ul>"},{"location":"lamini_python_class/lamini/#raises","title":"Raises","text":"<ul> <li><code>HTTPException(497)</code>: If a project with the given name already exists</li> <li><code>HTTPException(499)</code>: If project creation fails</li> </ul>"},{"location":"lamini_python_class/lamini/#example","title":"Example","text":"<pre><code>from lamini.classify.lamini_classifier import LaminiClassifier\nclassifier = Classifier()\nresult = classifier.initialize(\n    name=\"sentiment_classifier\",\n    classes=[\"positive\", \"negative\"],\n    examples={\n        \"positive\": [\"great movie!\", \"loved it\"],\n        \"negative\": [\"terrible film\", \"waste of time\"]\n    }\n)\nprint(f\"Created project {result['name']} with job {result['job_id']}\")\n</code></pre>"},{"location":"lamini_python_class/lamini/#train_1","title":"train","text":"<p>Train a classifier model on provided data.</p> <pre><code>def train(\n    data_or_dataset_id: Union[str, List[Dict]],\n    finetune_args: Optional[dict] = None,\n    gpu_config: Optional[dict] = None,\n    is_public: Optional[bool] = None,\n    **kwargs\n) -&gt; Dict[str, str]\n</code></pre>"},{"location":"lamini_python_class/lamini/#parameters_3","title":"Parameters","text":"<ul> <li><code>data_or_dataset_id</code>: Either a dataset ID string or list of training examples</li> <li><code>finetune_args</code>: Optional dictionary of fine-tuning parameters</li> <li><code>gpu_config</code>: Optional GPU configuration settings</li> <li><code>is_public</code>: Whether to make the trained model public</li> </ul>"},{"location":"lamini_python_class/lamini/#returns_2","title":"Returns","text":"<p>Dictionary containing job information including:</p> <ul> <li><code>job_id</code>: ID of the training job</li> <li><code>dataset_id</code>: ID of the dataset used for training</li> </ul>"},{"location":"lamini_python_class/lamini/#classify","title":"classify","text":"<p>Run classification on input text using a trained model.</p> <pre><code>def classify(\n    prompt: Union[str, List[str]],\n    top_n: Optional[int] = None,\n    threshold: Optional[float] = None,\n    metadata: Optional[bool] = None\n) -&gt; Union[str, List[str]]\n</code></pre>"},{"location":"lamini_python_class/lamini/#parameters_4","title":"Parameters","text":"<ul> <li><code>prompt</code>: Input text or list of texts to classify</li> <li><code>top_n</code>: Optional number of top predictions to return</li> <li><code>threshold</code>: Optional confidence threshold for predictions</li> <li><code>metadata</code>: Whether to return prediction metadata</li> </ul>"},{"location":"lamini_python_class/lamini/#returns_3","title":"Returns","text":"<p>Predicted class(es) for the input text(s)</p>"},{"location":"lamini_python_class/lamini/#example_1","title":"Example","text":"<pre><code>cls.classify(\"woof\")\n</code></pre> <pre><code>{\n  \"classification\": [\n    [\n      {\n        \"class_id\": 1,\n        \"class_name\": \"dog\",\n        \"prob\": 0.9263275590881269\n      },\n      {\n        \"class_id\": 0,\n        \"class_name\": \"cat\",\n        \"prob\": 0.2736724409118731\n      }\n    ]\n  ]\n}\n</code></pre>"},{"location":"lamini_python_class/lamini/#add","title":"add","text":"<p>Add additional training examples to an existing classifier project.</p> <pre><code>def add(\n    project_name: str,\n    dataset_name: str,\n    data: Dict[str, List[str]]\n) -&gt; Dict[str, bool]\n</code></pre>"},{"location":"lamini_python_class/lamini/#parameters_5","title":"Parameters","text":"<ul> <li><code>project_name</code>: Name of the existing classifier project</li> <li><code>dataset_name</code>: Name for the new dataset being added</li> <li><code>data</code>: Dictionary mapping class names to lists of example texts</li> </ul>"},{"location":"lamini_python_class/lamini/#returns_4","title":"Returns","text":"<p>Dictionary containing:</p> <ul> <li><code>success</code>: Boolean indicating if examples were added successfully</li> </ul>"},{"location":"lamini_python_class/lamini/#example_2","title":"Example","text":"<pre><code>classifier.add(\n    project_name=\"sentiment_classifier\",\n    dataset_name=\"additional_examples\",\n    data={\n        \"positive\": [\"excellent work\", \"fantastic results\"],\n        \"negative\": [\"poor quality\", \"disappointing outcome\"]\n    }\n)\n</code></pre>"},{"location":"lamini_python_class/lamini/#laminione_evalerone_evaler","title":"lamini.one_evaler.one_evaler","text":""},{"location":"lamini_python_class/lamini/#run","title":"run","text":"<p>Run evaluation on a model using provided test data.</p> <pre><code>def run() -&gt; Dict[str, Union[str, List, Dict, str]]\n</code></pre>"},{"location":"lamini_python_class/lamini/#parameters_6","title":"Parameters","text":"<ul> <li><code>test_model_id</code>: ID of the model to evaluate</li> <li><code>eval_data</code>: List of dictionaries with <code>input</code> and <code>target</code> keys</li> <li><code>eval_data_id</code>: name or ID for the evaluation dataset</li> <li><code>base_model_id</code>: ID of the base model to compare against (optional)</li> <li><code>fuzzy</code>: Whether to perform fuzzy matching of predictions (optional)</li> <li><code>sbs</code>: Whether to perform side-by-side comparison with the base model (optional)</li> </ul>"},{"location":"lamini_python_class/lamini/#returns_5","title":"Returns","text":"<p>Dictionary containing:</p> <ul> <li><code>eval_job_id</code>: Unique identifier for the evaluation job</li> <li><code>eval_data_id</code>: ID of the evaluation dataset used</li> <li><code>metrics</code>: Evaluation metrics results</li> <li><code>status</code>: Status of the evaluation job (\"COMPLETED\" or \"FAILED\")</li> <li><code>predictions</code>: List of actual model outputs</li> </ul>"},{"location":"lamini_python_class/lamini/#example_3","title":"Example","text":"<pre><code>from lamini.one_evaler import LaminiOneEvaler\n\nevaluator = LaminiOneEvaler(\n    test_model_id=\"my-model-id\",\n    eval_data=[\n        {\"input\": \"text1\", \"output\": \"label1\"},\n        {\"input\": \"text2\", \"output\": \"label2\"}\n    ],\n    test_eval_type=\"classifier\"\n)\n\nresult = evaluator.run()\nprint(f\"Evaluation completed with job ID: {result['eval_job_id']}\")\nprint(f\"Metrics: {result['metrics']}\")\n</code></pre>"},{"location":"lamini_python_class/lamini/#notes","title":"Notes","text":"<ul> <li>The evaluation compares model predictions against provided ground truth labels</li> <li>Can optionally perform side-by-side (sbs) comparison with a base model</li> <li>Supports fuzzy matching of predictions when fuzzy=True</li> </ul>"},{"location":"memory_experiments/","title":"Memory Experiment","text":"<p>Memory Experiment provides a framework for running structured experiments with LLMs, combining validators and generators within an agentic pipeline. It enables systematic testing and evaluation of synthetic data generation pipelines.</p> <p>The need for synthetic data comes into play when you need to build out high quality datasets for fine tuning or RAG. Many cases, there isn't enough ground truth data, the agentic pipelines from Lamini allow for the codification of manual data creation/curation/validation into an automated pipeline.</p>"},{"location":"memory_experiments/#quick-start-with-python-sdk","title":"Quick Start with Python SDK","text":"<p>First, make sure your API key is set (get yours at app.lamini.ai/account):</p> TerminalPython SDK <pre><code>export LAMINI_API_KEY=\"&lt;YOUR-LAMINI-API-KEY&gt;\"\n</code></pre> <pre><code>import lamini\nimport os\n\nlamini.api_key = os.environ[\"LAMINI_API_KEY\"]\n</code></pre> <p>To initialize your pipeline, you'll need to set up your pipeline components and bundle then within the BaseAgenticPipeline object:</p> Python SDK <p>Set up your generators:</p> <pre><code>from lamini.experiment.generators import QuestionsToConceptsGenerator\nfrom lamini.experiment.pipeline import BaseAgenticPipeline\n\n# Create a generator\nconcept_generator = QuestionsToConceptsGenerator(\n    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n    name=\"concept_generator\"\n)\n\n# Create pipeline\npipeline = BaseAgenticPipeline(\n    generators={\"concept\": concept_generator},\n    record_dir=\"./experiment_results\"\n)\n</code></pre> <p>Set up your validators:</p> <pre><code>from lamini.experiment.validators import BaseValidator\n\n# Create a validator\nconcept_validator = BaseValidator(\n    name=\"concept_validator\",\n    instruction=\"Validate if the following concepts are clear and well-defined: {concepts}\",\n    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n    output_type={\"is_valid\": \"bool\", \"feedback\": \"str\"},\n    is_valid_field=\"is_valid\"\n)\n\n# Update pipeline with validator\npipeline = BaseAgenticPipeline(\n    generators={\"concept\": concept_generator},\n    validators={\"concept\": concept_validator},\n    record_dir=\"./experiment_results\"\n)\n</code></pre> <p>Run the pipeline: <pre><code>from lamini.experiment import ExperimentObject\n\n# Create input\nexperiment_obj = ExperimentObject(\n    experiment_step=\"generation\",\n    data={\"questions\": [\"What is machine learning?\", \"How do neural networks work?\"]}\n)\n\n# Execute experiment\nresults = pipeline(experiment_obj)\n</code></pre></p> <p>You can further track your AgenticPipeline experiments within the MemoryExperiment object. Memory experiment is intended to be used to track a sinlge use case (factualQA or text-to-SQL) for LLM development using fine-tuning or RAG. This includes pipeline execution, validation, and memory tuning or RAG jobs, then finally evaluating the results. Iterating is done through enhancing the pipeline components. Track your experiment progress using the MemoryExperiment class:</p> <pre><code>from lamini.experiment import MemoryExperiment\n\n# Create a memory experiment to track your pipeline\nmemory_experiment = MemoryExperiment(\n    agentic_pipeline=pipeline,\n    model=\"meta-llama/Llama-3.1-8B-Instruct\",  # Model for evaluation/memory tasks\n    record_dir=\"./experiment_results\"\n)\n\n# Run the experiment\nresults = memory_experiment(experiment_obj)\n\n# Evaluate results with structured output\nresponse = memory_experiment.evaluate(\n    experiment_obj=ExperimentObject(\n        experiment_step=\"evaluation\",\n        data={\"prompt\": \"Explain the concepts of machine learning\"}\n    ),\n    output_type={\"explanation\": \"str\", \"confidence_score\": \"float\"}\n)\n</code></pre>"},{"location":"memory_experiments/#iteration","title":"Iteration","text":"<p>To improve your experiment results:</p> <ol> <li>Review pipeline results in your record_dir with respect to a golden test  set</li> <li>Add or modify validators for quality control, based on what errors are  occurring from the generator and validator outputs with respect to the  golden test set</li> <li>Experiment with different models and prompts, some models may perform  better for certain tasks and some models require different styles of prompt  engineering techniques</li> <li>Fine-tune generator instructions based on validation results</li> <li>Repeat</li> </ol> <p>The experiment framework provides flexibility to iterate and improve your  pipeline until you achieve the desired performance for your use case.</p>"},{"location":"memory_experiments/best_debugging_practices/","title":"Best Debugging Practices for Text-to-SQL Agentic Pipeline","text":""},{"location":"memory_experiments/best_debugging_practices/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"memory_experiments/best_debugging_practices/#1-logical-errors-in-syntactically-valid-sql","title":"1. Logical Errors in Syntactically Valid SQL","text":"<ul> <li>Issue: SQL can pass syntax validation but produce incorrect results  </li> <li>Solution: Enhance your glossary with semantic relationships and domain-specific examples  </li> <li>Example: When querying for \"best selling products,\" the model might sort by revenue instead of units sold  </li> <li>Action: Add explicit definitions in your glossary: <code>\"best_selling\": \"Refers to products with highest unit sales, not revenue\"</code></li> </ul>"},{"location":"memory_experiments/best_debugging_practices/#2-model-selection-strategy","title":"2. Model Selection Strategy","text":"<ul> <li> <p>Primary Model: Use robust models for initial query generation (e.g., Llama 3.1-8B)</p> </li> <li> <p>Specialized Models: Use dedicated models for tasks that need focused capabilities:</p> <ul> <li>Data generation: Models with strong pattern recognition</li> <li>Debugging: Models with good reasoning abilities</li> <li>Fine-tuning: Balance between performance and training efficiency</li> </ul> </li> <li> <p>Practical Approach: Keep a baseline model for general use, but experiment with specialized models in components where errors persist</p> </li> </ul>"},{"location":"memory_experiments/best_debugging_practices/#3-case-sensitivity-and-string-normalization","title":"3. Case Sensitivity and String Normalization","text":"<ul> <li>Issue: Models may struggle with case differences (<code>\"lamini\"</code> vs. <code>\"Lamini\"</code>)</li> <li>Solution: Provide examples in your glossary showing variations  </li> <li>Example Glossary Entry:   <pre><code>\"brands\": {\n  \"definition\": \"Product manufacturers\",\n  \"examples\": [\"Lamini\", \"LlamaCo\", \"Sheep Inc.\"],\n  \"case_sensitivity\": \"Brand names are case-sensitive in our database\"\n}\n</code></pre></li> </ul>"},{"location":"memory_experiments/best_debugging_practices/#4-business-logic-representation","title":"4. Business Logic Representation","text":"<ul> <li>Issue: Models struggle with domain-specific calculations  </li> <li>Solution: Include formulas and calculation methods in your glossary  </li> <li>Example:   <pre><code>\"sales_volume\": {\n  \"definition\": \"Total units sold in a time period\",\n  \"calculation\": \"SUM(units_sold) WHERE date BETWEEN start_date AND end_date\",\n  \"examples\": [\n    {\n      \"question\": \"What was the sales volume in January?\", \n      \"sql\": \"SELECT SUM(units_sold) FROM sales WHERE date BETWEEN '2025-01-01' AND '2025-01-31'\"\n    }\n  ]\n}\n</code></pre></li> </ul>"},{"location":"memory_experiments/best_debugging_practices/#5-preventing-overfitting","title":"5. Preventing Overfitting","text":"<ul> <li>Generate Diverse Training Data: Use multiple question generators to create varied examples  </li> <li>Verify Coverage: Analyze generated data to ensure all SQL concepts are represented  </li> <li>Testing Approach: Reserve truly novel questions for evaluation that weren't in training  </li> </ul>"},{"location":"memory_experiments/best_debugging_practices/#understanding-sql-validation-limitations","title":"Understanding SQL Validation Limitations","text":""},{"location":"memory_experiments/best_debugging_practices/#laminis-sqlvalidator-capabilities-and-gaps","title":"Lamini's <code>SQLValidator</code> Capabilities and Gaps","text":"<p>The <code>SQLValidator</code> uses <code>EXPLAIN QUERY PLAN</code> for basic validation:</p> <ul> <li> <p>\u2705 Catches:</p> <ul> <li>Syntax errors</li> <li>Missing tables or columns</li> </ul> </li> <li> <p>\u274c Misses:</p> <ul> <li>Runtime errors</li> <li>Logical flaws (e.g., wrong WHERE clause logic)</li> <li>Data type mismatches</li> <li>SQL dialect incompatibilities</li> </ul> </li> </ul> <p>Examples of what it can\u2019t catch: - Data type mismatches:   <pre><code>SELECT * FROM users WHERE age = 'abc';\n</code></pre> - Invalid operations:   <pre><code>SELECT total_amount / 0 FROM sales;\n</code></pre> - Unsupported functions:   <pre><code>SELECT DATEADD(day, 5, '2024-01-01');  -- Fails in SQLite\n</code></pre></p>"},{"location":"memory_experiments/best_debugging_practices/#enhanced-validation-approaches","title":"Enhanced Validation Approaches","text":""},{"location":"memory_experiments/best_debugging_practices/#1-runtime-execution-validator","title":"1. Runtime Execution Validator","text":"<ul> <li>Pros: Catches actual execution errors</li> <li>Cons: May timeout on large datasets or inefficient queries</li> <li>Implementation: Add timeout thresholds and sandbox execution</li> </ul>"},{"location":"memory_experiments/best_debugging_practices/#2-result-validation-checker","title":"2. Result Validation Checker","text":"<ul> <li>Limitation: Empty results may be valid (e.g., no matching records)</li> <li>Solution: Validate sample queries with known results</li> </ul>"},{"location":"memory_experiments/best_debugging_practices/#systematic-error-resolution","title":"Systematic Error Resolution","text":""},{"location":"memory_experiments/best_debugging_practices/#common-error-categories-and-fixes","title":"Common Error Categories and Fixes","text":""},{"location":"memory_experiments/best_debugging_practices/#1-missing-object-errors","title":"1. Missing Object Errors","text":"<ul> <li>Issue: Queries fail due to column or table names that don't exist  </li> <li>Fix: Implement fuzzy matching   <pre><code># \"customer_nme\" doesn't exist, but suggest closest match\nsuggested_column = find_similar_column(\"customer_nme\", schema_columns)\n</code></pre></li> </ul>"},{"location":"memory_experiments/best_debugging_practices/#2-data-format-inconsistencies","title":"2. Data Format Inconsistencies","text":"<ul> <li>Issue: Format differences cause mismatches  </li> <li>Fix: Provide example values in glossary <pre><code>\"location\": {\n  \"definition\": \"Customer location\",\n  \"format\": \"City, State\",\n  \"examples\": [\"Sacramento, CA\", \"Boston, MA\"]\n}\n</code></pre></li> </ul>"},{"location":"memory_experiments/best_debugging_practices/#3-sql-dialect-differences","title":"3. SQL Dialect Differences","text":"<ul> <li>Issue: Functions vary across SQL dialects  </li> <li>Fix: Maintain function mapping table <pre><code>\"function_mappings\": {\n  \"DATEADD\": {\n    \"sqlite\": \"datetime(date, '+N days')\",\n    \"postgres\": \"date + interval 'N days'\"\n  }\n}\n</code></pre></li> </ul>"},{"location":"memory_experiments/best_practices/","title":"Best Practices","text":""},{"location":"memory_experiments/best_practices/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Defining Your Use Case</li> <li>Finding Relevant Data</li> <li>Starting Memory Experiments</li> <li>Task Analysis and Evaluation</li> </ul>"},{"location":"memory_experiments/best_practices/#defining-your-use-case","title":"Defining Your Use Case","text":"<p>Before beginning any memory tuning work, clearly articulate what you want the model to do by addressing these key areas:</p>"},{"location":"memory_experiments/best_practices/#task-definition","title":"Task Definition","text":"<ul> <li>What specific task or problem will the model solve?<ul> <li>Example: \"Classify customer support tickets by urgency\" or \"Generate product descriptions from specifications\"</li> </ul> </li> </ul> <p>Example Use Case: Legal Document Analysis</p>"},{"location":"memory_experiments/best_practices/#success-metrics","title":"Success Metrics","text":"<ul> <li>How will success be measured?<ul> <li>Example: \"95% classification accuracy on test set\" or \"Average response time under 2 seconds\"</li> </ul> </li> <li>Define clear metrics (accuracy, response time, user satisfaction, etc.)<ul> <li>Example: \"F1 score for urgency classification\" or \"ROUGE scores for summary quality\"</li> </ul> </li> <li>Set target performance levels<ul> <li>Example: \"Minimum 90% accuracy across all categories\" or \"Maximum 5% false urgents\"</li> </ul> </li> <li>Plan how to validate results<ul> <li>Example: \"Weekly manual review of 100 random predictions\" or \"A/B testing with current process\"</li> </ul> </li> </ul>"},{"location":"memory_experiments/best_practices/#business-impact","title":"Business Impact","text":"<ul> <li>How will this improve existing processes?<ul> <li>Example: \"Reduce ticket routing time by 75%\" or \"Eliminate manual classification backlog\"</li> </ul> </li> <li>What new opportunities will it create?<ul> <li>Example: \"24/7 automated ticket prioritization\" or \"Real-time urgency alerting system\"</li> </ul> </li> <li>What are the expected cost savings or revenue increases?<ul> <li>Example: \"Reduce support staff overtime by 30%\" or \"Increase customer satisfaction by 25%\"</li> </ul> </li> </ul>"},{"location":"memory_experiments/best_practices/#constraints-and-requirements","title":"Constraints and Requirements","text":"<ul> <li>Budget limitations<ul> <li>Example: \"Maximum compute cost of $1000/month\" or \"One-time training budget of $5000\"</li> </ul> </li> <li>Technical infrastructure requirements<ul> <li>Example: \"Must run on existing cloud infrastructure\" or \"Maximum latency of 100ms\"</li> </ul> </li> <li>Data privacy and security considerations<ul> <li>Example: \"No PII storage\" or \"HIPAA compliance required\"</li> </ul> </li> <li>Response time requirements<ul> <li>Example: \"Classification within 5 seconds\" or \"Batch processing within 1 hour\"</li> </ul> </li> <li>Accuracy thresholds<ul> <li>Example: \"99.9% accuracy for high-urgency tickets\" or \"Maximum 1% false positives\"</li> </ul> </li> </ul>"},{"location":"memory_experiments/best_practices/#stakeholder-analysis","title":"Stakeholder Analysis","text":"<ul> <li>Who will use the model?<ul> <li>Example: \"Front-line support staff\" or \"Customer service managers\"</li> </ul> </li> <li>Who will maintain it?<ul> <li>Example: \"ML Operations team\" or \"Support engineering group\"</li> </ul> </li> <li>Who needs to approve it?<ul> <li>Example: \"Security compliance team\" or \"Customer support director\"</li> </ul> </li> <li>Who will be impacted by it?<ul> <li>Example: \"Customer support representatives\" or \"End customers waiting for responses\"</li> </ul> </li> </ul>"},{"location":"memory_experiments/best_practices/#finding-relevant-data","title":"Finding Relevant Data","text":"<p>Gather examples that represent your intended use case. Look for existing data if enhancing current processes, or create new data for novel applications.</p>"},{"location":"memory_experiments/best_practices/#example-scenario-customer-support-ticket-classification","title":"Example Scenario: Customer Support Ticket Classification","text":"<p>For existing processes: - Collect historical tickets with manual urgency classifications - Include resolution times and outcomes - Analyze existing classification patterns</p> <p>For new opportunities: - Create synthetic examples with sample support tickets - Engage support staff to label historical data - Research public datasets from similar industries</p>"},{"location":"memory_experiments/best_practices/#starting-memory-experiments","title":"Starting Memory Experiments","text":""},{"location":"memory_experiments/best_practices/#manual-task-analysis","title":"Manual Task Analysis","text":"<p>Begin by performing the task yourself to understand the process thoroughly. </p> <p>Document: - Your step-by-step approach - Required information sources - Reasoning methodology</p>"},{"location":"memory_experiments/best_practices/#evaluation-framework","title":"Evaluation Framework","text":"<p>Develop a robust evaluation strategy combining:</p> <p>Qualitative Metrics: - Response quality - User satisfaction</p> <p>Quantifiable Components: - Factual correctness - Source alignment</p> <p>Note: Always validate metric correlations with stakeholders.</p>"},{"location":"memory_experiments/best_practices/#risk-assessment","title":"Risk Assessment","text":"<p>Consider potential failure points: - Input variations that could cause confusion   - Edge cases and corner cases   - Ambiguous scenarios requiring judgment   - Common error patterns   - Security and safety considerations</p>"},{"location":"memory_experiments/best_practices/#complexity-analysis","title":"Complexity Analysis","text":"<p>Evaluate task difficulty across multiple dimensions: - Required domain knowledge - Process complexity - Context dependencies - Information freshness requirements - Reasoning depth</p>"},{"location":"memory_experiments/best_practices/#task-dimensioning","title":"Task Dimensioning","text":"<p>Break down the task into measurable components:</p> <p>Input Complexity: - Length and format variations - Information quality and completeness - Input source diversity</p> <p>Processing Requirements: - Required reasoning steps - Decision dependencies - External knowledge needs</p> <p>Output Specifications: - Format requirements - Consistency needs - Validation criteria</p> <p>These components provide a framework for automated metric tracking and evaluation. </p>"},{"location":"memory_experiments/data_generation/","title":"Data Generation","text":"<p>You can run the command below to generate training data for the memory experiment. It initiates multiple generators to generate data and saves the data to the output parquet file.  In addition, you can optionally register the dataset for the experiment in the experiment parquet file.</p> <pre><code>lamini generate \\\n--experiment-name &lt;my_exp&gt; \\\n--pipeline-yml &lt;path_to_pipeline.yml&gt; \\\n--seed-data-jsonl &lt;path_to_seed_data.jsonl&gt; \\\n--sqlite-file &lt;path_to_db.sqlite&gt; \\\n--output-parquet &lt;path_to_res.parquet&gt; \\\n--experiment-parquet &lt;path_to_exp.parquet&gt;\n</code></pre>"},{"location":"memory_experiments/data_generation/#options","title":"Options","text":"<ul> <li><code>--experiment-name</code> (required) - name for the experiment</li> <li><code>--pipeline-yml</code> (required) - path to pipeline config yml file</li> <li><code>--seed-data-jsonl</code> (required) - path to seed data jsonl file</li> <li><code>--sqlite-file</code> (required) - path to sqlite file</li> <li><code>--output-parquet</code> (required) - path to save the generated data in parquet format</li> <li><code>--experiment-parquet</code> (optional) - path to register the generated data for the experiment in parquet format</li> </ul>"},{"location":"memory_experiments/data_generation/#seed-data-jsonl","title":"Seed data jsonl","text":"<p>The seed data jsonl file should look like below. where each line contains three keys: <code>input</code>, <code>output</code>, and <code>glossary</code>.</p> <ul> <li><code>input</code> - natural language description of the query</li> <li><code>output</code> - SQL code for input</li> <li><code>glossary</code> - a list of dicts where each <code>input</code> is a term and the corresponding <code>output</code> is the definition of the term.  Note that the <code>glossary</code> value is the same in every line of the jsonl.</li> </ul> <pre><code>{\"input\": \"find name from the highest ...\",\n  \"output\": \"SELECT name FROM ...\",\n  \"glossary\": [\n     {\"input\": \"ABC\", \"output\": \"ABC means ...\"},\n     {\"input\": \"BCD\", \"output\": \"BCD means ...\"},\n  ],\n  \"input\": \"find price ...\",\n  \"output\": \"SELECT price FROM ...\",\n  \"glossary\": [\n     {\"input\": \"ABC\", \"output\": \"ABC means ...\"},\n     {\"input\": \"BCD\", \"output\": \"BCD means ...\"},\n  ]\n}\n</code></pre>"},{"location":"memory_experiments/evaluation/","title":"Evaluating Model Performance","text":""},{"location":"memory_experiments/evaluation/#overview","title":"Overview","text":"<p>This guide walks through the process of evaluating large language models (LLMs) with a focus on real-world impact. Evaluations (evals) ensure your model behaves as expected\u2014so you can iterate and deploy with confidence.</p> <p>We'll use a customer support chatbot example throughout this guide to demonstrate each step in context.</p>"},{"location":"memory_experiments/evaluation/#table-of-contents","title":"Table of Contents","text":"<ul> <li>What is Eval?</li> <li>Create an Eval Set</li> <li>Build an Eval Pipeline</li> <li>Run Eval on Base Model</li> <li>Generate Synthetic Data</li> <li>Run Eval on Tuned Model</li> </ul>"},{"location":"memory_experiments/evaluation/#what-is-eval","title":"What is Eval?","text":"<p>Evaluations (evals) help you determine whether your model can do a good job in real life. They give you a repeatable way to measure model performance against your expectations.</p>"},{"location":"memory_experiments/evaluation/#customer-support-example","title":"Customer Support Example","text":"<p>Let's say you're replacing a rule-based chatbot that frustrates users by saying \"I don't know\" too often. Your goal is to evaluate if an LLM can give helpful, accurate answers based on your internal help documentation.</p> <p>Since the real world is complex, start small\u2014just like how you wouldn't train your model on every customer issue at once, you don't need to eval everything upfront either. Start with one core area, like account login issues.</p>"},{"location":"memory_experiments/evaluation/#create-an-eval-set","title":"Create an Eval Set","text":""},{"location":"memory_experiments/evaluation/#understand-your-objective","title":"Understand Your Objective","text":"<p>What do you want the model to do? What does \"correct\" mean in your context?</p> <p>Example: You want the model to answer basic customer questions (e.g., password reset, account deactivation) without escalating to a human rep unnecessarily. Success means fewer users asking for human support.</p>"},{"location":"memory_experiments/evaluation/#analyze-the-problem","title":"Analyze the Problem","text":"<p>What's going wrong now? Where is the existing system failing?</p> <p>Example: The current chatbot often gives: - \"Sorry, I don't understand.\" - Hallucinated info (e.g., claiming there's a password reset link that doesn't exist) - Irrelevant answers</p> <p>Review chat logs to find real examples where the bot failed. Ask: - Why are users dissatisfied? - What should a good answer look like?</p>"},{"location":"memory_experiments/evaluation/#example-answers","title":"Example Answers","text":"<p>Bad Answer: Q: \"How do I reset my password?\" A: \"Sorry, I'm not sure. Please contact support.\"</p> <p>Good Answer: A: \"To reset your password, go to [example.com/reset] and follow the instructions. Let me know if you need help.\"</p> <p>Create 20 Questions and Answers Start with a small, simple eval set\u2014just 20 examples is enough to begin. Example: Q: \"Can I delete my account?\" A: \"Yes, you can delete your account by going to Settings &gt; Account &gt; Delete Account.\" Your answers must be accurate\u2014these are your gold answers. The model's job is to match or closely approximate them.</p>"},{"location":"memory_experiments/evaluation/#build-an-eval-pipeline","title":"Build an Eval Pipeline","text":"<p>Now let's automate the evaluation. Human review is slow and subjective. We want a pipeline that can repeatedly and consistently check how well the model is doing. Eval = Compare Output to Gold Answer Start by defining: what counts as \"correct\"? Customer Support Example:  You may want to check: Factuality \u2013 Is the answer accurate? Source alignment \u2013 Does it come from your knowledge base? Semantic similarity \u2013 Is it close enough in meaning to the gold answer? String match \u2013 Useful if you're expecting very structured responses. Example Pipeline for Support Chatbot (Factual QA) AnswerValidator \u2192 Checks if model output matches the gold answer (exact or semantically). FactValidator \u2192 Ensures the answer exists in your customer support docs. Test the Evaluator Itself Run the pipeline on your gold set. If your evaluator is wrongly marking good answers as bad (or vice versa), improve the prompt or break the task into smaller validators.</p>"},{"location":"memory_experiments/evaluation/#run-eval-on-base-model","title":"Run Eval on Base Model","text":"<p>With the pipeline ready, it's time to see how your base model performs. Customer Support Example:  You run your base model on the 20-question eval set. Outcomes: If it performs well, great! Start adding more complex questions. If it performs poorly, that's also great\u2014you now know what to improve. Example Result:  Out of 20 questions: 5 correct 7 partially correct (missing some info) 8 wrong or hallucinated Now you know where to focus your training.</p>"},{"location":"memory_experiments/evaluation/#generate-synthetic-data","title":"Generate Synthetic Data","text":"<p>Now that you've identified weak spots, it's time to improve your model. A powerful way to do that is to generate synthetic training data based on your eval. With Lamini, you can easily construct an agentic data pipeline with generators and validators that suit your use case and create high quality synthetic data. A guide on generate synthetic data is coming soon too.</p>"},{"location":"memory_experiments/evaluation/#run-eval-on-tuned-model","title":"Run Eval on Tuned Model","text":"<p>After training on synthetic data, re-run your eval set to see how your model has improved. Once the model performs well on your eval set, you'll want to go to the next level: more difficult, more complex evaluations. The majority of the work will be error analysis and iteration\u2014analyzing the eval result, figuring out what the model is not doing well on, adding more training data, or improving your pipelines.</p>"},{"location":"memory_experiments/factual_qa/","title":"Factual Question-Answering with Memory Experiment","text":"<p>The Memory Experiment framework provides powerful tools for creating, validating, and improving factual question-answering models. This document outlines how to use the framework for building and fine-tuning models that can accurately answer questions based on provided source materials.</p>"},{"location":"memory_experiments/factual_qa/#overview","title":"Overview","text":"<p>Factual QA LLMs enable users to get accurate answers from source materials. The Memory Experiment framework streamlines the process of:</p> <ol> <li>Extracting key facts and concepts from source materials</li> <li>Generating high-quality training questions and answers</li> <li>Validating answer accuracy against source material</li> <li>Fine-tuning models on generated data</li> <li>Evaluating model performance</li> </ol> <p>The complete pipeline helps you create domain-specific factual QA models with high accuracy and comprehensive coverage of source materials.</p>"},{"location":"memory_experiments/factual_qa/#getting-started","title":"Getting Started","text":""},{"location":"memory_experiments/factual_qa/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li>A Lamini API key (get yours at app.lamini.ai)</li> <li>Python 3.7+ with pip installed</li> <li>The Lamini Python SDK: <code>pip install lamini</code></li> </ul>"},{"location":"memory_experiments/factual_qa/#required-inputs","title":"Required Inputs","text":"<p>To build a factual QA model with Memory Experiment, you'll need:</p> <ol> <li>Documents: Text documents containing ground truth information</li> <li>Evaluation Set: 20-35 sample questions with corresponding answers from source materials</li> <li>Knowledge Base: Structured storage of extracted facts and relationships, typically stored within a JSON lines file.</li> </ol>"},{"location":"memory_experiments/factual_qa/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"memory_experiments/factual_qa/#1-setup-project-structure","title":"1. Setup Project Structure","text":"<p>Create a directory for your factual QA project:</p> <pre><code>mkdir factual_qa_project\ncd factual_qa_project\n</code></pre> <p>Prepare your input files:</p> <ul> <li><code>source_materials/</code>: Directory containing your documents</li> <li><code>eval_set.jsonl</code>: Question-answer pairs for evaluation</li> <li><code>knowledge_base.json</code>: Structured storage of extracted facts</li> </ul> <p>Create a <code>config.yml</code> file to manage API keys and other settings:</p> <pre><code>api:\n  url: \"https://app.lamini.ai\"\n  key: \"YOUR_LAMINI_API_KEY\"\n\nmodel:\n  default: \"meta-llama/Llama-3.1-8B-Instruct\"\n  memory_tuning: \"meta-llama/Llama-3.1-8B-Instruct\"\n\npaths:\n  source_materials: \"path/to/your/source_materials/\"\n  gold_test_set: \"path/to/your/eval_set.jsonl\"\n  knowledge_base: \"path/to/your/knowledge_base.json\"\n\nmemory_tuning:\n  max_steps: 1000\n  learning_rate: 3e-5\n  max_gpus: 1\n  max_nodes: 1\n</code></pre>"},{"location":"memory_experiments/factual_qa/#2-generate-training-data","title":"2. Generate Training Data","text":"<p>Extract facts and generate questions using various generators:</p> <pre><code>import os\nimport lamini\nimport pathlib\nimport datetime\n\nfrom lamini.experiment.generators import (\n    FactExtractor,\n    ConceptExtractor,\n    QuestionGenerator,\n    AnswerGenerator,\n    QuestionDecomposer\n)\nfrom lamini.experiment.validators import (\n    FactualValidator,\n    RelevanceValidator,\n    AnswerValidator\n)\n\n# Load configuration\nconfig = load_config(\"config.yml\")\n\n# Set up Lamini API credentials\nlamini.api_url = config['api']['url']\nlamini.api_key = config['api']['key']\n\n# Initialize generators\ngenerators = {\n    \"fact\": FactExtractor(model=config['model']['default']),\n    \"concept\": ConceptExtractor(model=config['model']['default']),\n    \"question\": QuestionGenerator(model=config['model']['default']),\n    \"answer\": AnswerGenerator(model=config['model']['default']),\n    \"decomposer\": QuestionDecomposer(model=config['model']['default'])\n}\n\n# Process source materials to generate training data\n# (See generate_data.py in the repository for the complete implementation)\n</code></pre>"},{"location":"memory_experiments/factual_qa/#3-analyze-generated-data","title":"3. Analyze Generated Data","text":"<p>Analyze the generated data to identify coverage gaps and validate accuracy:</p> <pre><code>from lamini import ErrorAnalysis, FactualErrorAnalysis\nfrom lamini.experiment.validators import (\n    FactualValidator,\n    RelevanceValidator\n)\n\n# Initialize ErrorAnalysis\nerror_analysis = ErrorAnalysis(\n    model=config['model']['default'],\n    knowledge_base=knowledge_base,\n    glossary=formatted_glossary\n)\n\n# Analyze topic coverage\ncoverage_analysis = error_analysis.analyze_concept_coverage(\n    gold_questions=gold_questions,\n    generated_questions=generated_questions\n)\n\n# Generate additional questions for missing concepts\nadditional_questions = []\nif coverage_analysis[\"missing_concepts\"] or coverage_analysis[\"underrepresented_concepts\"]:\n    additional_questions = error_analysis.generate_additional_questions(\n        coverage_analysis=coverage_analysis,\n        num_questions_per_concept=2\n    )\n\n# Analyze answer accuracy\nfactual_error_analysis = FactualErrorAnalysis(model=config['model']['default'])\nincorrect_answers = factual_error_analysis.extract_incorrect_answers(results_data)\n</code></pre>"},{"location":"memory_experiments/factual_qa/#4-memory-tuning","title":"4. Memory Tuning","text":"<p>Fine-tune a model on your generated data:</p> <pre><code>from lamini import Lamini\n\n# Configure Memory Tuning\nmodel_name = config['model']['memory_tuning']\nmax_steps = config['memory_tuning']['max_steps']\nlearning_rate = config['memory_tuning']['learning_rate']\n\n# Submit Memory Tuning job\nllm = Lamini(model_name=model_name)\nresults = llm.tune(\n    data_or_dataset_id=training_data,\n    finetune_args={\n        \"max_steps\": max_steps,\n        \"learning_rate\": learning_rate,\n    },\n    gpu_config={\n        \"max_gpus\": max_gpus,\n        \"max_nodes\": max_nodes\n    }\n)\n</code></pre>"},{"location":"memory_experiments/factual_qa/#5-evaluate-model-performance","title":"5. Evaluate Model Performance","text":"<p>Test your fine-tuned model against the evaluation set:</p> <pre><code>from lamini.experiment.error_analysis_eval import FactualQAPipeline\n\n# Initialize the evaluation pipeline\npipeline = FactualQAPipeline(\n    model=config['model']['default'],\n    knowledge_base=knowledge_base\n)\n\n# Run evaluation\nevaluation_results = pipeline.evaluate_answers(\n    inference_results,\n    source_materials=config['paths']['source_materials']\n)\n\n# Generate and save report\nreport = pipeline.generate_report(evaluation_results)\n</code></pre>"},{"location":"memory_experiments/factual_qa/#custom-components","title":"Custom Components","text":""},{"location":"memory_experiments/factual_qa/#custom-fact-extractor","title":"Custom Fact Extractor","text":"<p>Create a custom fact extractor:</p> <pre><code>from lamini.experiment.generators import BaseGenerator\n\nclass CustomFactExtractor(BaseGenerator):\n    def __init__(self, model, name):\n        instruction = \"\"\"\n        Given the following document, extract key facts and their relationships.\n\n        Document:\n        {source_material}\n\n        Extract facts that are:\n        1. Explicitly stated\n        2. Important for understanding the material\n        3. Can be used to answer potential questions\n        \"\"\"\n\n        super().__init__(\n            name=name,\n            instruction=instruction,\n            model=model,\n            output_type={\"facts\": \"list\"}\n        )\n</code></pre>"},{"location":"memory_experiments/factual_qa/#custom-answer-validator","title":"Custom Answer Validator","text":"<p>Create a custom validator to check answer accuracy:</p> <pre><code>from lamini.experiment.validators import BaseValidator\n\nclass CustomAnswerValidator(BaseValidator):\n    def __init__(self, model, name, knowledge_base):\n        instruction = \"\"\"\n        Validate if the following answer is correct according to the document.\n\n        Question: {question}\n        Given Answer: {answer}\n        Source Material: {source}\n\n        Check for:\n        1. Factual accuracy\n        2. Completeness\n        3. Relevance to the question\n        \"\"\"\n\n        super().__init__(\n            name=name,\n            instruction=instruction,\n            model=model,\n            is_valid_field=\"is_valid\",\n            output_type={\"is_valid\": \"bool\", \"feedback\": \"str\"}\n        )\n</code></pre>"},{"location":"memory_experiments/factual_qa/#best-practices","title":"Best Practices","text":"<ol> <li>Quality Documents: Ensure documents are well-organized and contain clear, factual information</li> <li>Comprehensive Extraction: Extract both explicit facts and implicit relationships</li> <li>Diverse Question Types: Generate questions that test different types of knowledge and reasoning</li> <li>Validation Chain: Use multiple validators to ensure answer accuracy and relevance</li> <li>Knowledge Base Structure: Organize extracted facts in a way that preserves relationships and context</li> </ol>"},{"location":"memory_experiments/factual_qa/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Incorrect Answers: Review fact extraction process and knowledge base coverage</li> <li>Poor Performance: Increase training data diversity or adjust fine-tuning parameters</li> <li>Missing Concepts: Use the analyzer to identify and generate data for uncovered concepts</li> <li>Inconsistent Answers: Check for conflicting information in source materials</li> </ul>"},{"location":"memory_experiments/factual_qa/#conclusion","title":"Conclusion","text":"<p>The Memory Experiment framework provides a robust solution for building factual QA models that can accurately answer questions based on source materials. By following this guide, you can create, fine-tune, and evaluate models that provide reliable and accurate answers in your specific domain.</p>"},{"location":"memory_experiments/generators/","title":"Generators","text":"<p>A generator is a component that takes in a prompt and outputs a structured response. Generators ideally are lightweight and can be run in parallel. Keeping a generator light is important because you can have multiple generators in your pipeline. By light, we mean that the generator should be focusing on a single task and not trying to do too much. A single task can vary from extracting a concept to generating a list of questions from a piece of text.</p> <p>You can create a simple question generator that takes a piece of text and generates a question that can be answered using that text. Here's an example:</p> <pre><code>from lamini.experiment.generators import BaseGenerator\n\n# Create a simple question generator\nquestion_generator = BaseGenerator(\n    name=\"question_generator\",\n    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n    instruction=\"Generate a question that can be answered using this text: {text}\",\n    output_type={\"question\": \"str\"}\n)\n\n# Use the generator\ntext = \"The first human to walk on the moon was Neil Armstrong in 1969.\"\nresult = question_generator(PromptObject(data={\"text\": text}))\nprint(result.response[\"question\"])\n# Example output: \"Who was the first person to walk on the moon?\"\n</code></pre> <p>You can iterate with generators based on the task you want to accomplish. For example, if you want to extract high level concepts from multiple questions, you can use the <code>QuestionsToConceptsGenerator</code>. </p> <pre><code>from lamini.experiment.generators import QuestionToConceptGenerator\n\n# Create a generator\nconcept_generator = QuestionToConceptGenerator(\n    name=\"concept_generator\",\n    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n    instruction=\"Extract a key concept from this question: {question}\",\n    output_type={\"concept\": \"str\"}\n)\n</code></pre> <p>If you want to customize a generator, you can do so by subclassing the BaseGenerator. Here's a complete example:</p> <pre><code>from lamini.experiment.generators import BaseGenerator\nfrom lamini.generation.base_prompt_object import PromptObject\nfrom pydantic import BaseModel\n\nclass ConceptOutput(BaseModel):\n    concept: str\n    confidence: float\n\nclass CustomGenerator(BaseGenerator):\n    def __init__(self):\n        super().__init__(\n            name=\"custom_generator\",\n            instruction=\"Analyze this text and extract the main concept: {text}\",\n            model=\"meta-llama/Llama-3.1-8B-Instruct\",\n            output_type=ConceptOutput\n        )\n\n    def preprocess(self, prompt_obj: PromptObject) -&gt; PromptObject:\n        # Optional: Modify the prompt before generation\n        return prompt_obj\n\n    def postprocess(self, prompt_obj: PromptObject) -&gt; PromptObject:\n        # Optional: Process the results after generation\n        return prompt_obj\n</code></pre> <p>The BaseGenerator provides several key features: - Structured input/output handling - Template-based instructions with metadata injection - Pre and post-processing hooks - Error handling and logging - Result storage and transformation</p>"},{"location":"memory_experiments/generators/#available-generators","title":"Available Generators","text":"Generator Description <code>QuestionsToConceptsGenerator</code> Extracts concepts from multiple questions <code>QuestionToConceptGenerator</code> Extracts concepts from a single question <code>ConceptToSQLInterpretationGenerator</code> Converts concepts into SQL interpretations <code>SchemaToSQLGenerator</code> Generates SQL queries based on database schema <code>SQLDebuggerGenerator</code> Helps debug and fix SQL queries <code>ComparativeQuestionGenerator</code> Generates questions that compare different aspects <code>EdgeCaseQuestionGenerator</code> Creates questions focusing on edge cases and boundary conditions <code>GranularQuestionGenerator</code> Generates detailed, specific questions <code>ParaphrasingQuestionGenerator</code> Rephrases questions in different ways <code>PatternQuestionGenerator</code> Creates questions based on specific patterns <code>QuestionDecomposerGenerator</code> Breaks down complex questions into simpler components <code>VariationQuestionGenerator</code> Generates variations of existing questions <code>SaveGenerator</code> Handles saving and persistence of generator outputs"},{"location":"memory_experiments/generators/#best-practices","title":"Best Practices","text":"<ol> <li>Start with as few generators as possible. For example, just 1 QuestionAnswerGenerator, or 1 QuestionGenerator and 1 AnswerGenerator. Run the pipeline and check the generated data, identify issues.</li> <li>Iterate by modifying the prompt or generators These pre-built generators give you a framework and a gooding starting point. However, it is very likely that you will need to modify each generator to make it work for your use case and specific data. You can experiment with modifying the prompts, decoupling a big generator into small ones, adding more generators, and so on.</li> </ol>"},{"location":"memory_experiments/pipelines/","title":"Agentic Pipeline","text":""},{"location":"memory_experiments/pipelines/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Quick Start</li> <li>Pipeline Components<ul> <li>Generators and Validators</li> <li>Pipeline Steps</li> <li>Result Recording</li> </ul> </li> <li>Pipeline Validation<ul> <li>Step Logic Validation</li> <li>Pipeline Spotcheck</li> </ul> </li> <li>Batch Processing</li> <li>Serialization</li> </ul> <p>The Agentic Pipeline orchestrates a sequence of LLM generators and validators, managing data flow between components and recording execution results. It currently supports single-item with batch processing coming in future releases. Further, the pipeline supports automatic result recording and pipeline validation capabilities.</p>"},{"location":"memory_experiments/pipelines/#quick-start","title":"Quick Start","text":"<p>First, ensure your API key is set up as described in the Getting Started guide.</p> Python SDK <p>Create a basic pipeline with a generator:</p> <pre><code>from lamini.experiment.generators import BaseGenerator\nfrom lamini.experiment.pipeline import BaseAgenticPipeline\nfrom lamini.experiment.base_experiment_object import ExperimentObject\n\n# Create a custom generator\nclass QuestionGenerator(BaseGenerator):\n    def __init__(self, model, name):\n        super().__init__(\n            name=name,\n            model=model,\n            instruction=\"Generate an interesting question based on this fact: {text}\\nMake sure the question requires understanding of the fact to answer correctly.\",\n            output_type={\"question\": \"str\"}  # Changed to output a question\n        )\n\n\n# Initialize generator with required parameters\ngenerator = QuestionGenerator(\n    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n    name=\"question_generator\"\n)\n\n# Create pipeline\npipeline = BaseAgenticPipeline(\n    generators={\"process\": generator},\n    record_dir=\"./pipeline_results\"\n)\n\n# Execute pipeline\nexperiment_obj = ExperimentObject(\n    data={\"text\": \"Hello world\"}\n)\nresults = pipeline(experiment_obj)\n</code></pre>"},{"location":"memory_experiments/pipelines/#pipeline-components","title":"Pipeline Components","text":""},{"location":"memory_experiments/pipelines/#generators-and-validators","title":"Generators and Validators","text":"<p>The pipeline supports both generators and validators. Generators require:</p> <ul> <li>A name</li> <li>An instruction template with metadata keys in curly braces</li> <li>An output type specification (either a Pydantic model or dictionary)</li> <li>Optional role prefix for the prompt</li> <li>Optional pre/post processing methods</li> </ul> <pre><code>from lamini.experiment.validators import BaseValidator\n\n# Create a validator\nclass LengthValidator(BaseValidator):\n    def __init__(self):\n        super().__init__(\n            name=\"length_validator\",\n            instruction=\"Validate text length: {processed_text}\",\n            output_type={\"is_valid\": \"bool\"}\n        )\n\n    def postprocess(self, prompt_obj):\n        # Add validation logic\n        text = prompt_obj.data[\"processed_text\"]\n        prompt_obj.response = {\"is_valid\": len(text) &gt; 10}\n        return prompt_obj\n\n# Create pipeline with both generator and validator\npipeline = BaseAgenticPipeline(\n    generators={\"process\": generator},\n    validators={\"length_check\": LengthValidator()},\n    order=[\"process\", \"length_check\"],  # Specify execution order\n    record_dir=\"./pipeline_results\"\n)\n</code></pre>"},{"location":"memory_experiments/pipelines/#pipeline-steps","title":"Pipeline Steps","text":"<p>Each component in the pipeline is wrapped in a PipelineStep, which manages:</p> <ul> <li>The worker (generator or validator)</li> <li>Queue of items to process</li> <li>Connection to the next step</li> </ul>"},{"location":"memory_experiments/pipelines/#result-recording","title":"Result Recording","text":"<p>The pipeline automatically records results in the specified <code>record_dir</code>:</p> <pre><code>pipeline = BaseAgenticPipeline(\n    generators={\"process\": generator},\n    record_dir=\"./experiment_results\",\n    record_step=True,  # Record intermediate results\n    record_results=True  # Record final results\n)\n</code></pre> <p>Results are saved in JSON Lines format:</p> <ul> <li><code>{step_name}_full.jsonl</code>: Complete data including prompts and responses</li> <li><code>{step_name}_data_io.jsonl</code>: Input/output data for each step</li> <li><code>{step_name}_prompt_response.jsonl</code>: Prompt-response pairs</li> <li><code>data_io.jsonl</code>: Final aggregated input/output data</li> </ul>"},{"location":"memory_experiments/pipelines/#pipeline-validation","title":"Pipeline Validation","text":"<p>The pipeline includes built-in validation mechanisms. These mechanisms are run automatically before the pipeline is run through the entire set of prompts provided. This is to ensure that the pipeline is working as expected early, as the pipeline may take a while to run through the entire set of prompts.</p> <p>The two pipeline validation mechanisms are: 1. Step Logic Validation     Verifies input/output key compatibility between steps, i.e. compile the pipeline can check input/output keys of each step are compatible. 2. Pipeline Spotcheck     Tests the runtime functionality of the pipeline, by executing each step within the pipeline.</p>"},{"location":"memory_experiments/pipelines/#step-logic-validation","title":"Step Logic Validation","text":"<p>Verifies input/output key compatibility between steps, as a user can unknowingly change the output keys of a step but the later steps may not have been updated to handle the new keys. This validation ensures that the pipeline is working as expected.</p> <pre><code># Pipeline automatically validates that each step's required inputs\n# are available from previous steps' outputs\npipeline.pipeline_step_logic(experiment_object)\n</code></pre>"},{"location":"memory_experiments/pipelines/#pipeline-spotcheck","title":"Pipeline Spotcheck","text":"<p>Tests the complete pipeline with a single sample:</p> <pre><code># Executes one sample through all steps to verify functionality\npipeline.pipline_spotcheck(experiment_object)\n</code></pre>"},{"location":"memory_experiments/pipelines/#batch-processing","title":"Batch Processing","text":"<p>The pipeline supports batch processing of multiple inputs:</p> <pre><code># Create multiple experiment objects\nexperiment_objects = [\n    ExperimentObject(\n        data={\"text\": \"First input\"}\n    ),\n    ExperimentObject(\n        data={\"text\": \"Second input\"}\n    )\n]\n\n# Process batch\nresults = pipeline(experiment_objects)\n</code></pre>"},{"location":"memory_experiments/pipelines/#serialization","title":"Serialization","text":"<p>Pipelines can be saved and loaded using JSON:</p> <pre><code># Save pipeline configuration\nconfig = pipeline.to_json()\n\n# Load pipeline from configuration\nloaded_pipeline = BaseAgenticPipeline.from_json(config)\n</code></pre> <p>The Agentic Pipeline serves as the foundation for more complex experiments like MemoryExperiment and MemoryRAGExperiment, providing robust execution, validation, and recording capabilities for systematic LLM development.</p>"},{"location":"memory_experiments/prompt_formatting/","title":"Generator and Validator Prompt Engineering","text":"<p>Both Generators and Validators utilize python string formatting to inject variables into the prompt. For example, if you want to inject the <code>content</code> into the prompt, you can do so by using the following syntax:</p> <pre><code>prompt = \"\"\"\nFirst, review the following content:\n\n{chunk}\n\nNext, extract a fact within that content above. Be sure the fact is \nonly one sentence and is directly coming from the provided content above.\n\"\"\"\n</code></pre> <p>Inject variables are designated by curly braces <code>{}</code>. This indicates to the generator or validator that the variable should be coming from the <code>data</code> attribute of the <code>PromptObject</code> during pipeline execution.</p>"},{"location":"memory_experiments/prompt_formatting/#promptobject-data","title":"PromptObject Data","text":"<p>So before the pipeline is executed, the <code>PromptObject</code> will be populated with the <code>data</code> attribute.</p> <pre><code>prompt_obj = PromptObject(\n    data={\"chunk\": \"This is a test\"}\n)\n</code></pre> <p>When the pipeline is executed, the <code>PromptObject</code> will be populated with the <code>data</code> attribute.</p> <pre><code>results = pipeline(prompt_obj)\n</code></pre> <p>Batch processing is also supported through providing a list of <code>PromptObject</code> instances to the pipeline. All PromptObjects needs to have the same keys in the <code>data</code> attribute.</p> <pre><code>prompt_objs = [\n    PromptObject(data={\"chunk\": \"This is a test\"}),\n    PromptObject(data={\"chunk\": \"This is another test\"})\n]  \n\nresults = pipeline(prompt_objs)\n</code></pre>"},{"location":"memory_experiments/txt2sql/","title":"Text-to-SQL with Memory Experiment","text":"<p>The Memory Experiment framework provides powerful tools for creating, validating, and improving text-to-SQL models. This document outlines how to use the framework for building and fine-tuning text-to-SQL models for domain-specific databases.</p>"},{"location":"memory_experiments/txt2sql/#overview","title":"Overview","text":"<p>Text-to-SQL LLMs enable users to query databases using natural language questions. The Memory Experiment framework streamlines the process of:</p> <ol> <li>Generating training data based on sample questions</li> <li>Validating generated SQL queries</li> <li>Analyzing coverage of SQL concepts</li> <li>Fine-tuning models on generated data</li> <li>Evaluating model performance</li> </ol> <p>The complete pipeline helps you create domain-specific text-to-SQL models with high accuracy and comprehensive coverage of database concepts.</p>"},{"location":"memory_experiments/txt2sql/#getting-started","title":"Getting Started","text":""},{"location":"memory_experiments/txt2sql/#prerequisites","title":"Prerequisites","text":"<p>Before starting, ensure you have:</p> <ul> <li>A Lamini API key (get yours at app.lamini.ai)</li> <li>Python 3.7+ with pip installed</li> <li>The Lamini Python SDK: <code>pip install lamini</code></li> </ul>"},{"location":"memory_experiments/txt2sql/#required-inputs","title":"Required Inputs","text":"<p>To build a text-to-SQL model with Memory Experiment, you'll need:</p> <ol> <li>Database Schema: SQL schema of your target database</li> <li>Glossary File: JSON file with domain-specific terminology, abbreviations, and business logic</li> <li>Evaluation Set: 20-35 sample questions with corresponding SQL queries</li> <li>SQLite Database: For testing and validating generated queries</li> </ol>"},{"location":"memory_experiments/txt2sql/#step-by-step-guide","title":"Step-by-Step Guide","text":""},{"location":"memory_experiments/txt2sql/#1-setup-project-structure","title":"1. Setup Project Structure","text":"<p>Create a directory for your text-to-SQL project:</p> <pre><code>mkdir txt2sql_project\ncd txt2sql_project\n</code></pre> <p>Prepare your input files:</p> <ul> <li><code>glossary.json</code>: Key-value pairs of domain terms and their definitions</li> <li><code>eval_set.jsonl</code>: Question-SQL pairs for evaluation</li> <li><code>database.sqlite</code>: SQLite database for testing and fetching the schema</li> </ul> <p>Create a <code>config.yml</code> file to manage API keys and other settings:</p> <pre><code>api:\n  url: \"https://app.lamini.ai\"\n  key: \"YOUR_LAMINI_API_KEY\"\n\nmodel:\n  default: \"meta-llama/Llama-3.1-8B-Instruct\"\n  memory_tuning: \"meta-llama/Llama-3.1-8B-Instruct\"\n\ndatabase:\n  path: \"path/to/your/database.sqlite\"\n\npaths:\n  gold_test_set: \"path/to/your/eval_set.jsonl\"\n  glossary: \"path/to/your/glossary.jsonl\"\n\nmemory_tuning:\n  max_steps: 1000\n  learning_rate: 3e-5\n  max_gpus: 1\n  max_nodes: 1\n</code></pre>"},{"location":"memory_experiments/txt2sql/#2-generate-training-data","title":"2. Generate Training Data","text":"<p>Generate variations of questions and SQL queries based on your evaluation set:</p> <pre><code>import os\nimport lamini\nimport pathlib\nimport datetime\n\nfrom lamini.experiment.generators import (\n    SchemaToSQLGenerator,\n    SQLDebuggerGenerator,\n    PatternQuestionGenerator,\n    VariationQuestionGenerator,\n    QuestionDecomposerGenerator\n)\nfrom lamini.experiment.validators import (\n    SQLValidator\n)\n\nfrom helpers import (\n    load_config,\n    process_jsonl,\n    read_jsonl, \n    get_schema, \n    format_glossary, \n    save_results, \n    generate_variations,\n    process_variation\n)\n\n# Load configuration\nconfig = load_config(\"config.yml\")\n\n# Set up Lamini API credentials\nlamini.api_url = config['api']['url']\nlamini.api_key = config['api']['key']\n\n# Initialize generators and validators\ngenerators = {\n    \"pattern\": PatternQuestionGenerator(model=config['model']['default']),\n    \"variation\": VariationQuestionGenerator(model=config['model']['default']),\n    \"decomposer\": QuestionDecomposerGenerator(model=config['model']['default'])\n}\n\nsql_gen = SchemaToSQLGenerator(\n    model=config['model']['default'],\n    db_type=\"sqlite\",\n    db_params=str(config['database']['path']),\n    schema=get_schema(config['database']['path'])\n)\n\n# Process evaluation questions to generate training data\n# (See generate_data.py in the repository for the complete implementation)\n</code></pre>"},{"location":"memory_experiments/txt2sql/#3-analyze-generated-data","title":"3. Analyze Generated Data","text":"<p>Analyze the generated data to identify missing concepts and problematic queries:</p> <pre><code>import os\nimport json\nimport lamini\nfrom typing import List, Dict\nfrom lamini import ErrorAnalysis, SQLErrorAnalysis\nfrom lamini.experiment.generators import (\n    SchemaToSQLGenerator,\n    SQLDebuggerGenerator\n)\nfrom lamini.experiment.validators import (\n    SQLValidator\n)\n\n# Initialize ErrorAnalysis\nerror_analysis = ErrorAnalysis(\n    model=config['model']['default'],\n    schema=schema,\n    glossary=formatted_glossary\n)\n\n# Analyze topic coverage\ncoverage_analysis = error_analysis.analyze_topic_coverage(\n    gold_questions=gold_questions,\n    generated_questions=generated_questions\n)\n\n# Generate additional questions for missing topics\nadditional_questions = []\nif coverage_analysis[\"missing_topics\"] or coverage_analysis[\"underrepresented_topics\"]:\n    additional_questions = error_analysis.generate_additional_questions(\n        coverage_analysis=coverage_analysis,\n        num_questions_per_topic=2\n    )\n\n# Analyze SQL errors\nsql_error_analysis = SQLErrorAnalysis(model=config['model']['default'])\nfailed_queries = sql_error_analysis.extract_failed_queries(results_data)\n\n# (See analyze_generated_data.py in the repository for the complete implementation)\n</code></pre>"},{"location":"memory_experiments/txt2sql/#4-memory-tuning","title":"4. Memory Tuning","text":"<p>Fine-tune a model on your generated data:</p> <pre><code>import os\nimport lamini\nfrom lamini import Lamini\nfrom helpers import read_jsonl, load_config\n\n# Load configuration\nconfig = load_config(\"config.yml\")\n\n# Set up Lamini API credentials\nlamini.api_url = config['api']['url']\nlamini.api_key = config['api']['key']\n\n# Read training data\ninput_file = \"flattened_training_data.jsonl\"\nrows = read_jsonl(input_file)\n\n# Configure Memory Tuning\nmodel_name = config['model']['memory_tuning']\nmax_steps = config['memory_tuning']['max_steps']\nlearning_rate = config['memory_tuning']['learning_rate']\nmax_gpus = config['memory_tuning']['max_gpus']\nmax_nodes = config['memory_tuning']['max_nodes']\n\n# Submit Memory Tuning job\nllm = Lamini(model_name=model_name)\nresults = llm.tune(\n    data_or_dataset_id=rows,\n    finetune_args={\n        \"max_steps\": max_steps,\n        \"learning_rate\": learning_rate,\n    },\n    gpu_config={\n        \"max_gpus\": max_gpus,\n        \"max_nodes\": max_nodes\n    }\n)\n\n# (See memory_tuning.py in the repository for the complete implementation)\n</code></pre>"},{"location":"memory_experiments/txt2sql/#5-evaluate-model-performance","title":"5. Evaluate Model Performance","text":"<p>Test your fine-tuned model against the evaluation set:</p> <pre><code>import json\nimport os\nimport lamini\nfrom lamini import Lamini\nfrom helpers import save_results_to_jsonl, load_config\nfrom lamini.experiment.error_analysis_eval import SQLExecutionPipeline\n\n# Load configuration\nconfig = load_config(\"config.yml\")\n\n# Set up Lamini API credentials\nlamini.api_url = config['api']['url']\nlamini.api_key = config['api']['key']\n\n# Initialize the model with your fine-tuned model ID\nmodel_id = \"YOUR_TUNED_MODEL_ID\"  # Replace with your model ID from Memory Tuning\nllm = Lamini(model_name=model_id)\n\n# Read test cases\ntest_cases = read_test_set(config['paths']['gold_test_set'])\n\n# Run inference\ninference_results = run_inference(test_cases, model_id)\n\n# Initialize SQL execution pipeline for evaluation\npipeline = SQLExecutionPipeline(\n    model=config['model']['default'],\n    db_type=\"sqlite\"\n)\n\n# Run evaluation\nevaluation_results = pipeline.evaluate_queries(\n    inference_results,\n    connection_params={\"db_path\": config['database']['path']}\n)\n\n# Generate and save report\nreport = pipeline.generate_report(evaluation_results)\n\n# (See run_inference.py in the repository for the complete implementation)\n</code></pre>"},{"location":"memory_experiments/txt2sql/#custom-components","title":"Custom Components","text":""},{"location":"memory_experiments/txt2sql/#custom-sql-generator","title":"Custom SQL Generator","text":"<p>Create a custom SQL question generator by subclassing the base generator:</p> <pre><code>from lamini.experiment.generators import BaseGenerator\n\nclass CustomSQLQuestionGenerator(BaseGenerator):\n    def __init__(self, model, name):\n        instruction = \"\"\"\n        Given the following database schema and glossary, generate {num_variations} \n        variations of each question in the list of questions.\n\n        Schema:\n        {schema}\n\n        Glossary:\n        {glossary}\n\n        Questions:\n        {questions}\n\n        Generate diverse variations that test different SQL concepts while \n        maintaining the same intent as the original questions.\n        \"\"\"\n\n        super().__init__(\n            name=name,\n            instruction=instruction,\n            model=model,\n            output_type={\"variations\": \"list\"}\n        )\n\n    def postprocess(self, prompt):\n        # Add any custom post-processing logic\n        return prompt\n</code></pre>"},{"location":"memory_experiments/txt2sql/#custom-sql-validator","title":"Custom SQL Validator","text":"<p>Create a custom SQL validator to check query correctness:</p> <pre><code>from lamini.experiment.validators import BaseValidator\n\nclass CustomSQLValidator(BaseValidator):\n    def __init__(self, model, name, db_path):\n        instruction = \"\"\"\n        Validate if the following SQL query is correct and will run against \n        the given database schema.\n\n        Schema:\n        {schema}\n\n        Query:\n        {query}\n\n        Provide detailed feedback on any issues found.\n        \"\"\"\n\n        super().__init__(\n            name=name,\n            instruction=instruction,\n            model=model,\n            is_valid_field=\"is_valid\",\n            output_type={\"is_valid\": \"bool\", \"feedback\": \"str\"}\n        )\n\n        self.db_path = db_path\n\n    def validate(self, prompt):\n        # Add custom validation logic using SQLite\n        # Example: Execute query against database and catch errors\n        return prompt\n</code></pre>"},{"location":"memory_experiments/txt2sql/#complete-example","title":"Complete Example","text":"<p>For a complete, working example of a text-to-SQL pipeline, visit our GitHub repository:</p> <p>SQL Generation &amp; Analysis Pipeline</p> <p>The repository contains all the scripts mentioned in this documentation:</p> <ul> <li><code>generate_data.py</code>: Generate training questions and SQL queries</li> <li><code>analyze_generated_data.py</code>: Analyze coverage and generate additional data</li> <li><code>memory_tuning.py</code>: Fine-tune a model on generated data</li> <li><code>run_inference.py</code>: Evaluate model performance</li> </ul>"},{"location":"memory_experiments/txt2sql/#best-practices","title":"Best Practices","text":"<ol> <li>Quality Glossary: Create a comprehensive glossary covering domain-specific terms, abbreviations, and business logic</li> <li>Diverse Evaluation Set: Include various query types (SELECT, JOIN, GROUP BY, etc.) in your evaluation set</li> <li>Iterative Improvement: Analyze failed queries and use them to improve your generators, validators and glossary</li> <li>Schema: Add key-value pairs about the schema to the glosary for the model to understand table relationships</li> <li>Validation Checks: Use multiple validators to ensure query correctness, execution, and result accuracy</li> </ol>"},{"location":"memory_experiments/txt2sql/#troubleshooting","title":"Troubleshooting","text":"<ul> <li>Invalid SQL Generation: Check your schema formatting and ensure glossary covers all domain terms</li> <li>Poor Performance: Increase the diversity of your generated data or fine-tune for more steps</li> <li>Missing Concepts: Use the analyzer to identify and generate data for uncovered concepts</li> <li>Execution Errors: Verify your SQLite database matches the schema exactly</li> </ul>"},{"location":"memory_experiments/txt2sql/#conclusion","title":"Conclusion","text":"<p>The Memory Experiment framework provides a powerful and flexible way to build text-to-SQL models for domain-specific databases. By following this guide, you can create, fine-tune, and evaluate models that accurately convert natural language questions to SQL queries in your specific domain.</p>"},{"location":"memory_experiments/validators/","title":"Validators","text":"<p>Validators are components that take in a prompt and output a structured response to flag whether the output of a generator is valid or not. Validators are used to validate the output of a generator. For example, you can use a factual validator to ensure that the output of a generator is factually correct with respect to a provided text.</p> <pre><code>from lamini.experiment.validators import FactualityValidator\n\n# Create validator\nfactual_validator = FactualityValidator(\n    name=\"factual_validator\",\n    instruction=\"Validate if the following concept is factually accurate: {concept}\",\n    model=\"meta-llama/Llama-3.1-8B-Instruct\",\n)\n\n# Update pipeline\npipeline = BaseAgenticPipeline(\n    generators={\"concept\": concept_generator},\n    validators={\"factual\": factual_validator},\n    order=[\"concept\", \"factual\"],\n    record_dir=\"./experiment_results\"\n)\n</code></pre> <p>If you want to customize a validator, you can do so by subclassing <code>BaseValidator</code> and implementing the required initialization parameters:</p> <pre><code>from lamini.experiment.validators import BaseValidator\n\nclass CustomValidator(BaseValidator):\n    def __init__(self):\n        super().__init__(\n            name=\"custom_validator\",\n            instruction=\"Your validation instruction here: {input}\",\n            is_valid_field=\"is_valid\"  # Optional: specify custom validation field name\n        )\n\n    def validate(self, prompt: PromptObject) -&gt; PromptObject:\n        # Custom validation logic here\n        return prompt\n</code></pre> <p>Validators are different from generators in that they must include a boolean validation field in their output. This is enforced by the <code>BaseValidator</code> class, which can handle the validation field in three ways:</p> <ol> <li>Using the default <code>is_valid</code> field if no custom output type is specified</li> <li>Using a custom validation field name specified by <code>is_valid_field</code></li> <li>Including a boolean field in a custom <code>output_type</code> dictionary</li> </ol> <p>For example:</p> <pre><code># Default validation field\nvalidator = CustomValidator(\n    name=\"validator\",\n    instruction=\"Validate this: {input}\"\n)  # Uses \"is_valid\" field\n\n# Custom validation field name\nvalidator = CustomValidator(\n    name=\"validator\",\n    instruction=\"Validate this: {input}\",\n    is_valid_field=\"passed_validation\"\n)\n\n# Custom output type with validation field\nvalidator = CustomValidator(\n    name=\"validator\",\n    instruction=\"Validate this: {input}\",\n    is_valid_field=\"is_valid\",\n    output_type={\n        \"is_valid\": \"bool\",\n        \"confidence\": \"float\",\n        \"reason\": \"str\"\n    }\n)\n</code></pre>"},{"location":"memory_experiments/validators/#available-validators","title":"Available Validators","text":"Validator Description <code>FactualityValidator</code> Validates whether generated content is factually accurate with respect to provided reference text <code>SQLValidator</code> Validates SQL query syntax and structure <code>SQLScoreValidator</code> Validates SQL queries by comparing their execution results <code>BaseSQLValidator</code> Base class for SQL-related validators <code>BaseValidator</code> Base class for creating custom validators"},{"location":"memory_experiments/validators/#best-practices","title":"Best Practices","text":"<ol> <li>Start with as few validators as possible. For example, just 1 FactualityValidator, or 1 SQLValidator and 1 SQLScoreValidator. Run the pipeline and check the generated data, identify issues.</li> <li>Iterate by modifying the prompt or validators These pre-built validators give you a framework and a gooding starting point. However, it is very likely that you will need to modify each validator to make it work for your use case and specific data. You can experiment with modifying the prompts, decoupling a big validator into small ones, adding more validators, and so on.</li> <li>Test run validators Make sure you run tests and validate that each validator is validating correctly! You may want to iterate on the prompt and model used for the validator (anything else?)</li> </ol>"},{"location":"memory_rag/","title":"Memory RAG","text":"<p>Memory RAG is a simple approach that boosts LLM accuracy from ~50% to 90-95% compared to GPT-4. It creates contextual embeddings that capture meaning and relationships, allowing smaller models to achieve high accuracy without complex RAG setups or fine-tuning overhead.</p>"},{"location":"memory_rag/#quick-start-with-python-or-rest-api","title":"Quick Start with Python or REST API","text":"<p>First, make sure your API key is set (get yours at app.lamini.ai):</p> TerminalPython SDK <pre><code>export LAMINI_API_KEY=\"&lt;YOUR-LAMINI-API-KEY&gt;\"\n</code></pre> <pre><code>import lamini\nimport os\n\nlamini.api_key = os.environ[\"LAMINI_API_KEY\"]\n</code></pre> <p>To use Memory RAG, build an index by uploading documents and selecting a base open-source LLM. This is the model you'll use to query over your documents.</p> Python SDKREST API <p>Initialize the Memory RAG client:</p> <pre><code>from lamini import MemoryRAG\n\nclient = MemoryRAG(\"meta-llama/Llama-3.1-8B-Instruct\")\n</code></pre> <p>Find a PDF file to embed: <pre><code>lamini_wikipedia_page_pdf = (\"https://huggingface.co/datasets/lamini/\"\n                            \"lamini-wikipedia-page/blob/main/\"\n                            \"Lamini-wikipedia-page.pdf\")\n</code></pre></p> <p>Download the PDF file: <pre><code>import requests\nimport os\n\n# Download the PDF file\nresponse = requests.get(lamini_wikipedia_page_pdf)\n\n# Save locally\npdf_path = \"lamini_wikipedia.pdf\"\nwith open(pdf_path, \"wb\") as f:\n    f.write(response.content)\n</code></pre></p> <p>Embed and build the Memory RAG Index: <pre><code>response = client.memory_index(documents=[pdf_path])\n</code></pre></p> <p>Expected <code>response</code>: <pre><code>{'job_id': 1, 'status': 'CREATED'}\n</code></pre></p> <pre><code>curl --location 'https://huggingface.co/datasets/lamini/lamini-wikipedia-page/resolve/main/Lamini-wikipedia-page.pdf' --output lamini_wikipedia.pdf\n\ncurl --location 'https://api.lamini.ai/alpha/memory-rag/train' \\\n    --header 'Authorization: Bearer $LAMINI_API_KEY' \\\n    --form 'files=@\"lamini_wikipedia.pdf\"' \\\n    --form 'model_name=\"meta-llama/Llama-3.1-8B-Instruct\"'\n</code></pre> <p>Expected response: <pre><code>{\"job_id\":1,\"status\":\"CREATED\"}\n</code></pre></p> <p>Next, Memory embedding and indexing will complete. Poll for status:</p> Python SDKREST API <pre><code>job_id = response['job_id']\n\nstatus = client.status(job_id)\n</code></pre> <pre><code>curl --location 'https://api.lamini.ai/alpha/memory-rag/status' \\\n    --header 'Authorization: Bearer $LAMINI_API_KEY' \\\n    --header 'Content-Type: application/json' \\\n    --data '{\n        \"job_id\": 1\n    }'\n</code></pre> <p>Finally, run the Memory RAG model:</p> Python SDKREST API <p>Create a prompt: <pre><code>user_prompt = \"How is lamini related to llamas?\"\nprompt_template = \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n {prompt} &lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n\"\nprompt = prompt_template.format(prompt=user_prompt)\n</code></pre></p> <p>Pass the prompt to the Memory RAG model: <pre><code>response = client.query(prompt)\n</code></pre></p> <pre><code>curl --location 'https://api.lamini.ai/alpha/memory-rag/completions' \\\n    --header 'Authorization: Bearer $LAMINI_API_KEY' \\\n    --header 'Content-Type: application/json' \\\n    --data '{\n        \"prompt\": \"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\\n\\n How are you? &lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\\n\\n\",\n        \"job_id\": 1,\n        \"rag_query_size\": 3,\n        \"model_name\": \"meta-llama/Llama-3.1-8B-Instruct\"\n    }'\n</code></pre> <p>If it's not yet ready, you'll get this response: <pre><code>{\"detail\":\"Trained memory rag model 15505 status is not COMPLETED, current status: CREATED.\"}\n</code></pre></p>"},{"location":"memory_rag/#iteration","title":"Iteration","text":"<p>You are now ready to run evaluation of this model. To do so, build out an evaluation set that consists of Question/Answer pairs for the expected answers you have for the provided question. The more representative your questions and answer pairs are to your production use case, the better the model is evaluated. Rate the models performance in reference to this evaluation set. If the model performs poorly, try iterating on the Memory Rag job with additional data. If the model performs well enough, then you can consider it ready for production!</p>"},{"location":"self_managed/OIDC/","title":"User Authentication with OIDC","text":"<p>Note</p> <p>OIDC authentication is only available when self-managing Lamini Platform. Contact us to learn more.</p> <p>Lamini Platform supports Open ID Connect (OIDC) for user authentication. When enabled, only OIDC-authenticated users are able to access your Lamini instance. When an unauthenticated user tries to access your Lamini instance, they will be redirected to the OIDC identity provider you specify to log in. You can use any vendor-provided OIDC provider (like Auth0, Okta, AWS IAM, GCP Identity Platform, Azure Entra, and many more) or any internal service that adheres to the OIDC standard.</p> <p>After a user has signed in to Lamini Platform, they can create API keys and authenticate requests as described in API authentication.</p>"},{"location":"self_managed/OIDC/#setup-flow","title":"Setup flow","text":"<ol> <li>Determine the URI where your Lamini Platform instance will run.</li> <li>Create an application in your OIDC provider for Lamini and configure it.<ol> <li>Example configuration: Auth0<ol> <li>Application Type: <code>Regular Web Application</code></li> <li>Login URL: <code>https://&lt;LAMINI_INSTANCE_URI&gt;/v1/auth/login</code></li> <li>Callback URL <code>https://&lt;LAMINI_INSTANCE_URI&gt;/v1/auth/auth</code></li> <li>Logout URL <code>https://&lt;LAMINI_INSTANCE_URI&gt;</code></li> <li>Web origins <code>https://&lt;LAMINI_INSTANCE_URI&gt;</code></li> </ol> </li> <li>Example configuration: Google<ol> <li>Redirect URI: <code>https://&lt;LAMINI_INSTANCE_URI&gt;/v1/auth/auth</code></li> <li>Authorized Origin: <code>https://&lt;LAMINI_INSTANCE_URI&gt;</code></li> </ol> </li> </ol> </li> <li>Get the Application Client ID, Application Client Secret, and the OIDC Connect URL for your OIDC provider.<ol> <li>Example URL: Auth0: <code>https://&lt;YOUR-AUTH0-APP&gt;/.well-known/openid-configuration</code></li> <li>Example URL: Google: <code>https://accounts.google.com/.well-known/openid-configuration</code></li> </ol> </li> <li> <p>Configure OIDC in the <code>llama_config_edits.yaml</code> file for your Lamini install</p> <ol> <li>Set <code>disable_auth</code> to <code>False</code> to enable auth</li> </ol> <pre><code>auth:\n  disable_auth: False\n</code></pre> <ol> <li>Set <code>website</code> to the URI of your Lamini Platform instance</li> </ol> <pre><code>powerml:\n  website: \"https://&lt;LAMINI_INSTANCE_URI&gt;\"\n</code></pre> <ol> <li>Set the <code>client_id</code>, <code>client_secret</code>, and <code>server_metadata_url</code> values</li> </ol> <pre><code>auth_provider:\n  client_id: \"&lt;CLIENT_ID&gt;\"\n  client_secret: \"&lt;CLIENT_SECRET&gt;\"\n  server_metadata_url: \"&lt;OIDC_CONNECT_URL&gt;\"\n</code></pre> </li> </ol>"},{"location":"self_managed/auth_setup/","title":"Auth Setup","text":""},{"location":"self_managed/auth_setup/#prerequisites","title":"Prerequisites","text":"<p>This example uses auth0.com as the authentication provider.</p>"},{"location":"self_managed/auth_setup/#step-1-configure-authentication","title":"Step 1: Configure Authentication","text":"<p>Configure Auth0 as shown:</p> <p></p> <p>Obtain the necessary Auth0 information:</p> <p></p> <p>set auth.disable_auth to False in llama_config_edits.yaml:</p> <pre><code>auth:\n    disable_auth: False\n</code></pre> <p>Add the following credentials from your application setup on Auth0 in llama_config_edits.yaml:</p> <pre><code>auth_provider:\n    client_id: &lt;client_id&gt;\n    client_secret: &lt;client_secret&gt;\n    server_metadata_url: &lt;server_metadata_url&gt;\n</code></pre> <p>Run the ./generate_helm_charts.sh script to regenerate the lamini folder.</p>"},{"location":"self_managed/auth_setup/#step-2-add-a-new-user-and-token","title":"Step 2: Add a New User and Token","text":"<p>After installation, sign up a new user. We recommend using a group email such as engineer@example.com so that access isn't tied to a specific employee's account.</p> <p>Replace the test_token in llama_config_edits.yaml with the API key of engineer@example.com:</p> <pre><code>auth:\n    test_token: &lt;api_key&gt;\n</code></pre> <p>Delete the existing lamini folder, then run ./generate_helm_charts.sh again to regenerate it.</p>"},{"location":"self_managed/auth_setup/#step-3-set-the-admin-token","title":"Step 3: Set the Admin Token","text":"<p>Update the admin_token in lamini/values.yaml with the corresponding API key:</p> <pre><code>security:\n  admin_token: &lt;api_key&gt;\n</code></pre>"},{"location":"self_managed/auth_setup/#step-4-disable-the-test-token-optional","title":"Step 4: Disable the Test Token (Optional)","text":"<p>You can disable the test token in the database with the following SQL command:</p> <pre><code>update tokens set deleted = true where token='test_token';\n</code></pre>"},{"location":"self_managed/aws_eks_setup/","title":"AWS EKS Setup","text":"<p>Note</p> <p>The Lamini installer is only available when self-managing Lamini Platform. Contact us to learn more.</p>"},{"location":"self_managed/aws_eks_setup/#summary","title":"Summary","text":"<p>This installation covers the steps of setting up an AWS EKS cluster, and creating a S3-backed NFS storage gateway for installing Lamini Platform.</p> <ul> <li>Prerequisites</li> <li>Install AWS CLI</li> <li>Configure AWS credential</li> <li>Create Key Pair</li> <li>Create EKS Cluster</li> <li>Set Up NFS</li> <li>Install Lamini Installer</li> </ul>"},{"location":"self_managed/aws_eks_setup/#prerequisites","title":"Prerequisites","text":"<p>An AWS account with GPU instances available, such as the G6e type or higher.</p>"},{"location":"self_managed/aws_eks_setup/#install-aws-cli","title":"Install AWS CLI","text":"<p>Follow the AWS instruction to install the AWS CLI Check the AWS CLI installation.</p> <pre><code>aws --version\n</code></pre>"},{"location":"self_managed/aws_eks_setup/#configure-aws-credential","title":"Configure AWS credential","text":"<p>Create and get the AWS Access Key ID and AWS Secret Access Key.</p> <p> </p> <p>Configure AWS credential.</p> <pre><code>aws configure\n</code></pre> <p></p> <pre><code>aws eks update-kubeconfig --name &lt;eks-cluster-name&gt;\n</code></pre> <p></p>"},{"location":"self_managed/aws_eks_setup/#create-key-pair","title":"Create Key Pair","text":"<p>Create Key Pair with AWS CLI, this is used to get SSH access to the nodes in the EKS cluster.</p> <pre><code>aws ec2 create-key-pair --key-name my-eks-keypair --query 'KeyMaterial' --output text &gt; my-eks-keypair.pem\nchmod 400 my-eks-keypair.pem\n</code></pre>"},{"location":"self_managed/aws_eks_setup/#create-eks-cluster","title":"Create EKS Cluster","text":"<p>Use <code>eksctl</code> to manage EKS clusters. Eksctl is a \"battery-included\" tool, which manages many dependent aspects of EKS cluster. For example, it automatically install GPU plugins when requesting GPU instances, which is not the case when manually create with EKS web UI or <code>aws</code> CLI.</p> <p>To install eksctl on mac:</p> <pre><code>brew install eksctl\n</code></pre> <p>To Create a new EKS cluster:</p> <pre><code>eksctl create cluster \\\n  --name=&lt;name&gt; \\\n  --region=&lt;region&gt; \\\n  --node-type=&lt;type&gt; \\\n  --nodes 2 \\\n  --nodes-min=2 \\\n  --nodes-max=2 \\\n  --managed \\\n  --ssh-access \\\n  --ssh-public-key=my-eks-keypair\n</code></pre> <p>To add a new node group to an existing EKS cluster:</p> <pre><code>eksctl create nodegroup \\\n  --cluster=&lt;cluster-name&gt; \\\n  --name=&lt;node-type&gt;-group \\\n  --region=&lt;region&gt; \\\n  --nodes=1 \\\n  --nodes-min=1 \\\n  --nodes-max=1 \\\n  --node-type=&lt;node-type&gt; \\\n  --managed \\\n  --ssh-access \\\n  --ssh-public-key=my-eks-keypair\n</code></pre> <p>To remove a node group from an existing EKS cluster:</p> <pre><code>eksctl delete nodegroup \\\n  --cluster=&lt;cluster-name&gt; \\\n  --name=&lt;node-group-name&gt; \\\n  --region=&lt;region&gt;\n</code></pre> <p>To get access to a EKS cluster:</p> <pre><code>REGION=&lt;your-region&gt;\neksctl get cluster --region=${REGION}\n# This will write kube config for the cluster\neksctl utils write-kubeconfig --cluster=yaxiong-test-3 --region=${REGION}\nkubectl get node\n</code></pre>"},{"location":"self_managed/aws_eks_setup/#set-up-nfs","title":"Set Up NFS","text":"<p>Create AWS S3 File Gateway. Type Storage Gateway in the search bar of AWS Console. </p> <p>Click on Create gateway. </p> <p>Enter gateway name and timezone. </p> <p>Launch EC2 instance as the gateway instance, choose the following options: Platform options: Amazon EC2; VPC network: Select the VPC of the created EKS cluster; Key pair: Select the created instance key pair; then click Launch the instance. </p> <p>Connect to AWS - select the IP address connection option and publicly accessible endpoint option. </p> <p>Activate gateway. </p> <p>Configure after activating the gateway. </p> <p>Create file share. </p> <p>Select the gateway that just created, set NFS protocol, and then create the S3 bucket.  </p> <p>Select the S3 that was just created.</p> <p></p> <p>Create the file share.</p> <p></p> <p>Add the file share client access restriction as needed.</p> <p> </p> <p>Note down the NFS IP and path that will be used in the NFS setup for Lamini installation. They are used as NFS_IP in NFS_SUBFOLDER_PATH for installing NFS provisioner when installing Lamini Platform.</p> <p></p>"},{"location":"self_managed/aws_eks_setup/#install-lamini-platform-with-installer","title":"Install Lamini Platform with installer","text":"<p>Follow the Kubernetes installation guide to install Lamini Platform with the installer.</p>"},{"location":"self_managed/disaster_recovery/","title":"Disaster recovery","text":""},{"location":"self_managed/disaster_recovery/#scenarios-and-actions","title":"Scenarios and actions","text":""},{"location":"self_managed/disaster_recovery/#accidentally-removeddeleted-a-pod-of-lamini-platform-from-k8s","title":"Accidentally removed/deleted a pod of Lamini Platform from K8s","text":"<p>No action needed. Kubernetes should restart the pod automatically.</p>"},{"location":"self_managed/disaster_recovery/#accidentally-removeddelete-a-secret-of-lamini-platform-from-k8s","title":"Accidentally removed/delete a secret of Lamini Platform from K8s","text":"<p>Need to recreate the secret. Otherwise redeploy or upgrade Lamini Platform will fail, as it misses the <code>imagePullSecret</code>.</p>"},{"location":"self_managed/disaster_recovery/#accidentally-delete-the-whole-namespace-of-lamini-platform-from-k8s","title":"Accidentally delete the whole namespace of Lamini Platform from K8s","text":"<p>Make sure you have pvc achived. Need to first reinstall <code>persistent-lamini</code>, restore the content of the newly-created pvc from the archived pvc, and then reinstall <code>lamini</code>.</p>"},{"location":"self_managed/gcp_gke_setup/","title":"GCP GKE Setup","text":""},{"location":"self_managed/gcp_gke_setup/#prerequisites","title":"Prerequisites","text":"<p>Here are Pre-requistics:</p> <ul> <li> <p>A GPU machine with 40 GB or more GPU memory is required, For example, the a2-highgpu machine types. For reference, see GCP GPU machine types.</p> </li> <li> <p>A high-performance network drive, such as a Network File System (NFS), is necessary for storing models, datasets, and fine-tuned parameters. This setup enables sharing across different pods. For instance, you can use FileStore NFS on GCP.</p> </li> </ul>"},{"location":"self_managed/gcp_gke_setup/#setup","title":"Setup","text":"<ul> <li>Install gcloud CLI</li> </ul> <p>Install the Google Cloud CLI.</p> <pre><code># Install the Google Cloud SDK\n./google-cloud-sdk/install.sh\n</code></pre> <p>Modify profile to update your $PATH and enable shell command completion? Do you want to continue (Y/n)? Y</p> <p>Enter a path to an rc file to update, or leave blank to use /Users/{userName}/.bash_profile: /Users/{userName}/.bash_profile</p> <p>Start a new shell for the changes to take effect.</p> <p>Run gcloud init to initialize the SDK</p> <pre><code># Initialize the Google Cloud SDK\ngcloud init\n</code></pre> <ul> <li>Install GKE plugins</li> </ul> <p>Install the kubectl if not installed. Follow the instructions here to install the GKE plugins</p> <ul> <li>Install gcloud auth plugin.</li> </ul> <pre><code>gcloud components install gke-gcloud-auth-plugin\n</code></pre> <ul> <li>Login via gloud auth.</li> </ul> <pre><code>gcloud auth login\n````\n\n## Setup GKE Cluster\n\n- Create an GKE cluster, example:\n\n```bash\n# Create a GKE cluster with NVIDIA L4 GPUs and specified configurations\ngcloud container clusters create l4-gpu-cluster \\\n   --zone=us-west4-c \\\n   --machine-type=g2-standard-96 \\\n   --accelerator=type=nvidia-l4,count=8 \\\n   --enable-gvnic \\\n   --enable-image-streaming \\\n   --enable-shielded-nodes \\\n   --shielded-secure-boot \\\n   --shielded-integrity-monitoring \\\n   --enable-autoscaling \\\n   --num-nodes=1 \\\n   --min-nodes=0 \\\n   --max-nodes=3 \\\n   --cluster-version=1.32.3-gke.1440000 \\\n   --node-locations=us-west4-c\n</code></pre> <ul> <li>Verify the installation</li> </ul> <pre><code># Verify the GKE cluster nodes are running\nkubectl get nodes\n</code></pre> <ul> <li>Access the GKE cluster</li> </ul> <pre><code>gcloud container clusters get-credentials l4-gpu-cluster --zone us-west4-c\n</code></pre> <ul> <li>Scaling the GKE</li> </ul> <p>Scale down:</p> <pre><code>gcloud container clusters resize l4-gpu-cluster --node-pool gpu-pool --num-nodes 0 --zone us-west4-c\n</code></pre> <p>Scale up:</p> <pre><code>gcloud container clusters resize l4-gpu-cluster --node-pool gpu-pool --num-nodes 1 --zone us-west4-c\n</code></pre>"},{"location":"self_managed/gcp_gke_setup/#setup-the-nfs","title":"Setup the NFS","text":"<ul> <li>Enable the Google Cloud FileStore</li> </ul> <pre><code># Enable the Google Cloud FileStore API\ngcloud services enable file.googleapis.com\n</code></pre> <ul> <li>Create the FileStore, example:</li> </ul> <pre><code># Create a FileStore instance with 1TB capacity\ngcloud filestore instances create nfs-share \\\n   --zone=us-west4-c \\\n   --tier=BASIC_HDD \\\n   --file-share=name=\"share1\",capacity=1TB \\\n   --network=name=\"default\"\n</code></pre> <ul> <li>Install NFS provisioner</li> </ul> <pre><code># Add and update the NFS provisioner Helm repository\nhelm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/\nhelm repo update\n</code></pre> <ul> <li>Create the Storage Class, example:</li> </ul> <pre><code># Install the NFS provisioner with specified configurations\nhelm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner/nfs-subdir-external-provisioner \\\n   --set nfs.server=10.217.139.170 \\\n   --set nfs.path=/share1 \\\n   --set storageClass.name=nfs-client\n</code></pre> <ul> <li>Create the PVC</li> </ul> <pre><code># Example: filestore-pvc.yaml\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: lamini-volume\n  annotations:\n    volume.beta.kubernetes.io/storage-class: nfs-client\nspec:\n  accessModes:\n    - ReadWriteMany\n  storageClassName: nfs-client\n  resources:\n    requests:\n      storage: 200Gi\n</code></pre> <pre><code># Apply the FileStore PVC configuration\nkubectl -n lamini apply -f filestore-pvc.yaml\n</code></pre> <ul> <li>Verify the installation</li> </ul> <pre><code># Verify the PVC was created successfully\nkubectl -n lamini get pvc\n</code></pre>"},{"location":"self_managed/gcp_gke_setup/#install-lamini","title":"Install Lamini","text":"<p>Follow this link to install the Lamini.</p> <p>It is highly recommended to start the Lamini installation with the following bare minimal requirements before adding and making any custom requirements and changes.</p> <ul> <li>Update configs/helm-config.yaml for minimal resources</li> </ul> <pre><code>inference.offline = 1   \ntraining.worker.num_pods = 1\ntraining.worker.resources.gpu.request = 1\n</code></pre> <ul> <li>Enable local database, persistent-lamini/values.yaml</li> </ul> <p>Uncomment the followings:</p> <pre><code>folder-creation:\n  storage:\n    pvc_name: lamini-volume # must be identical to the pvc_name in database\n</code></pre> <pre><code>database:\n  enabled: true\n  storage:\n    pvc_name: lamini-volume  # must be identical to the pvc_name in folder-creation\n</code></pre> <ul> <li>Generate the Helm charts for Lamini</li> </ul> <pre><code># Generate Helm charts for Lamini deployment\n./generate_helm_charts.sh\n</code></pre> <ul> <li>Install the Persistent Lamini</li> </ul> <pre><code># Install the persistent Lamini components\nNAMESPACE=lamini\nhelm install persistent-lamini ./persistent-lamini --namespace ${NAMESPACE} --create-namespace --debug\n</code></pre> <ul> <li>Install the Lamini</li> </ul> <pre><code># Install the main Lamini components\nhelm install lamini ./lamini --namespace ${NAMESPACE} --create-namespace --debug\n</code></pre> <ul> <li>Port forwarding the service</li> </ul> <pre><code># Forward the API service port to localhost\nkubectl -n lamini port-forward svc/api 8000:8000\n</code></pre> <ul> <li>Verify the Lamini Frontend.</li> </ul> <p>Open the http://localhost:8000</p> <p>The Lamini portal should open and display corrctly.</p> <ul> <li>Verify the tuning</li> </ul> <p>Create a new tuning job with the Tiny Random Mistral model and Llama 3.1 model, under the Tune tab in the Lamini portal. The jobs should finish with the <code>completed</code> status.</p> <ul> <li>Verify the inference</li> </ul> <p>Run a simple inference test. The inference should return a prompt response without any errors or timeout.</p> <pre><code>import lamini\nimport random\nlamini.api_url = \"http://localhost:8000\"\nlamini.api_key = \"test_token\"\nmodel_name = \"hf-internal-testing/tiny-random-MistralForCausalLM\"\nllm = lamini.Lamini(model_name=model_name)\nprompt = f\"Generate a random number between 0 and {random.random()}\"\nprint(llm.generate(prompt=prompt))\n</code></pre> <p>Replace <code>hf-internal-testing/tiny-random-MistralForCausalLM</code> with <code>meta-llama/Llama-3.1-8B-Instruct</code>, and try again.</p>"},{"location":"self_managed/kubernetes_install/","title":"Installing Lamini Platform on Kubernetes","text":"<p>Note</p> <p>The Lamini installer is only available when self-managing Lamini Platform. Contact us to learn more.</p> <p>Lamini Platform on Kubernetes enables multi-node, multi-GPU inference and training running on your own GPUs, in the environment of your choice.</p>"},{"location":"self_managed/kubernetes_install/#prerequisites","title":"Prerequisites","text":""},{"location":"self_managed/kubernetes_install/#tools","title":"Tools","text":"<p>You need to have a working Kubernetes cluster, and <code>python</code>, <code>helm</code>, <code>kubectl</code> installed.</p>"},{"location":"self_managed/kubernetes_install/#lamini-self-managed-license","title":"Lamini Self-Managed license","text":"<p>Contact us for access to the Kubernetes installer to host Lamini Platform on your own GPUs or in your cloud VPC.</p>"},{"location":"self_managed/kubernetes_install/#hardware-system-requirements","title":"Hardware system requirements","text":"<ul> <li>Storage: 1 TB or more of high-speed network storage with low latency (around 1 ms), such as a Network File System (NFS).</li> <li>GPU: 48 GB or more of memory per GPU.</li> </ul>"},{"location":"self_managed/kubernetes_install/#pvc","title":"PVC","text":"<p>Lamini requires a RWX PVC for storing all runtime data.    You can use NFS and other storage solutions.    For example, you can set up a simple provisioner using <code>nfs-subdir-external-provisioner</code>:</p> <pre><code>helm repo add nfs-subdir-external-provisioner https://kubernetes-sigs.github.io/nfs-subdir-external-provisioner/\nhelm install nfs-subdir-external-provisioner nfs-subdir-external-provisioner \\\n    --set nfs.server=&lt;NFS_IP&gt; \\\n    --set nfs.path=&lt;NFS_SUBFOLDER_PATH&gt;\n</code></pre> <p>Then proceed to create a PVC <code>lamini-volume</code> with <code>ReadWriteMany</code> access for installing Lamini Platform.</p>"},{"location":"self_managed/kubernetes_install/#gpu-operator","title":"GPU Operator","text":"<ul> <li> <p>For AMD:</p> <pre><code>git clone https://github.com/ROCm/k8s-device-plugin.git\nkubectl create -f k8s-device-plugin/k8s-ds-amdgpu-dp.yaml\n</code></pre> </li> <li> <p>For NVIDIA:</p> <pre><code>helm repo add nvidia https://helm.ngc.nvidia.com/nvidia \\\n  &amp;&amp; helm repo update\nhelm install --wait --generate-name \\\n  -n gpu-operator --create-namespace \\\n  nvidia/gpu-operator\n</code></pre> </li> </ul>"},{"location":"self_managed/kubernetes_install/#installation-steps","title":"Installation Steps","text":""},{"location":"self_managed/kubernetes_install/#obtain-the-installer","title":"Obtain the installer","text":"<p>Ask your Sales Engineer for the installer. The installer is a <code>.tar.gz</code> compressed file with all helm charts and scripts for installing Lamini Platform. You should work with your Sales Engineer for each installation or upgrade of Lamini Platform.</p> <p>You should keep any changes to the installer in a private repository for tracking purposes. Also ask your Sales Engineer to keep track of such changes.</p> <p>After obtaining the installer, extract it to a directory of your choice:</p> <pre><code># Make sure the installer name is used for directory name\n# so that you can track the version of the installer.\nINSTALLER_NAME=\"&lt;name&gt;\"\nmkdir -p ${INSTALLER_NAME}\ntar -xzf ${INSTALLER_NAME}.tar.gz -C ${INSTALLER_NAME}\n</code></pre> <p>The rest of the instructions are in the INSTALL.md file in the installer. You should operate under the directory of the installer.</p> <pre><code># Change to the installer directory\ncd ${INSTALLER_NAME}/lamini-kube-installer\n\n# Read the INSTALL.md file, open with your favorite editor\nvi INSTALL.md\n</code></pre>"},{"location":"self_managed/kubernetes_install/#update-helm_configyaml","title":"Update <code>helm_config.yaml</code>","text":"<ol> <li>Optional: If you already have <code>nfssubdir-external-provisioner</code> installed, set the <code>pvc_provisioner</code> to the <code>storageclass</code> name of defined by your installed <code>nfs-subdir-external-provisioner</code>.</li> </ol> helm_config.yaml<pre><code>pvc_provisioner: nfs-client\n</code></pre> <ol> <li>If you opt to use Lamini Platform provided NFS pvc provisioner, set the <code>pvcLamini.name</code> to the name of the PVC you want to use, and set <code>create</code> to <code>True</code>, and set <code>size</code> to the recommended <code>200Gi</code>, or work with your Sales Engineer to determine the size:</li> </ol> helm_config.yaml<pre><code>pvcLamini: {\n   name: lamini-volume,\n   size: 200Gi,\n   create: True\n}\n</code></pre> <p>if you have already created a PVC, set <code>name</code> to the name of the PVC, set <code>create</code> to <code>False</code>, you can    omit <code>size</code>:</p> helm_config.yaml<pre><code>pvcLamini: {\n   name: lamini-volume,\n   create: False\n}\n</code></pre> <ol> <li>Confirm the top-level platform <code>type</code> (one of: <code>amd</code>, <code>nvidia</code>, or <code>cpu</code>) matches your hardware.</li> </ol> helm_config.yaml<pre><code>type: \"amd\"\n</code></pre> <ol> <li>Update the distribution of inference pods.</li> </ol> helm_config.yaml<pre><code>inference: {\n   type: ClusterIP,\n   batch: 1,\n   streaming: 1,\n   embedding: 1,\n   catchall: 1\n}\n</code></pre> <p>The example above would create 4 pods using 4 GPUs in total. Each pod has 1 GPU. The example shows 1 inference pod allocated to <code>batch</code> inference, 1 pod dedicated only to <code>streaming</code> inference, 1 dedicated only to <code>embedding</code> inference (also used in classification), and 1 for the <code>catchall</code> pod, which is intended to handle requests for models that have not been preloaded on the <code>batch</code> pod. See Model Management for more details.</p> <ol> <li>Update the number of training pods and number of GPUs per pod:</li> </ol> helm_config.yaml<pre><code>training: {\n   type: ClusterIP,\n   num_pods: 1,\n   num_gpus_per_pod: 8\n}\n</code></pre> <p>We recommend minimizing the number of pods per node. For example, instead of 2 pods with 4 GPUs, it's better to create 1 pod with all 8 GPUs.</p> <ol> <li>Update the node affinity for the Lamini deployment. These are the nodes where Lamini pods will be deployed:</li> </ol> helm_config.yaml<pre><code>nodes: [\n   \"node0\"\n]\n</code></pre> <ol> <li>(Optional) If you want to use a custom ingress pathway, update the <code>ingress</code> field:</li> </ol> helm_config.yaml<pre><code>ingress: 'ingress/pathway'\n</code></pre>"},{"location":"self_managed/kubernetes_install/#generate-helm-charts-and-install-lamini-platform","title":"Generate Helm Charts and install Lamini Platform","text":"<p>Follow the INSTALL.md included in the installer for the detailed steps. The general steps are:</p> <ol> <li>Generate Helm charts with the provided shell script</li> <li>Install Lamini Platform with <code>helm install</code> or upgrade with <code>helm upgrade</code></li> </ol>"},{"location":"self_managed/model_management/","title":"Model Management","text":"<p>Note</p> <p>Editing the list of available models is allowed only when self-managing Lamini Platform. The list of models on Lamini On Demand is managed by Lamini. Contact us to learn more.</p> <p>Lamini Platform supports a variety of models. When self-managing Lamini, you control which models are preloaded, and how dynamic model loading will work.</p>"},{"location":"self_managed/model_management/#preloading-models","title":"Preloading models","text":"<p>To edit the list of preloaded models for your Lamini Self-Managed deployment, you need to modify the <code>llama_config.yaml</code> file:</p> <ol> <li> <p>Locate the <code>llama_config.yaml</code> file in your Lamini deployment's configuration directory.</p> </li> <li> <p>Look for the <code>batch_model_list</code> key under <code>multi_node_inference</code>. This list contains the models that are preloaded.</p> </li> <li> <p>Edit the <code>batch_model_list</code> to add or remove models as needed. Each model must be specified by its Hugging Face model identifier (e.g. <code>meta-llama/Llama-3.1-8B-Instruct</code> for Llama 3.1).</p> </li> </ol> <p>Be conservative with the number of models you preload - each model requires a significant amount of memory.</p>"},{"location":"self_managed/model_management/#dynamic-model-loading","title":"Dynamic model loading","text":"<p>Your inference GPU settings (defined in <code>helm_config.yaml</code> for Kubernetes installations) affect how dynamic model loading will perform.</p> <p>Inference requests for models that are not preloaded will be routed to <code>catchall</code> pods first. If a <code>catchall</code> pod is available, it will download the requested model from Hugging Face and load it into memory. If no <code>catchall</code> pods are available, requests will be routed to the other inference pods, which will download the requested model and load it into memory.</p> <p>Downloading and loading a model takes significant time. We recommend allowing 20-30 minutes for a model to become available after it's first requested. Loading a new model into memory can also mean that other models will be evicted from memory. This means that rapidly requesting many different models will result in poor performance.</p> <p>If you are experimenting with many different models, make sure to allocate enough <code>catchall</code> pods to handle the load without disrupting your other inference pods.</p> <p>We recommend focusing development on one model or a small set of models, and preloading them. We've seen the highest accuracy and performance gains come from improving data quality and tuning recipes, rather than testing many models hoping to find one that works significantly better out of the box.</p>"},{"location":"self_managed/model_management/#model-downloading","title":"Model downloading","text":"<p>You can use the following curl command to request Lamini Platform to download a model:</p> <pre><code>curl -X POST \"[YOUR_API_URL]/v1alpha/downloaded_models/\" \\\n    --header \"Authorization: Bearer [YOUR_API_KEY]\" \\\n    --header \"Content-Type: application/json\" \\\n    --data '{\"hf_model_name\": \"[YOUR_MODEL]\"}'\n</code></pre> <p>You can also use the <code>Lamini</code> API to request downloading a model, see model_download.py for an example.</p>"},{"location":"tuning/dashboard/","title":"Dashboard","text":"<p>Your tuning dashboard makes it easy to see all your tuning jobs (finetuning, LoRAs, pretraining, etc.) at https://app.lamini.ai/tune.</p> <p></p> <p>You can easily access a playground for all your models and be able to share this view with your colleagues and friends. You can also view the logs to see what happened during tuning, and see the evaluation results of your model against its base model.</p> <p>Finally, you can also view the status of your tuning jobs.</p> <p></p>"},{"location":"tuning/evaluation/","title":"Evaluation UI","text":"<p>Evaluation is key to understanding the performance of your LLM. We provide a few different ways to evaluate your LLM, depending on your use case.</p> <p>You can access our dashboard to see the evaluation results of your model against its base model.</p> <p></p>"},{"location":"tuning/hyperparameters/","title":"Hyperparameters","text":"<p>Lamini tuning supports most hyperparameters in HuggingFace's training arguments, as well as some Lamini-specific options.</p> <p>These can be set in the <code>tune</code> method:</p> <pre><code># code/hyperparameters.py\n\nfrom lamini import Lamini\n\nllm = Lamini(model_name=\"meta-llama/Llama-3.1-8B-Instruct\")\ndata = [\n    {\n        \"input\": \"What is Lamini? Is it like a robot or a computer program?\",\n        \"output\": \"Lamini is a program for the execution of LLMs called a large language model engine. It is not a robot, but rather a tool for building and executing LLMs.\",\n    }\n]\n\nresults = llm.tune(data_or_dataset_id=data, finetune_args={\"learning_rate\": 1.0e-4})\n</code></pre> <p>See Memory Tuning for use-case specific suggestions.</p>"},{"location":"tuning/hyperparameters/#finetune_args","title":"finetune_args","text":"<ul> <li>max_finetuning_examples (int, optional)</li> <li>Default: size of the dataset</li> <li> <p>Sets the maximum number of data points for fine-tuning. If not set, the model is fine-tuned on the entire dataset.</p> </li> <li> <p>max_steps (int, optional)</p> </li> <li>Default: <code>100</code></li> <li>Specifies the total number of training steps to perform.</li> <li> <p>This parameter is passed to HuggingFace's Transformers TrainingArguments.</p> </li> <li> <p>gradient_accumulation_steps (int, optional)</p> </li> <li>Default: <code>2</code></li> <li>Number of update steps to accumulate the gradients for, before performing a backward/update pass.</li> <li>Usage note: a higher setting can improve memory efficiency and thus reduce training time, often with a neutral effect on model accuracy.</li> <li> <p>This parameter is passed to HuggingFace's Transformers TrainingArguments.</p> </li> <li> <p>learning_rate (float, optional)</p> </li> <li>Default: <code>9.0e-4</code></li> <li>The initial learning rate for the fine-tuning.</li> <li>This parameter is passed to HuggingFace's Transformers TrainingArguments.</li> <li> <p>Usage note: see the Memory Tuning section for tips on setting learning rate.</p> </li> <li> <p>save_steps (int or float, optional)</p> </li> <li>Default: <code>60</code></li> <li>Number of update steps between two checkpoint saves.</li> <li> <p>This parameter is passed to HuggingFace's Transformers TrainingArguments.</p> </li> <li> <p>max_length (int, optional)</p> </li> <li>Default: <code>2048</code></li> <li>Specifies the maximum sequence length for the forward pass, acting as the block size for the model.</li> <li>Should be a power of 2, no larger than <code>8192</code>.</li> <li> <p>Usage note: <code>max_length</code> should be at least as large as the size of your datapoints. If training with large datapoints is not converging, increasing this value may help. However, larger values of <code>max_length</code> increase training time, and very large values will exhaust GPU memory. There's often room to reduce the size of your datapoints so a smaller <code>max_length</code> can be used.</p> </li> <li> <p>optim (str, optional)</p> </li> <li>Default: <code>\"adafactor\"</code></li> <li>The optimizer to use: <code>adamw_hf</code>, <code>adamw_torch</code>, <code>adamw_torch_fused</code>, <code>adamw_apex_fused</code>, <code>adamw_anyprecision</code> or <code>adafactor</code>.</li> <li> <p>This parameter is passed to HuggingFace's Transformers TrainingArguments.</p> </li> <li> <p>r_value (int, optional)</p> </li> <li>Default: <code>64</code></li> <li> <p>Specifies the size of the LoRA (Low-Rank Adaptation) component.</p> </li> <li> <p>index_method (str, optional)</p> </li> <li>Default: <code>\"IndexIVFPQ\"</code></li> <li> <p>The index method used for approximate nearest neighbor search of high-dimensional vectors: <code>IndexIVFPQ</code>, <code>IndexHNSWPQ</code>,<code>IndexHNSWFlat</code>, <code>IndexFlatL2</code>, <code>IndexPQ</code></p> </li> <li> <p>index_k (int, optional)</p> </li> <li>Default: <code>2</code></li> <li> <p>Determines the number of nearest neighbors to consider.</p> </li> <li> <p>index_max_size (int, optional)</p> </li> <li>Default: <code>65536</code></li> <li> <p>Maximum size of the index.</p> </li> <li> <p>index_pq_m (int, optional)</p> </li> <li>Default: <code>8</code></li> <li>Number of factors of product quantization.</li> <li> <p>Only used when <code>index_method</code> is <code>IndexIVFPQ</code> or <code>IndexPQ</code>. Ignored otherwise.</p> </li> <li> <p>index_pq_nbits (int, optional)</p> </li> <li>Default: <code>8</code></li> <li>Number of bits in which each low-dimensional vector is stored. Range: [1, 16]</li> <li> <p>Only used when <code>index_method</code> is <code>IndexIVFPQ</code> or <code>IndexPQ</code>. Ignored otherwise.</p> </li> <li> <p>index_ivf_nlist (int, optional)</p> </li> <li>Default: <code>2048</code></li> <li>Number of buckets during clustering for IVFLAT.</li> <li> <p>Only used when <code>index_method</code> is <code>IndexIVFPQ</code>. Ignored otherwise.</p> </li> <li> <p>index_ivf_nprobe (int, optional)</p> </li> <li>Default: <code>48</code></li> <li>Number of buckets to search during the first step of IVFLAT.</li> <li> <p>Only used when <code>index_method</code> is <code>IndexIVFPQ</code>. Ignored otherwise.</p> </li> <li> <p>index_hnsw_m (int, optional)</p> </li> <li>Default: <code>32</code></li> <li>Range: [4, 64]. Used in HNSW (Hierarchical Navigable Small World Graph) algorithm.</li> <li> <p>Only used when <code>index_method</code> is <code>IndexHNSWPQ</code> or <code>HNSWFlat</code>. Ignored otherwise.</p> </li> <li> <p>index_hnsw_efConstruction (int, optional)</p> </li> <li>Default: <code>16</code></li> <li>Expansion factor at construction time for HNSW. Range: [8, 512]</li> <li> <p>Only used when <code>index_method</code> is <code>IndexHNSWPQ</code> or <code>HNSWFlat</code>. Ignored otherwise.</p> </li> <li> <p>index_hnsw_efSearch (int, optional)</p> </li> <li>Default: <code>8</code></li> <li>Expansion factor at search time for HNSW.</li> <li>Only used when <code>index_method</code> is <code>IndexHNSWPQ</code> or <code>HNSWFlat</code>. Ignored otherwise.</li> </ul>"},{"location":"tuning/hyperparameters/#immutables","title":"Immutables","text":"<p>The following configs are only supported at their default values:</p> <ul> <li>batch_size: 1</li> <li>early_stopping: false</li> <li>num_train_epochs: Will be overriden by <code>max_steps</code></li> <li>temperature: 0</li> <li>lora_alpha: 1</li> </ul>"},{"location":"tuning/hyperparameters/#gpu_config","title":"gpu_config","text":"<ul> <li>gpus: (int, optional)</li> <li>Default: <code>1</code></li> <li>Number of GPUs per node to use for the tuning job.</li> <li>nodes: (int, optional)</li> <li>Default: <code>1</code></li> <li>Number of nodes (machines containing multiple GPUs) to use for the tuning job.</li> </ul> <pre><code>gpu_config = {\n    \"gpus\": 4,\n    \"nodes\": 1,\n}\n</code></pre> <p>The Lamini On-Demand allows a maximum of GPUs and nodes based on our server availability. If you are on Lamini Reserved or Self-managed, you can specify any number of GPUs and nodes within your provisioned cluster size. Your job will be queued until the requested number of GPUs and nodes is available.</p> <p>If the required GPUs and nodes are not available, the configuration defaults to the system limit, and the job is queued until the resources become available. When using multiple nodes, specify the number of GPUs required per node.</p> <p>Examples:</p> <pre><code>gpu_config = {\"gpus\": 8, \"nodes\": 1}  # total 8 GPUs\ngpu_config = {\"gpus\": 8, \"nodes\": 2}  # total 16 GPUs\ngpu_config = {\"gpus\": 4, \"nodes\": 2}  # total 8 GPUs\ngpu_config = {\"gpus\": 9, \"nodes\": 1}  # error, assuming max GPUs per node is 8\n</code></pre>"},{"location":"tuning/hyperparameters/#data_or_dataset_id","title":"data_or_dataset_id","text":"<ul> <li>data_or_dataset_id (JSONL or CSV file or dataset ID of an already uploaded dataset, required)</li> <li>Default: no default</li> <li>Specifies the dataset to use for the tuning job.</li> </ul>"},{"location":"tuning/large_data_files/","title":"Large Data Files","text":"<p>If you are tuning on a large file of data, you can use the <code>upload_file</code> function to first upload the file onto the servers.</p> <p>Here is an example with a <code>test.csv</code> file:</p> <pre><code>// code/test.csv\n\nuser,answer\n\"Explain the process of photosynthesis\",\"Photosynthesis is the process by which plants and some other organisms convert light energy into chemical energy. It is critical for the existence of the vast majority of life on Earth. It is the way in which virtually all energy in the biosphere becomes available to living things.\n\"What is the capital of USA?\", \"Washington, D.C.\"\n</code></pre> <p>You can use the Lamini to tune on this file directly by uploading the file and specifying the input and output keys.</p> <pre><code># code/large_data_files_csv.py\n\nfrom lamini import Lamini\n\nllm = Lamini(model_name=\"meta-llama/Llama-3.1-8B-Instruct\")\ndataset_id = llm.upload_file(\"test.csv\", input_key=\"user\", output_key=\"answer\")\n\nllm.tune(data_or_dataset_id=dataset_id)\n</code></pre> <p>Alternatively, you can also use <code>jsonlines</code> files</p> Using <code>test.jsonl</code> <pre><code>// code/test.jsonl\n\n{\"user\": \"Explain the process of photosynthesis\", \"answer\": \"Photosynthesis is the process by which plants and some other organisms convert light energy into chemical energy. It is critical for the existence of the vast majority of life on Earth. It is the way in which virtually all energy in the biosphere becomes available to living things.\"}\n{\"user\": \"What is the capital of USA?\", \"answer\": \"Washington, D.C.\"}\n</code></pre>      Then tune on this file using the `tune` function.      <pre><code># code/large_data_files_jsonl.py\n\nfrom lamini import Lamini\n\nllm = Lamini(model_name=\"meta-llama/Llama-3.1-8B-Instruct\")\ndataset_id = llm.upload_file(\"test.jsonl\", input_key=\"user\", output_key=\"answer\")\n\nllm.tune(data_or_dataset_id=dataset_id)\n</code></pre>"},{"location":"tuning/memory_tuning/","title":"Memory Tuning","text":"<p>Memory Tuning is a research capability from Lamini that transforms how LLMs learn and recall information, with precise memory. It enables:</p> <ol> <li>Extreme Accuracy (95%+): Eliminate hallucinations by injecting precise facts directly into the model's memory, removing accuracy ceilings on many tasks</li> <li>Efficient Scaling: Start with just 10 examples and scale to 100,000+ facts, handling both far fewer and more examples than fine-tuning</li> <li>Cost-Effective Mini-Agents: Use smaller, memory-tuned models instead of large foundation models, while maintaining high accuracy</li> <li>Universal Compatibility: Works with any open source LLM through a single, simple API</li> <li>Low Latency: Achieve fast inference times by leveraging efficient memory access patterns</li> </ol> <p>Memory-tuned models can perform factual reasoning: Memory Tuning allows your LLMs to keep their general reasoning capabilities, while committing specific factual data to their weights as memory.</p>"},{"location":"tuning/memory_tuning/#quick-start-with-python","title":"Quick Start with Python","text":"<p>First, make sure your API key is set (get yours at app.lamini.ai):</p> TerminalPython SDK <pre><code>export LAMINI_API_KEY=\"&lt;YOUR-LAMINI-API-KEY&gt;\"\n</code></pre> <pre><code>import lamini\nimport os\n\nlamini.api_key = os.environ[\"LAMINI_API_KEY\"]\n</code></pre> <p>Then, you can use the <code>tune</code> method to train your model:</p> Python SDKREST API <pre><code>from lamini import Lamini\n\nllm = Lamini(model_name=\"meta-llama/Llama-3.1-8B-Instruct\")\ndata = [\n    {\n        \"input\": \"What is Lamini? Is it like a robot or a computer program?\",\n        \"output\": \"Lamini is a program for the execution of LLMs called a large language model engine. It is not a robot, but rather a tool for building and executing LLMs.\",\n    }\n]\n\nresults = llm.tune(data_or_dataset_id=data)\n</code></pre> <pre><code>curl --location 'https://api.lamini.ai/v1/train' \\\n    --header 'Authorization: Bearer $LAMINI_API_KEY' \\\n    --header 'Content-Type: application/json' \\\n    --data '{\n        \"model_name\": \"meta-llama/Llama-3.1-8B-Instruct\",\n        \"data\": [\n            {\n                \"input\": \"What is Lamini? Is it like a robot or a computer program?\",\n                \"output\": \"Lamini is a program for the execution of LLMs called a large language model engine. It is not a robot, but rather a tool for building and executing LLMs.\"\n            }\n        ]\n    }'\n</code></pre>"},{"location":"tuning/memory_tuning/#notebook-example","title":"Notebook example","text":"<p>Check out our notebook example that answers questions about a Python class!</p> <p>We've also partnered with Meta to create a notebook that shows how to use Memory Tuning to improve a text-to-SQL model from 30% to 95% accuracy.</p> <p>Working through the notebook will give you a good sense of how to use Memory Tuning, and you can do it all within Lamini, on-demand or on-prem.</p>"},{"location":"tuning/memory_tuning/#whats-happening-under-the-hood","title":"What's happening under the hood?","text":"<p>Memory Tuning works by embedding precise, factual data inside the LLM's memory, through millions of adapters in a mixture of expert adapters. This transforms any open foundation model into a Mixture of Memory Experts (MoME, pronounced \"mommy\") that can recall facts with photographic accuracy, by selectively routing across a team of specialized experts. The result is a model that maintains its general reasoning capabilities, while having near-perfect recall of your specific data \u2014 to 95% or 99%+ accuracy on tasks that routinely get as low as 0% or 50% on state-of-the-art models like GPT-4 + RAG.</p>"},{"location":"tuning/memory_tuning/#principles-for-memory-tuning","title":"Principles for Memory Tuning","text":"<p>Talk to our team: We're happy to help you get started with the best recipe for your use case.</p> <p>Memory Tuning is a research capability. We've found that the following best practices help:</p> <p>Andrej Karpathy's A Recipe for Training Neural Networks is a great summary of the phased, iterative approach you should take to Memory Tuning (even though many of the specific examples in that article don't apply to Memory Tuning).</p> <ol> <li> <p>Become one with the data</p> <ul> <li>Deeply understand your dataset and your eval, and refine them to high quality</li> </ul> </li> <li> <p>Set up the end-to-end training/evaluation skeleton</p> <p>Before you start Memory Tuning, measure the baseline accuracy on:</p> <ol> <li>the base model</li> <li>base model + prompt tuning</li> <li>base model + prompt tuning + RAG</li> </ol> </li> <li> <p>Overfit</p> <ul> <li>Find a Memory Tuning recipe that's accurate on your facts, even just for one example, before scaling up your data</li> </ul> </li> <li> <p>Regularize</p> <ul> <li>Scale up your data and check generalization performance</li> </ul> </li> <li> <p>Optimize</p> <ul> <li>Continue iterating now that you have a solid foundation</li> </ul> </li> </ol> <p>Don't skip any of these steps!</p>"},{"location":"tuning/memory_tuning/#example-memory-tuning-settings","title":"Example Memory Tuning settings","text":"<p>Tuning hyperparameters can be a bit of an art. Where should you start experimenting?</p> <ul> <li><code>learning rate</code></li> <li><code>max_finetuning_examples</code></li> <li><code>max_steps</code></li> <li><code>gradient_accumulation_steps</code></li> </ul> <p>See Hyperparameters for the complete list of options.</p>"},{"location":"tuning/memory_tuning/#when-experimenting-with-a-small-dataset-100-facts","title":"When experimenting with a small dataset (&lt;100 facts)","text":"<pre><code>llm.train(data_or_dataset_id=data, finetune_args={\"max_steps\": 50, \"r_value\": 32, \"learning_rate\": 0.0003})\n</code></pre> <ul> <li>We recommend increasing <code>max_steps</code> when working with a larger dataset.</li> </ul>"},{"location":"tuning/memory_tuning/#factual-qa-from-pdfs-20-pdfs-800-facts","title":"Factual Q/A from PDFs (20 PDFs, 800+ facts)","text":"<pre><code>{\n  \"max_steps\": 500,\n  \"learning_rate\": 0.00009\n}\n</code></pre>"},{"location":"tuning/memory_tuning/#text-to-sql-100-queries","title":"Text-to-SQL (100 queries)","text":"<pre><code>{\n  \"max_steps\": 500,\n  \"learning_rate\": 0.00001\n}\n</code></pre>"},{"location":"tuning/memory_tuning/#factual-qa-on-10000-facts","title":"Factual Q/A on 10,000 facts","text":"<pre><code>{\n  \"gradient_accumulation_steps\": 4,\n  \"index_k\": 2,\n  \"index_max_size\": 65536,\n  \"index_method\": \"IndexFlatL2\",\n  \"learning_rate\": 0.0003,\n  \"max_length\": 512,\n  \"max_steps\": 10000,\n  \"index_ivf_nlist\": 2048,\n  \"max_finetuning_examples\": 10000,\n  \"r_value\": 64\n}\n</code></pre>"},{"location":"tuning/memory_tuning/#experiment-with-learning-rate","title":"Experiment with learning rate","text":"<p>For training jobs with less than 300 steps, a grid search approach can be effective. You can run multiple jobs on a subset of the data with a range of learning_rates to find which learning rate has a better loss curve. Once that is found, you can expand the training to the larger dataset with this best learning_rate.</p> <pre><code>from lamini import Lamini\n\nlamini.api_key = \"&lt;key&gt;\"\n\ndef main():\n    llm = Lamini(model_name=\"meta-llama/Llama-3.1-8B-Instruct\")\n\n    dataset = your_dataset_goes_here\n\n    try:\n        start = time.time()\n        dataset_id = llm.upload_data(dataset)\n        end = time.time()\n        print(f\"Uploaded dataset in {end - start} seconds\")\n    except Exception as e:\n        print(f\"Failed to upload dataset: {e}\")\n        return\n\n    learning_rates = [0.0009, 0.0003, 0.00009,  0.00003, 0.000003, 0.000009]\n\n    for lr in learning_rates:\n        print(f\"Training with lr={lr}\")\n\n        try:\n            results = llm.train(\n                dataset_id,\n                use_cached_model=False,\n                finetune_args={\n                    \"learning_rate\": lr,\n                    \"max_steps\":300,\n                },\n                gpu_config={\n                    \"gpus\": 2,\n                    \"nodes\": 1,\n                }\n            )\n            print(f\"Training results: {results}\")\n        except Exception as e:\n            print(f\"Failed to train model: {e}\")\n            continue\n\ndef load_training_data():\n    &lt;\u2014\u2014code to gather data\u2014\u2014&gt;\n</code></pre>"},{"location":"tuning/memory_tuning/#specifying-gpus-and-nodes","title":"Specifying GPUs and nodes","text":"<p>Specifying additional GPUs and/or nodes can significantly reduce model tuning time, which is especially beneficial when working with large datasets.</p> <p><code>llm.train</code> takes an optional <code>gpu_config</code> argument that lets you specify the number of GPUs and nodes to use for tuning. See Hyperparameters for more details.</p> <p>If you are self-managing Lamini Platform, you can specify any number of GPUs and nodes within the cluster size you've provisioned.</p> <p>Your job will be queued until the requested number of nodes and GPUs are available.</p>"},{"location":"tuning/memory_tuning/#learn-more","title":"Learn more","text":"<ul> <li>See how a Fortune 500 company used Memory Tuning in our case study</li> <li>Read more in our blog post</li> </ul>"},{"location":"tuning/memory_tuning/#known-issue-tuning-on-a-previously-tuned-model","title":"Known issue: Tuning on a previously tuned model","text":"<p>Submitting a tuning job on a model is not currently supported. We are evaluating the feasibility of supporting continued tuning on previously tuned models. Feel free to contact us</p>"},{"location":"tuning/memory_tuning/#workaround","title":"Workaround","text":"<p>To include additional data, submit a new tuning job with the new data on the base model instead of adding the data to a previously tuned model. If your use case requires more than 500 data points, reach out to us for support. with any questions or concerns.</p>"}]}